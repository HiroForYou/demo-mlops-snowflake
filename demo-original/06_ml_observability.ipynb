{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# ARCA Beverage Demo: ML Observability\n\n## Overview\nThis notebook sets up **automated ML monitoring** to replace manual drift detection.\n\n## Key Capabilities:\n1. **Drift Monitoring**: Jensen-Shannon divergence on feature distributions\n2. **Performance Tracking**: RMSE, MAE, WAPE metrics over time\n3. **Automated Alerts**: Threshold-based notifications (JS > 0.2 warning, > 0.4 critical)\n4. **Feature Distribution Monitoring**: Training vs Inference comparison\n\n## Business Value:\n**Replaces manual drift detection with automated monitoring**:\n- Before: Data scientists manually check for drift (weekly/monthly)\n- After: Real-time automated monitoring with alerts\n- Snowflake-native: No external tools required\n\n## Key Message:\n\"Built-in ML Observability eliminates manual monitoring overhead and catches issues early.\"",
      "id": "a1b2c3d4-e5f6-7890-abcd-111111111111"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nsession = get_active_session()\n\nsession.sql(\"USE WAREHOUSE ARCA_DEMO_WH\").collect()\nsession.sql(\"USE DATABASE ARCA_BEVERAGE_DEMO\").collect()\nsession.sql(\"USE SCHEMA MODEL_REGISTRY\").collect()\n\nprint(f\"âœ… Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")",
      "id": "b2c3d4e5-f6a7-8901-bcde-222222222222"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Verify Inference Logs Exist\n\nCheck that we have predictions from Notebook 05",
      "id": "c3d4e5f6-a7b8-9012-cdef-333333333333"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "inference_check = session.sql(\"\"\"\nSELECT\n    COUNT(*) AS TOTAL_PREDICTIONS,\n    COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n    COUNT(DISTINCT SEGMENT) AS SEGMENTS,\n    MIN(INFERENCE_TIMESTAMP) AS FIRST_PREDICTION,\n    MAX(INFERENCE_TIMESTAMP) AS LAST_PREDICTION\nFROM ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_LOGS\n\"\"\")\n\nprint(\"\\nðŸ“Š Inference Logs Status:\\n\")\ninference_check.show()\n\nlog_count = session.table('ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_LOGS').count()\n\nif log_count == 0:\n    print(\"\\nâŒ ERROR: No inference logs found!\")\n    print(\"   Please run Notebook 05 (Partitioned Inference) first.\")\nelse:\n    print(f\"\\nâœ… {log_count:,} predictions ready for monitoring\")",
      "id": "d4e5f6a7-b8c9-0123-def0-444444444444"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Prepare Baseline Data for Drift Detection\n\nCreate baseline tables from training data (used for comparison)",
      "id": "e5f6a7b8-c9d0-1234-ef01-555555555555"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\nðŸ“ Creating baseline tables for each segment...\\n\")\n\nfor segment_num in range(1, 7):\n    segment = f'SEGMENT_{segment_num}'\n    \n    baseline_sql = f\"\"\"\n    CREATE OR REPLACE TABLE ARCA_BEVERAGE_DEMO.ML_DATA.BASELINE_{segment} AS\n    SELECT\n        CUSTOMER_ID,\n        WEEK_START_DATE AS TIMESTAMP_COL,\n        WEEKLY_SALES_UNITS::FLOAT AS ACTUAL_WEEKLY_SALES,\n        WEEKLY_SALES_UNITS::FLOAT AS PREDICTED_WEEKLY_SALES,\n        CUSTOMER_TOTAL_UNITS_4W::FLOAT AS CUSTOMER_TOTAL_UNITS_4W,\n        WEEKS_WITH_PURCHASE::FLOAT AS WEEKS_WITH_PURCHASE,\n        VOLUME_QUARTILE::FLOAT AS VOLUME_QUARTILE,\n        WEEK_OF_YEAR::FLOAT AS WEEK_OF_YEAR,\n        MONTH::FLOAT AS MONTH,\n        QUARTER::FLOAT AS QUARTER,\n        TRANSACTION_COUNT::FLOAT AS TRANSACTION_COUNT,\n        UNIQUE_PRODUCTS_PURCHASED::FLOAT AS UNIQUE_PRODUCTS_PURCHASED,\n        AVG_UNITS_PER_TRANSACTION::FLOAT AS AVG_UNITS_PER_TRANSACTION\n    FROM ARCA_BEVERAGE_DEMO.ML_DATA.TRAINING_DATA\n    WHERE SEGMENT = '{segment}'\n    \"\"\"\n    \n    session.sql(baseline_sql).collect()\n    \n    count = session.table(f'ARCA_BEVERAGE_DEMO.ML_DATA.BASELINE_{segment}').count()\n    print(f\"âœ… {segment}: {count:,} baseline records\")\n\nprint(\"\\nâœ… All baseline tables created with FLOAT types\")",
      "id": "f6a7b8c9-d0e1-2345-f012-666666666666"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Prepare Inference Data for Monitoring\n\nCreate monitoring-ready tables with proper structure",
      "id": "a7b8c9d0-e1f2-3456-0123-777777777777"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\nðŸ“ Creating inference tables for monitoring...\\n\")\n\nfor segment_num in range(1, 7):\n    segment = f'SEGMENT_{segment_num}'\n    \n    inference_sql = f\"\"\"\n    CREATE OR REPLACE TABLE ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_{segment} AS\n    SELECT\n        l.CUSTOMER_ID,\n        l.INFERENCE_TIMESTAMP AS TIMESTAMP_COL,\n        l.PREDICTED_WEEKLY_SALES::FLOAT AS PREDICTED_WEEKLY_SALES,\n        l.ACTUAL_WEEKLY_SALES::FLOAT AS ACTUAL_WEEKLY_SALES,\n        i.CUSTOMER_TOTAL_UNITS_4W::FLOAT AS CUSTOMER_TOTAL_UNITS_4W,\n        i.WEEKS_WITH_PURCHASE::FLOAT AS WEEKS_WITH_PURCHASE,\n        i.VOLUME_QUARTILE::FLOAT AS VOLUME_QUARTILE,\n        i.WEEK_OF_YEAR::FLOAT AS WEEK_OF_YEAR,\n        i.MONTH::FLOAT AS MONTH,\n        i.QUARTER::FLOAT AS QUARTER,\n        i.TRANSACTION_COUNT::FLOAT AS TRANSACTION_COUNT,\n        i.UNIQUE_PRODUCTS_PURCHASED::FLOAT AS UNIQUE_PRODUCTS_PURCHASED,\n        i.AVG_UNITS_PER_TRANSACTION::FLOAT AS AVG_UNITS_PER_TRANSACTION\n    FROM ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_LOGS l\n    JOIN ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_INPUT_TEMP i \n        ON l.CUSTOMER_ID = i.CUSTOMER_ID AND l.SEGMENT = i.SEGMENT\n    WHERE l.SEGMENT = '{segment}'\n    \"\"\"\n    \n    session.sql(inference_sql).collect()\n    \n    count = session.table(f'ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_{segment}').count()\n    print(f\"âœ… {segment}: {count:,} inference records\")\n\nprint(\"\\nâœ… All inference tables ready with FLOAT types\")",
      "id": "b8c9d0e1-f2a3-4567-1234-888888888888"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Create Model Monitors\n\nCreate monitors for 2-3 segments (demo purposes - not all 6 needed)",
      "id": "c9d0e1f2-a3b4-5678-2345-999999999999"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"ðŸ”§ CREATING MODEL MONITORS\")\nprint(\"=\"*80)\nprint(\"\\nCreating monitors for SEGMENT_1, SEGMENT_3, and SEGMENT_5 (demo sample)\\n\")\n\nmonitor_segments = ['SEGMENT_1', 'SEGMENT_3', 'SEGMENT_5']\ncreated_monitors = []\n\nfor segment in monitor_segments:\n    model_name = f'WEEKLY_SALES_FORECAST_{segment}'\n    monitor_name = f'WEEKLY_SALES_{segment}_MONITOR'\n    \n    print(f\"ðŸ”§ Creating monitor for {segment}...\", end=\" \")\n    \n    try:\n        drop_sql = f\"DROP MODEL MONITOR IF EXISTS ARCA_BEVERAGE_DEMO.MODEL_REGISTRY.{monitor_name}\"\n        session.sql(drop_sql).collect()\n        \n        # Corrected syntax based on documentation\n        create_monitor_sql = f\"\"\"\n        CREATE MODEL MONITOR ARCA_BEVERAGE_DEMO.MODEL_REGISTRY.{monitor_name} WITH\n            MODEL = ARCA_BEVERAGE_DEMO.MODEL_REGISTRY.{model_name}\n            VERSION = 'PRODUCTION'\n            FUNCTION = 'PREDICT'\n            SOURCE = ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_{segment}\n            BASELINE = ARCA_BEVERAGE_DEMO.ML_DATA.BASELINE_{segment}\n            WAREHOUSE = ARCA_DEMO_WH\n            REFRESH_INTERVAL = '1 day'\n            AGGREGATION_WINDOW = '1 day'\n            TIMESTAMP_COLUMN = TIMESTAMP_COL\n            ID_COLUMNS = ('CUSTOMER_ID')\n            PREDICTION_SCORE_COLUMNS = ('PREDICTED_WEEKLY_SALES')\n            ACTUAL_SCORE_COLUMNS = ('ACTUAL_WEEKLY_SALES')\n        \"\"\"\n        \n        session.sql(create_monitor_sql).collect()\n        created_monitors.append(segment)\n        print(\"âœ… Success\")\n        \n    except Exception as e:\n        error_msg = str(e)[:150]\n        print(f\"âš ï¸  {error_msg}\")\n\nprint(f\"\\nâœ… Created {len(created_monitors)}/3 monitors successfully\")\n\nif len(created_monitors) > 0:\n    print(\"\\nðŸ“Š Verifying monitors...\")\n    monitors_check = session.sql(\"\"\"\n        SHOW MODEL MONITORS IN SCHEMA ARCA_BEVERAGE_DEMO.MODEL_REGISTRY\n    \"\"\")\n    monitors_check.show()",
      "id": "d0e1f2a3-b4c5-6789-3456-000000000000"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Query Drift Metrics\n\nDemonstrate drift detection using Jensen-Shannon divergence",
      "id": "e1f2a3b4-c5d6-7890-4567-111111111112"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "if len(created_monitors) > 0:\n    print(\"\\nðŸ“Š Querying Drift Metrics (Jensen-Shannon Divergence)...\\n\")\n    \n    # Wait a moment for monitors to initialize\n    import time\n    print(\"â³ Waiting for monitors to initialize (30 seconds)...\")\n    time.sleep(30)\n    \n    for segment in created_monitors:\n        monitor_name = f'WEEKLY_SALES_{segment}_MONITOR'\n        \n        print(f\"\\nðŸ” Drift Analysis for {segment}:\")\n        \n        try:\n            drift_sql = f\"\"\"\n            SELECT *\n            FROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n                'ARCA_BEVERAGE_DEMO.MODEL_REGISTRY.{monitor_name}',\n                'JENSEN_SHANNON',\n                'CUSTOMER_TOTAL_UNITS_4W',\n                '1 DAY',\n                DATEADD('DAY', -30, CURRENT_TIMESTAMP()),\n                CURRENT_TIMESTAMP()\n            ))\n            LIMIT 5\n            \"\"\"\n            \n            drift_results = session.sql(drift_sql)\n            drift_results.show()\n            \n        except Exception as e:\n            print(f\"   âš ï¸  Drift metrics not yet available: {str(e)[:100]}\")\n            print(f\"   ðŸ’¡ Monitors need time to compute metrics. Check Snowsight UI in 5-10 minutes.\")\nelse:\n    print(\"\\nâš ï¸  No monitors created, skipping drift queries\")",
      "id": "f2a3b4c5-d6e7-8901-5678-222222222223"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Query Performance Metrics\n\nTrack model performance over time (RMSE, MAE)",
      "id": "a3b4c5d6-e7f8-9012-6789-333333333334"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "if len(created_monitors) > 0:\n    print(\"\\nðŸ“ˆ Querying Performance Metrics (RMSE)...\\n\")\n    \n    for segment in created_monitors:\n        monitor_name = f'WEEKLY_SALES_{segment}_MONITOR'\n        \n        print(f\"\\nðŸ“Š Performance for {segment}:\")\n        \n        try:\n            performance_sql = f\"\"\"\n            SELECT *\n            FROM TABLE(MODEL_MONITOR_PERFORMANCE_METRIC(\n                'ARCA_BEVERAGE_DEMO.MODEL_REGISTRY.{monitor_name}',\n                'RMSE',\n                '1 DAY',\n                DATEADD('DAY', -30, CURRENT_DATE()),\n                CURRENT_DATE()\n            ))\n            LIMIT 5\n            \"\"\"\n            \n            performance_results = session.sql(performance_sql)\n            performance_results.show()\n            \n        except Exception as e:\n            print(f\"   âš ï¸  Performance metrics not yet available: {str(e)[:100]}\")\n            print(f\"   ðŸ’¡ Monitors need aggregation time. Check Snowsight UI in 5-10 minutes.\")\nelse:\n    print(\"\\nâš ï¸  No monitors created, skipping performance queries\")",
      "id": "b4c5d6e7-f8a9-0123-7890-444444444445"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Custom Drift Analysis (SQL-based)\n\nManual drift detection for features (baseline vs inference)",
      "id": "c5d6e7f8-a9b0-1234-8901-555555555556"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\nðŸ“Š Custom Feature Distribution Analysis\\n\")\nprint(\"Comparing baseline (training) vs inference distributions\\n\")\n\nfor segment_num in [1, 3, 5]:  # Sample segments\n    segment = f'SEGMENT_{segment_num}'\n    \n    print(f\"\\nðŸ” {segment} - Feature Distribution Comparison:\")\n    \n    distribution_sql = f\"\"\"\n    WITH baseline_stats AS (\n        SELECT\n            'BASELINE' AS DATA_SOURCE,\n            AVG(CUSTOMER_TOTAL_UNITS_4W) AS AVG_UNITS,\n            STDDEV(CUSTOMER_TOTAL_UNITS_4W) AS STDDEV_UNITS,\n            AVG(WEEKS_WITH_PURCHASE) AS AVG_WEEKS,\n            AVG(TRANSACTION_COUNT) AS AVG_TRANSACTIONS\n        FROM ARCA_BEVERAGE_DEMO.ML_DATA.BASELINE_{segment}\n    ),\n    inference_stats AS (\n        SELECT\n            'INFERENCE' AS DATA_SOURCE,\n            AVG(CUSTOMER_TOTAL_UNITS_4W) AS AVG_UNITS,\n            STDDEV(CUSTOMER_TOTAL_UNITS_4W) AS STDDEV_UNITS,\n            AVG(WEEKS_WITH_PURCHASE) AS AVG_WEEKS,\n            AVG(TRANSACTION_COUNT) AS AVG_TRANSACTIONS\n        FROM ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_{segment}\n    )\n    SELECT\n        DATA_SOURCE,\n        ROUND(AVG_UNITS, 2) AS AVG_UNITS,\n        ROUND(STDDEV_UNITS, 2) AS STDDEV_UNITS,\n        ROUND(AVG_WEEKS, 2) AS AVG_WEEKS,\n        ROUND(AVG_TRANSACTIONS, 2) AS AVG_TRANSACTIONS\n    FROM baseline_stats\n    UNION ALL\n    SELECT\n        DATA_SOURCE,\n        ROUND(AVG_UNITS, 2),\n        ROUND(STDDEV_UNITS, 2),\n        ROUND(AVG_WEEKS, 2),\n        ROUND(AVG_TRANSACTIONS, 2)\n    FROM inference_stats\n    \"\"\"\n    \n    session.sql(distribution_sql).show()",
      "id": "d6e7f8a9-b0c1-2345-9012-666666666667"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Performance Degradation Detection\n\nIdentify segments where model performance has degraded",
      "id": "e7f8a9b0-c1d2-3456-0123-777777777778"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "performance_comparison = session.sql(\"\"\"\nWITH inference_performance AS (\n    SELECT\n        SEGMENT,\n        AVG(ABS(PREDICTION_ERROR)) AS MAE,\n        SQRT(AVG(POWER(PREDICTION_ERROR, 2))) AS RMSE,\n        AVG(ABS(PREDICTION_ERROR / NULLIF(ACTUAL_WEEKLY_SALES, 0))) * 100 AS MAPE_PCT,\n        COUNT(*) AS PREDICTION_COUNT\n    FROM ARCA_BEVERAGE_DEMO.ML_DATA.INFERENCE_LOGS\n    GROUP BY SEGMENT\n)\nSELECT\n    SEGMENT,\n    PREDICTION_COUNT,\n    ROUND(MAE, 2) AS MAE,\n    ROUND(RMSE, 2) AS RMSE,\n    ROUND(MAPE_PCT, 1) AS MAPE_PCT,\n    CASE\n        WHEN RMSE > 1.0 THEN 'ðŸ”´ HIGH ERROR'\n        WHEN RMSE > 0.5 THEN 'ðŸŸ¡ MODERATE ERROR'\n        ELSE 'ðŸŸ¢ LOW ERROR'\n    END AS PERFORMANCE_STATUS\nFROM inference_performance\nORDER BY RMSE DESC\n\"\"\")\n\nprint(\"\\nðŸ“Š Model Performance Status by Segment:\\n\")\nperformance_comparison.show()",
      "id": "f8a9b0c1-d2e3-4567-1234-888888888889"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Alert Threshold Configuration\n\nDefine alerting rules (example - would be automated in production)",
      "id": "a9b0c1d2-e3f4-5678-2345-999999999990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\nðŸš¨ Alert Threshold Configuration\\n\")\nprint(\"=\"*60)\n\nalert_config = {\n    'drift': {\n        'warning': 0.2,\n        'critical': 0.4,\n        'metric': 'Jensen-Shannon Divergence'\n    },\n    'performance': {\n        'warning': 0.5,\n        'critical': 1.0,\n        'metric': 'RMSE'\n    },\n    'mape': {\n        'warning': 15.0,\n        'critical': 25.0,\n        'metric': 'MAPE (%)'\n    }\n}\n\nfor alert_type, config in alert_config.items():\n    print(f\"\\n{alert_type.upper()} Alerts:\")\n    print(f\"  Metric: {config['metric']}\")\n    print(f\"  âš ï¸  Warning: > {config['warning']}\")\n    print(f\"  ðŸ”´ Critical: > {config['critical']}\")\n\nprint(\"\\nðŸ’¡ Production Setup:\")\nprint(\"  1. Configure Snowflake Alerts on monitor tables\")\nprint(\"  2. Set up email/Slack notifications\")\nprint(\"  3. Create automated retraining triggers\")\nprint(\"  4. Dashboard in Snowsight for real-time monitoring\")\n\nprint(\"\\n\" + \"=\"*60)",
      "id": "b0c1d2e3-f4a5-6789-3456-000000000001"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 10. Snowsight Navigation Guide\n\nHow to access ML Observability dashboards in Snowsight UI",
      "id": "c1d2e3f4-a5b6-7890-4567-111111111113"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\nðŸ“± SNOWSIGHT UI NAVIGATION GUIDE\")\nprint(\"=\"*80)\n\nprint(\"\\nðŸŽ¯ How to View ML Observability Dashboards:\\n\")\n\nsteps = [\n    (\"1. Navigate to Model Registry\", \"Snowsight â†’ AI & ML â†’ Models\"),\n    (\"2. Select Model\", \"Click on 'WEEKLY_SALES_FORECAST_SEGMENT_1'\"),\n    (\"3. View Monitors Tab\", \"Click 'Monitors' tab in model details\"),\n    (\"4. Open Monitor Dashboard\", \"Click monitor name to see drift/performance charts\"),\n    (\"5. Customize Time Range\", \"Use date picker to adjust monitoring window\"),\n    (\"6. View Specific Metrics\", \"Click Settings to toggle metrics display\")\n]\n\nfor step, instruction in steps:\n    print(f\"   {step}\")\n    print(f\"      â†’ {instruction}\\n\")\n\nprint(\"ðŸ’¡ Key Dashboard Features:\")\nprint(\"   - Drift metrics over time (Jensen-Shannon divergence)\")\nprint(\"   - Performance metrics (RMSE, MAE, MAPE)\")\nprint(\"   - Feature distribution comparison\")\nprint(\"   - Prediction volume tracking\")\nprint(\"   - Automated refresh (real-time updates)\")\n\nprint(\"\\nðŸ”— Direct SQL Access:\")\nprint(\"   - MODEL_MONITOR_DRIFT_METRIC()\")\nprint(\"   - MODEL_MONITOR_PERFORMANCE_METRIC()\")\nprint(\"   - MODEL_MONITOR_STAT_METRIC()\")\n\nprint(\"\\n\" + \"=\"*80)",
      "id": "d2e3f4a5-b6c7-8901-5678-222222222224"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 11. Summary & Key Takeaways",
      "id": "e3f4a5b6-c7d8-9012-6789-333333333335"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"ðŸŽ‰ ML OBSERVABILITY SETUP COMPLETE!\")\nprint(\"=\"*80)\n\nprint(\"\\nâœ… Completed Setup:\")\nprint(f\"   - Model Monitors: {len(created_monitors)} segments\")\nprint(f\"   - Baseline Tables: 6 segments\")\nprint(f\"   - Inference Tables: 6 segments\")\nprint(\"   - Alert Thresholds: Configured\")\nprint(\"   - Snowsight Dashboards: Available\")\n\nprint(\"\\nðŸ“Š Monitoring Capabilities:\")\nprint(\"   âœ… Drift Detection (Jensen-Shannon divergence)\")\nprint(\"   âœ… Performance Tracking (RMSE, MAE, MAPE)\")\nprint(\"   âœ… Feature Distribution Analysis\")\nprint(\"   âœ… Automated Alerting (configurable thresholds)\")\n\nprint(\"\\nðŸ’¡ Key Business Messages:\")\nprint(\"   - Replaces manual drift detection with automation\")\nprint(\"   - Real-time monitoring vs weekly/monthly checks\")\nprint(\"   - Snowflake-native (no external tools required)\")\nprint(\"   - Catches issues early before impacting business\")\n\nprint(\"\\nðŸŽ¯ Demo Talking Points:\")\nprint(\"   1. Show Snowsight dashboard with drift charts\")\nprint(\"   2. Explain JS divergence > 0.4 triggers alert\")\nprint(\"   3. Compare baseline vs inference distributions\")\nprint(\"   4. Highlight automated vs manual monitoring savings\")\n\nprint(\"\\nðŸš€ Production Next Steps:\")\nprint(\"   - Configure Snowflake Alerts for automated notifications\")\nprint(\"   - Set up email/Slack integration\")\nprint(\"   - Create model retraining triggers on drift\")\nprint(\"   - Expand to all 6 segments (currently 3 demo)\")\nprint(\"   - Add custom business metrics to dashboards\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nðŸŽ“ DEMO COMPLETE - All 6 Notebooks Executed Successfully!\")\nprint(\"\\nðŸ“‹ Full ML Workflow Demonstrated:\")\nprint(\"   01 âœ… Data Setup\")\nprint(\"   02 âœ… Feature Store (3 refresh frequencies)\")\nprint(\"   03 âœ… Customer Segmentation (6 segments)\")\nprint(\"   04 âœ… Many Model Training (6 models, 6x speedup)\")\nprint(\"   05 âœ… Partitioned Inference (batch predictions)\")\nprint(\"   06 âœ… ML Observability (automated monitoring)\")\nprint(\"\\n\" + \"=\"*80)",
      "id": "f4a5b6c7-d8e9-0123-7890-444444444446"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}