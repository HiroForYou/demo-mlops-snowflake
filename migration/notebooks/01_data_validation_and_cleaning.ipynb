{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n#\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n#\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "e68a272e-af17-4528-977c-8e3d0a693bbe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "e7076ab5-7aef-40cd-8ae7-edf60ff5b901",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "45d698a0-e6d3-40e0-b46f-166a4b6623d2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\" * 80)\n\n# Check if table exists\ntry:\n    train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\")\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_lower = [col.lower() for col in columns]\nif \"uni_box_week\" in columns_lower:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_lower.index(\"uni_box_week\")]\n    print(f\"\\n‚úÖ Target variable 'uni_box_week' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable 'uni_box_week' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(\"Target variable 'uni_box_week' is required\")\n\n# Store target column name for later use\nTARGET_COLUMN = target_col\n",
      "id": "6b6d3a64-ae4f-4cf5-bfa8-54f775fb0386",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "68c7d939-0c7e-45e2-871e-d18e90bc0de2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\" * 80)\n\ntry:\n    inference_df = session.table(\n        \"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\"\n    )\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_lower = [col.lower() for col in inference_columns]\nif \"uni_box_week\" in inference_columns_lower:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable 'uni_box_week' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "2106006b-9d8e-4264-9234-fd5c5496e3fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "ce7f94a3-187a-49a6-85bf-2b441dbaa877"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\" * 80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\n    \"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN uni_box_week IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN week IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\n    \"\"\"\n    SELECT\n        SUM(CASE WHEN sum_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_sum_past_12_weeks,\n        SUM(CASE WHEN avg_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_avg_past_12_weeks,\n        SUM(CASE WHEN week_of_year IS NULL THEN 1 ELSE 0 END) AS NULL_week_of_year,\n        SUM(CASE WHEN stats_ntile_group IS NULL THEN 1 ELSE 0 END) AS NULL_stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "a2302640-3400-4b5a-9a94-357e5527e2a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "fc31d29f-b162-44e8-9340-63288cc251ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\" * 80)\n\ntarget_stats = session.sql(\n    \"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT uni_box_week) AS UNIQUE_VALUES,\n        MIN(uni_box_week) AS MIN_VALUE,\n        MAX(uni_box_week) AS MAX_VALUE,\n        AVG(uni_box_week) AS MEAN_VALUE,\n        STDDEV(uni_box_week) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY uni_box_week) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY uni_box_week) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY uni_box_week) AS Q3\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    WHERE uni_box_week IS NOT NULL\n\"\"\"\n)\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\n    \"\"\"\n    WITH stats AS (\n        SELECT\n            AVG(uni_box_week) AS mean_val,\n            STDDEV(uni_box_week) AS stddev_val\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN(uni_box_week) AS MIN_OUTLIER,\n        MAX(uni_box_week) AS MAX_OUTLIER\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED, stats\n    WHERE uni_box_week IS NOT NULL\n        AND (uni_box_week < mean_val - 3 * stddev_val \n             OR uni_box_week > mean_val + 3 * stddev_val)\n\"\"\"\n)\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "122198c9-f70d-425d-bde3-dbdbe04fe12f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "7867feaf-1637-46c0-af32-ea1a5ad2b94b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\" * 80)\n\n# Define excluded columns\nexcluded_cols = [\n    \"customer_id\",\n    \"brand_pres_ret\",\n    \"week\",\n    \"group\",\n    \"stats_group\",\n    \"percentile_group\",\n    \"stats_ntile_group\",\n]\n\n# Get feature columns from training (exclude target + excluded)\ntrain_feature_cols = [\n    col for col in columns if col not in excluded_cols and col != TARGET_COLUMN\n]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [col for col in inference_columns if col not in excluded_cols]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "1b0da49e-9ec0-41fd-ae43-f405e0928050",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "c47ad752-cd3f-4a55-9261-33476ef0ae64"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\" * 80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\nWHERE uni_box_week IS NOT NULL\n    AND customer_id IS NOT NULL\n    AND week IS NOT NULL\n    -- Remove extreme outliers (optional - adjust threshold as needed)\n    AND uni_box_week >= 0\n    AND uni_box_week <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY uni_box_week)\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(\n    \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n).count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\nWHERE customer_id IS NOT NULL\n    AND week IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(\n    \"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\"\n).count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "066b6e67-6bbf-475d-b678-d3ab3af370e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Validate stats_ntile_group Segmentation\n",
      "id": "3593544f-04fd-43d2-beb7-99abc9ee7aa9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç VALIDATING stats_ntile_group SEGMENTATION\")\nprint(\"=\" * 80)\n\n# Check if stats_ntile_group exists\nif \"stats_ntile_group\" not in columns_lower:\n    print(\"\\n‚ùå ERROR: Column 'stats_ntile_group' NOT found in training dataset!\")\n    print(\"   This column is required for 16-group model training.\")\n    raise ValueError(\"stats_ntile_group column is required\")\n\n# Find actual column name (preserving case)\nstats_ntile_col = columns[columns_lower.index(\"stats_ntile_group\")]\n\n# Get unique groups\ngroups_df = session.sql(\n    f\"\"\"\n    SELECT \n        {stats_ntile_col} AS GROUP_NAME,\n        COUNT(*) AS RECORD_COUNT,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        AVG(uni_box_week) AS AVG_TARGET,\n        MIN(uni_box_week) AS MIN_TARGET,\n        MAX(uni_box_week) AS MAX_TARGET\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE {stats_ntile_col} IS NOT NULL\n    GROUP BY {stats_ntile_col}\n    ORDER BY {stats_ntile_col}\n\"\"\"\n)\n\nprint(\"\\nüìä Group Distribution:\")\ngroups_df.show()\n\n# Get group count\ngroup_count = groups_df.count()\nprint(f\"\\nüìä Total unique groups: {group_count}\")\n\n# Validate we have exactly 16 groups\nif group_count != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {group_count}\")\n    print(\"   This may affect model training. Please verify segmentation logic.\")\nelse:\n    print(f\"\\n‚úÖ Validation passed: Exactly 16 groups found\")\n\n# Check minimum records per group (recommend at least 100)\nmin_records_check = session.sql(\n    f\"\"\"\n    SELECT \n        MIN(RECORD_COUNT) AS MIN_RECORDS,\n        MAX(RECORD_COUNT) AS MAX_RECORDS,\n        AVG(RECORD_COUNT) AS AVG_RECORDS\n    FROM (\n        SELECT \n            {stats_ntile_col},\n            COUNT(*) AS RECORD_COUNT\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n        WHERE {stats_ntile_col} IS NOT NULL\n        GROUP BY {stats_ntile_col}\n    )\n\"\"\"\n)\n\nprint(\"\\nüìä Records per Group Statistics:\")\nmin_records_check.show()\n\nmin_records_result = min_records_check.collect()[0]\nmin_records = min_records_result[\"MIN_RECORDS\"]\n\nif min_records < 100:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Some groups have less than 100 records (minimum: {min_records})\")\n    print(\"   This may affect model training quality.\")\nelse:\n    print(f\"\\n‚úÖ All groups have sufficient data (minimum: {min_records} records)\")\n",
      "id": "9b281c29-8d8a-441a-ba0f-050f8fdaca8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Summary Statistics\n",
      "id": "c4d4258d-327b-49d0-9092-b0fbeb508720"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\" * 80)\n\nsummary = session.sql(\n    \"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\n\"\"\"\n)\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Validation Summary:\")\nprint(f\"   ‚úÖ Training data validated: {cleaned_train_count:,} rows\")\nprint(f\"   ‚úÖ Inference data validated: {cleaned_inference_count:,} rows\")\nprint(f\"   ‚úÖ stats_ntile_group validated: {group_count} groups\")\nprint(f\"   ‚úÖ Minimum records per group: {min_records}\")\n\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables and group distribution\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\nprint(\"   3. Run 03_hyperparameter_search.py to find optimal hyperparameters per group\")\n",
      "id": "a518bbf9-2877-4038-b825-60abd8efe128",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}