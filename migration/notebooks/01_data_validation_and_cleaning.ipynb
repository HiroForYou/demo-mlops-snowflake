{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n#\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n#\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "c68bc2b1-3a96-4897-865e-b4f2a5195976"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Configuration: Database, schemas, and tables\nDATABASE = \"BD_AA_DEV\"\nSTORAGE_SCHEMA = \"SC_STORAGE_BMX_PS\"\nTRAIN_TABLE_STRUCTURED = f\"{DATABASE}.{STORAGE_SCHEMA}.TRAIN_DATASET_STRUCTURED\"\nINFERENCE_TABLE_STRUCTURED = f\"{DATABASE}.{STORAGE_SCHEMA}.INFERENCE_DATASET_STRUCTURED\"\nTRAIN_TABLE_CLEANED = f\"{DATABASE}.{STORAGE_SCHEMA}.TRAIN_DATASET_CLEANED\"\nINFERENCE_TABLE_CLEANED = f\"{DATABASE}.{STORAGE_SCHEMA}.INFERENCE_DATASET_CLEANED\"\n\n# Column constants\nTARGET_COLUMN = \"UNI_BOX_WEEK\"\nSTATS_NTILE_GROUP_COL = \"STATS_NTILE_GROUP\"\n\n# Excluded columns (metadata columns, not features) - defined once at the beginning\nEXCLUDED_COLS = [\n    \"CUSTOMER_ID\",\n    \"BRAND_PRES_RET\",\n    \"WEEK\",\n    \"GROUP\",\n    \"STATS_GROUP\",\n    \"PERCENTILE_GROUP\",\n    STATS_NTILE_GROUP_COL,\n    \"PROD_KEY\",\n]\n\n# Set context\nsession.sql(f\"USE DATABASE {DATABASE}\").collect()\nsession.sql(f\"USE SCHEMA {STORAGE_SCHEMA}\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "e0049456-b3fd-4a1c-88cd-ac44695c3128",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "20d27b42-1c43-4cd3-ab1c-527ada97166b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\" * 80)\n\n# Check if table exists\ntry:\n    train_df = session.table(TRAIN_TABLE_STRUCTURED)\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_upper = [col.upper() for col in columns]\nif TARGET_COLUMN in columns_upper:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_upper.index(TARGET_COLUMN)]\n    print(f\"\\n‚úÖ Target variable '{TARGET_COLUMN}' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable '{TARGET_COLUMN}' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(f\"Target variable '{TARGET_COLUMN}' is required\")\n",
      "id": "bb2bb94d-39fe-4471-a75f-594fae2fcc96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "3ea86e0f-23a6-4db7-b1a0-c2bac8098beb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\" * 80)\n\ntry:\n    inference_df = session.table(INFERENCE_TABLE_STRUCTURED)\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_upper = [col.upper() for col in inference_columns]\nif TARGET_COLUMN in inference_columns_upper:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable '{TARGET_COLUMN}' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "e68433a3-965a-4264-b4af-eb1448e0b025",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "281c3dff-7382-4df5-9007-eb9c4382ffe5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\" * 80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\n    f\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN {TARGET_COLUMN} IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN CUSTOMER_ID IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN WEEK IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM {TRAIN_TABLE_STRUCTURED}\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\n    f\"\"\"\n    SELECT\n        SUM(CASE WHEN SUM_PAST_12_WEEKS IS NULL THEN 1 ELSE 0 END) AS NULL_SUM_PAST_12_WEEKS,\n        SUM(CASE WHEN AVG_PAST_12_WEEKS IS NULL THEN 1 ELSE 0 END) AS NULL_AVG_PAST_12_WEEKS,\n        SUM(CASE WHEN WEEK_OF_YEAR IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK_OF_YEAR,\n        SUM(CASE WHEN STATS_NTILE_GROUP IS NULL THEN 1 ELSE 0 END) AS NULL_STATS_NTILE_GROUP\n    FROM {TRAIN_TABLE_STRUCTURED}\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "cdf79347-b1d9-44d3-9842-96088a1a4a0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "5fcc6927-a5a3-4aca-8bde-12f09b11664e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\" * 80)\n\ntarget_stats = session.sql(\n    f\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT {TARGET_COLUMN}) AS UNIQUE_VALUES,\n        MIN({TARGET_COLUMN}) AS MIN_VALUE,\n        MAX({TARGET_COLUMN}) AS MAX_VALUE,\n        AVG({TARGET_COLUMN}) AS MEAN_VALUE,\n        STDDEV({TARGET_COLUMN}) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {TARGET_COLUMN}) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY {TARGET_COLUMN}) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {TARGET_COLUMN}) AS Q3\n    FROM {TRAIN_TABLE_STRUCTURED}\n    WHERE {TARGET_COLUMN} IS NOT NULL\n\"\"\"\n)\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\n    f\"\"\"\n    WITH stats AS (\n        SELECT\n            AVG({TARGET_COLUMN}) AS mean_val,\n            STDDEV({TARGET_COLUMN}) AS stddev_val\n        FROM {TRAIN_TABLE_STRUCTURED}\n        WHERE {TARGET_COLUMN} IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN({TARGET_COLUMN}) AS MIN_OUTLIER,\n        MAX({TARGET_COLUMN}) AS MAX_OUTLIER\n    FROM {TRAIN_TABLE_STRUCTURED}, stats\n    WHERE {TARGET_COLUMN} IS NOT NULL\n        AND ({TARGET_COLUMN} < mean_val - 3 * stddev_val \n             OR {TARGET_COLUMN} > mean_val + 3 * stddev_val)\n\"\"\"\n)\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "1a890fbc-1a5a-43a5-9636-1aa43b7d8299",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "f22c3dd0-3dc9-492c-89d7-c27b45192e92"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\" * 80)\n\n# Get feature columns from training (exclude target + excluded)\n# EXCLUDED_COLS is already in UPPER CASE, so we compare case-insensitively\nexcluded_cols_upper = {col for col in EXCLUDED_COLS}\ntrain_feature_cols = [\n    col for col in columns \n    if col.upper() not in excluded_cols_upper and col.upper() != TARGET_COLUMN\n]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [\n    col for col in inference_columns \n    if col.upper() not in excluded_cols_upper\n]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "08ea7152-03e3-4c81-974a-d685dcef7fa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "896f91a8-b154-443d-95b5-49ae4910bccc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\" * 80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = f\"\"\"\nCREATE OR REPLACE TABLE {TRAIN_TABLE_CLEANED} AS\nSELECT *\nFROM {TRAIN_TABLE_STRUCTURED}\nWHERE {TARGET_COLUMN} IS NOT NULL\n    AND CUSTOMER_ID IS NOT NULL\n    AND WEEK IS NOT NULL\n    AND {TARGET_COLUMN} >= 0\n    AND {TARGET_COLUMN} <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY {TARGET_COLUMN})\n        FROM {TRAIN_TABLE_STRUCTURED}\n        WHERE {TARGET_COLUMN} IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(TRAIN_TABLE_CLEANED).count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = f\"\"\"\nCREATE OR REPLACE TABLE {INFERENCE_TABLE_CLEANED} AS\nSELECT *\nFROM {INFERENCE_TABLE_STRUCTURED}\nWHERE CUSTOMER_ID IS NOT NULL\n    AND WEEK IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(INFERENCE_TABLE_CLEANED).count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "7c474c1d-424d-4726-a071-f5e6ae3cbc53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Validate stats_ntile_group Segmentation\n",
      "id": "ffbf45b8-0ca0-4e5f-aaf4-fcd4f20dc652"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç VALIDATING stats_ntile_group SEGMENTATION\")\nprint(\"=\" * 80)\n\n# Check if STATS_NTILE_GROUP exists\nif STATS_NTILE_GROUP_COL not in columns_upper:\n    print(f\"\\n‚ùå ERROR: Column '{STATS_NTILE_GROUP_COL}' NOT found in training dataset!\")\n    print(\"   This column is required for 16-group model training.\")\n    raise ValueError(f\"{STATS_NTILE_GROUP_COL} column is required\")\n\n# Find actual column name (preserving case)\nstats_ntile_col = columns[columns_upper.index(STATS_NTILE_GROUP_COL)]\n\n# Get unique groups\ngroups_df = session.sql(\n    f\"\"\"\n    SELECT \n        {stats_ntile_col} AS GROUP_NAME,\n        COUNT(*) AS RECORD_COUNT,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        AVG({TARGET_COLUMN}) AS AVG_TARGET,\n        MIN({TARGET_COLUMN}) AS MIN_TARGET,\n        MAX({TARGET_COLUMN}) AS MAX_TARGET\n    FROM {TRAIN_TABLE_CLEANED}\n    WHERE {stats_ntile_col} IS NOT NULL\n    GROUP BY {stats_ntile_col}\n    ORDER BY {stats_ntile_col}\n\"\"\"\n)\n\nprint(\"\\nüìä Group Distribution:\")\ngroups_df.show()\n\n# Get group count\ngroup_count = groups_df.count()\nprint(f\"\\nüìä Total unique groups: {group_count}\")\n\n# Validate we have exactly 16 groups\nif group_count != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {group_count}\")\n    print(\"   This may affect model training. Please verify segmentation logic.\")\nelse:\n    print(f\"\\n‚úÖ Validation passed: Exactly 16 groups found\")\n\n# Check minimum records per group (recommend at least 100)\nmin_records_check = session.sql(\n    f\"\"\"\n    SELECT \n        MIN(RECORD_COUNT) AS MIN_RECORDS,\n        MAX(RECORD_COUNT) AS MAX_RECORDS,\n        AVG(RECORD_COUNT) AS AVG_RECORDS\n    FROM (\n        SELECT \n            {stats_ntile_col},\n            COUNT(*) AS RECORD_COUNT\n        FROM {TRAIN_TABLE_CLEANED}\n        WHERE {stats_ntile_col} IS NOT NULL\n        GROUP BY {stats_ntile_col}\n    )\n\"\"\"\n)\n\nprint(\"\\nüìä Records per Group Statistics:\")\nmin_records_check.show()\n\nmin_records_result = min_records_check.collect()[0]\nmin_records = min_records_result[\"MIN_RECORDS\"]\n\nif min_records < 100:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Some groups have less than 100 records (minimum: {min_records})\")\n    print(\"   This may affect model training quality.\")\nelse:\n    print(f\"\\n‚úÖ All groups have sufficient data (minimum: {min_records} records)\")\n",
      "id": "10cc3dca-1070-47ac-a986-2106fd885f6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Summary Statistics\n",
      "id": "7820f462-a94a-4910-a39f-6e31c403262c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\" * 80)\n\nsummary = session.sql(\n    f\"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {TRAIN_TABLE_STRUCTURED}\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {TRAIN_TABLE_CLEANED}\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {INFERENCE_TABLE_STRUCTURED}\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {INFERENCE_TABLE_CLEANED}\n\"\"\"\n)\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Validation Summary:\")\nprint(f\"   ‚úÖ Training data validated: {cleaned_train_count:,} rows\")\nprint(f\"   ‚úÖ Inference data validated: {cleaned_inference_count:,} rows\")\nprint(f\"   ‚úÖ stats_ntile_group validated: {group_count} groups\")\nprint(f\"   ‚úÖ Minimum records per group: {min_records}\")\n\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables and group distribution\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\nprint(\"   3. Run 03_hyperparameter_search.py to find optimal hyperparameters per group\")\n",
      "id": "4588ff8d-b741-4fd2-9c21-c1a7628ee7db",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}