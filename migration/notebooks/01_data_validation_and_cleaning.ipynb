{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n#\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n#\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "054f4971-d859-4c95-a6c2-a14addc0a019"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Configuration: Database, schemas, and tables\nDATABASE = \"BD_AA_DEV\"\nSTORAGE_SCHEMA = \"SC_STORAGE_BMX_PS\"\nTRAIN_TABLE_STRUCTURED = f\"{DATABASE}.{STORAGE_SCHEMA}.TRAIN_DATASET_STRUCTURED\"\nINFERENCE_TABLE_STRUCTURED = f\"{DATABASE}.{STORAGE_SCHEMA}.INFERENCE_DATASET_STRUCTURED\"\nTRAIN_TABLE_CLEANED = f\"{DATABASE}.{STORAGE_SCHEMA}.TRAIN_DATASET_CLEANED\"\nINFERENCE_TABLE_CLEANED = f\"{DATABASE}.{STORAGE_SCHEMA}.INFERENCE_DATASET_CLEANED\"\n\n# Column constants\nTARGET_COLUMN = \"UNI_BOX_WEEK\"\nSTATS_NTILE_GROUP_COL = \"STATS_NTILE_GROUP\"\n\n# Excluded columns (metadata columns, not features) - defined once at the beginning\nEXCLUDED_COLS = [\n    \"CUSTOMER_ID\",\n    \"BRAND_PRES_RET\",\n    \"WEEK\",\n    \"GROUP\",\n    \"STATS_GROUP\",\n    \"PERCENTILE_GROUP\",\n    STATS_NTILE_GROUP_COL,\n    \"PROD_KEY\",\n]\n\n# Set context\nsession.sql(f\"USE DATABASE {DATABASE}\").collect()\nsession.sql(f\"USE SCHEMA {STORAGE_SCHEMA}\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "b0feadc4-3b85-4204-bd5e-29339e39e401",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "bbefd8bb-67ad-4e39-82a9-a5e4dcd73c20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\" * 80)\n\n# Check if table exists\ntry:\n    train_df = session.table(TRAIN_TABLE_STRUCTURED)\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_upper = [col.upper() for col in columns]\nif TARGET_COLUMN in columns_upper:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_upper.index(TARGET_COLUMN)]\n    print(f\"\\n‚úÖ Target variable '{TARGET_COLUMN}' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable '{TARGET_COLUMN}' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(f\"Target variable '{TARGET_COLUMN}' is required\")\n",
      "id": "52b46d81-86ac-440c-94d9-19b1c1fc642f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "5b877bf0-f573-405b-89c8-69fdb00cfb20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\" * 80)\n\ntry:\n    inference_df = session.table(INFERENCE_TABLE_STRUCTURED)\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_upper = [col.upper() for col in inference_columns]\nif TARGET_COLUMN in inference_columns_upper:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable '{TARGET_COLUMN}' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "cdf23145-5649-42c9-a6bb-883061af9c2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "b1aa71a6-614f-4140-98ad-ed6bfe51babc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\" * 80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\n    f\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN {TARGET_COLUMN} IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN CUSTOMER_ID IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN WEEK IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM {TRAIN_TABLE_STRUCTURED}\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\n    f\"\"\"\n    SELECT\n        SUM(CASE WHEN SUM_PAST_12_WEEKS IS NULL THEN 1 ELSE 0 END) AS NULL_SUM_PAST_12_WEEKS,\n        SUM(CASE WHEN AVG_PAST_12_WEEKS IS NULL THEN 1 ELSE 0 END) AS NULL_AVG_PAST_12_WEEKS,\n        SUM(CASE WHEN WEEK_OF_YEAR IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK_OF_YEAR,\n        SUM(CASE WHEN STATS_NTILE_GROUP IS NULL THEN 1 ELSE 0 END) AS NULL_STATS_NTILE_GROUP\n    FROM {TRAIN_TABLE_STRUCTURED}\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "3a07f20b-f0ae-4511-bd6f-b1d2e4098008",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "e0e36c12-4121-4b48-aca9-46b7aa5ed05f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\" * 80)\n\ntarget_stats = session.sql(\n    f\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT {TARGET_COLUMN}) AS UNIQUE_VALUES,\n        MIN({TARGET_COLUMN}) AS MIN_VALUE,\n        MAX({TARGET_COLUMN}) AS MAX_VALUE,\n        AVG({TARGET_COLUMN}) AS MEAN_VALUE,\n        STDDEV({TARGET_COLUMN}) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {TARGET_COLUMN}) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY {TARGET_COLUMN}) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {TARGET_COLUMN}) AS Q3\n    FROM {TRAIN_TABLE_STRUCTURED}\n    WHERE {TARGET_COLUMN} IS NOT NULL\n\"\"\"\n)\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\n    f\"\"\"\n    WITH stats AS (\n        SELECT\n            AVG({TARGET_COLUMN}) AS mean_val,\n            STDDEV({TARGET_COLUMN}) AS stddev_val\n        FROM {TRAIN_TABLE_STRUCTURED}\n        WHERE {TARGET_COLUMN} IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN({TARGET_COLUMN}) AS MIN_OUTLIER,\n        MAX({TARGET_COLUMN}) AS MAX_OUTLIER\n    FROM {TRAIN_TABLE_STRUCTURED}, stats\n    WHERE {TARGET_COLUMN} IS NOT NULL\n        AND ({TARGET_COLUMN} < mean_val - 3 * stddev_val \n             OR {TARGET_COLUMN} > mean_val + 3 * stddev_val)\n\"\"\"\n)\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "55600150-ee80-4995-b519-1431162f1b5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "d80bd1bf-abed-4df4-9617-72a2b7e23df5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\" * 80)\n\n# Get feature columns from training (exclude target + excluded)\n# EXCLUDED_COLS is already in UPPER CASE, so we compare case-insensitively\nexcluded_cols_upper = {col for col in EXCLUDED_COLS}\ntrain_feature_cols = [\n    col for col in columns \n    if col.upper() not in excluded_cols_upper and col.upper() != TARGET_COLUMN\n]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [\n    col for col in inference_columns \n    if col.upper() not in excluded_cols_upper\n]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "0b2e6a15-2634-4bfd-9cae-a504e622767a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "da423043-ffef-4098-b907-43ea8c52f2bb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\" * 80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = f\"\"\"\nCREATE OR REPLACE TABLE {TRAIN_TABLE_CLEANED} AS\nSELECT *\nFROM {TRAIN_TABLE_STRUCTURED}\nWHERE {TARGET_COLUMN} IS NOT NULL\n    AND CUSTOMER_ID IS NOT NULL\n    AND WEEK IS NOT NULL\n    AND {TARGET_COLUMN} >= 0\n    AND {TARGET_COLUMN} <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY {TARGET_COLUMN})\n        FROM {TRAIN_TABLE_STRUCTURED}\n        WHERE {TARGET_COLUMN} IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(TRAIN_TABLE_CLEANED).count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = f\"\"\"\nCREATE OR REPLACE TABLE {INFERENCE_TABLE_CLEANED} AS\nSELECT *\nFROM {INFERENCE_TABLE_STRUCTURED}\nWHERE CUSTOMER_ID IS NOT NULL\n    AND WEEK IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(INFERENCE_TABLE_CLEANED).count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "5a58a3fe-96b1-4c57-9420-c9fd065b1f09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Validate stats_ntile_group Segmentation\n",
      "id": "ec6cc84e-e453-4edd-b446-bc0dafc96459"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç VALIDATING stats_ntile_group SEGMENTATION\")\nprint(\"=\" * 80)\n\n# Check if STATS_NTILE_GROUP exists\nif STATS_NTILE_GROUP_COL not in columns_upper:\n    print(f\"\\n‚ùå ERROR: Column '{STATS_NTILE_GROUP_COL}' NOT found in training dataset!\")\n    print(\"   This column is required for 16-group model training.\")\n    raise ValueError(f\"{STATS_NTILE_GROUP_COL} column is required\")\n\n# Find actual column name (preserving case)\nstats_ntile_col = columns[columns_upper.index(STATS_NTILE_GROUP_COL)]\n\n# Get unique groups\ngroups_df = session.sql(\n    f\"\"\"\n    SELECT \n        {stats_ntile_col} AS GROUP_NAME,\n        COUNT(*) AS RECORD_COUNT,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        AVG({TARGET_COLUMN}) AS AVG_TARGET,\n        MIN({TARGET_COLUMN}) AS MIN_TARGET,\n        MAX({TARGET_COLUMN}) AS MAX_TARGET\n    FROM {TRAIN_TABLE_CLEANED}\n    WHERE {stats_ntile_col} IS NOT NULL\n    GROUP BY {stats_ntile_col}\n    ORDER BY {stats_ntile_col}\n\"\"\"\n)\n\nprint(\"\\nüìä Group Distribution:\")\ngroups_df.show()\n\n# Get group count\ngroup_count = groups_df.count()\nprint(f\"\\nüìä Total unique groups: {group_count}\")\n\n# Validate we have exactly 16 groups\nif group_count != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {group_count}\")\n    print(\"   This may affect model training. Please verify segmentation logic.\")\nelse:\n    print(f\"\\n‚úÖ Validation passed: Exactly 16 groups found\")\n\n# Check minimum records per group (recommend at least 100)\nmin_records_check = session.sql(\n    f\"\"\"\n    SELECT \n        MIN(RECORD_COUNT) AS MIN_RECORDS,\n        MAX(RECORD_COUNT) AS MAX_RECORDS,\n        AVG(RECORD_COUNT) AS AVG_RECORDS\n    FROM (\n        SELECT \n            {stats_ntile_col},\n            COUNT(*) AS RECORD_COUNT\n        FROM {TRAIN_TABLE_CLEANED}\n        WHERE {stats_ntile_col} IS NOT NULL\n        GROUP BY {stats_ntile_col}\n    )\n\"\"\"\n)\n\nprint(\"\\nüìä Records per Group Statistics:\")\nmin_records_check.show()\n\nmin_records_result = min_records_check.collect()[0]\nmin_records = min_records_result[\"MIN_RECORDS\"]\n\nif min_records < 100:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Some groups have less than 100 records (minimum: {min_records})\")\n    print(\"   This may affect model training quality.\")\nelse:\n    print(f\"\\n‚úÖ All groups have sufficient data (minimum: {min_records} records)\")\n",
      "id": "2547a4f5-3f10-4828-9632-b13fe57b45d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Summary Statistics\n",
      "id": "42ce7728-264e-4b5e-ae61-7b2352549798"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\" * 80)\n\nsummary = session.sql(\n    f\"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {TRAIN_TABLE_STRUCTURED}\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {TRAIN_TABLE_CLEANED}\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {INFERENCE_TABLE_STRUCTURED}\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT CUSTOMER_ID) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT WEEK) AS UNIQUE_WEEKS\n    FROM {INFERENCE_TABLE_CLEANED}\n\"\"\"\n)\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Validation Summary:\")\nprint(f\"   ‚úÖ Training data validated: {cleaned_train_count:,} rows\")\nprint(f\"   ‚úÖ Inference data validated: {cleaned_inference_count:,} rows\")\nprint(f\"   ‚úÖ stats_ntile_group validated: {group_count} groups\")\nprint(f\"   ‚úÖ Minimum records per group: {min_records}\")\n\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables and group distribution\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\nprint(\"   3. Run 03_hyperparameter_search.py to find optimal hyperparameters per group\")\n",
      "id": "c1345586-d04f-434c-9456-e9da3bbaaedf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}