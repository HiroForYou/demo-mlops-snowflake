{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "833d0968-7676-4a3f-860a-c4009d92c23d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "892ef405-48e1-408a-b44f-be9baa30830a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "16e4fd53-fea7-4d84-9902-c6048851b0b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\"*80)\n\n# Check if table exists\ntry:\n    train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\")\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_lower = [col.lower() for col in columns]\nif 'uni_box_week' in columns_lower:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_lower.index('uni_box_week')]\n    print(f\"\\n‚úÖ Target variable 'uni_box_week' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable 'uni_box_week' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(\"Target variable 'uni_box_week' is required\")\n\n# Store target column name for later use\nTARGET_COLUMN = target_col\n",
      "id": "91fbd775-c11f-4188-a986-db78a1fc4267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "9c1bb67b-d5c4-4f3a-89ee-9f1cc0ec94d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\"*80)\n\ntry:\n    inference_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\")\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_lower = [col.lower() for col in inference_columns]\nif 'uni_box_week' in inference_columns_lower:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable 'uni_box_week' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "6e5cc31c-1d34-414d-a7ae-4c5d4c3dfcff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "869cf3b9-5aa3-4dc2-96dc-80c5618d35fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\"*80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN uni_box_week IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN week IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\")\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\"\"\"\n    SELECT\n        SUM(CASE WHEN sum_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_sum_past_12_weeks,\n        SUM(CASE WHEN avg_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_avg_past_12_weeks,\n        SUM(CASE WHEN week_of_year IS NULL THEN 1 ELSE 0 END) AS NULL_week_of_year,\n        SUM(CASE WHEN stats_ntile_group IS NULL THEN 1 ELSE 0 END) AS NULL_stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\")\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "5b7892d3-dca3-44b5-90f6-fd22041f8a11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "b353ff9c-5447-46fa-94d7-6c8c7c24718c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\"*80)\n\ntarget_stats = session.sql(\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT uni_box_week) AS UNIQUE_VALUES,\n        MIN(uni_box_week) AS MIN_VALUE,\n        MAX(uni_box_week) AS MAX_VALUE,\n        AVG(uni_box_week) AS MEAN_VALUE,\n        STDDEV(uni_box_week) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY uni_box_week) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY uni_box_week) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY uni_box_week) AS Q3\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    WHERE uni_box_week IS NOT NULL\n\"\"\")\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\"\"\"\n    WITH stats AS (\n        SELECT\n            AVG(uni_box_week) AS mean_val,\n            STDDEV(uni_box_week) AS stddev_val\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN(uni_box_week) AS MIN_OUTLIER,\n        MAX(uni_box_week) AS MAX_OUTLIER\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED, stats\n    WHERE uni_box_week IS NOT NULL\n        AND (uni_box_week < mean_val - 3 * stddev_val \n             OR uni_box_week > mean_val + 3 * stddev_val)\n\"\"\")\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "754da78e-c4fd-4bc3-af10-dfc73ad34f6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "b66fccf6-7107-4dab-b6cf-6524d92154ba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\"*80)\n\n# Define excluded columns\nexcluded_cols = [\n    'customer_id', 'brand_pres_ret', 'week', \n    'group', 'stats_group', 'percentile_group', 'stats_ntile_group'\n]\n\n# Get feature columns from training (exclude target + excluded)\ntrain_feature_cols = [col for col in columns \n                     if col not in excluded_cols and col != TARGET_COLUMN]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [col for col in inference_columns \n                         if col not in excluded_cols]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "1b4d9ba6-1681-4bb5-a2a6-c66b0015d6aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "d552e60a-e07f-4893-856d-367aa9806714"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\"*80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\nWHERE uni_box_week IS NOT NULL\n    AND customer_id IS NOT NULL\n    AND week IS NOT NULL\n    -- Remove extreme outliers (optional - adjust threshold as needed)\n    AND uni_box_week >= 0\n    AND uni_box_week <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY uni_box_week)\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\").count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\nWHERE customer_id IS NOT NULL\n    AND week IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\").count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "b1dc75c8-b99a-449c-be82-f7de603abe87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Summary Statistics\n",
      "id": "7a16538e-70e8-4960-825e-ea989b220052"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\"*80)\n\nsummary = session.sql(\"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\n\"\"\")\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\"*80)\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\n",
      "id": "27d8d379-8c57-4829-a210-fa3e1d097cfa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}