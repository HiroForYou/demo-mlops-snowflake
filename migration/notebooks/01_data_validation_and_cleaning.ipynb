{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n#\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n#\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "869d6e8a-c02a-4005-b19a-43cdd55d06b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "0ba7fd93-f99d-4832-ab14-bcd0fbbfd979",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "f3ce71a0-0b63-4d70-a320-97bdf572ea97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\" * 80)\n\n# Check if table exists\ntry:\n    train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\")\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_lower = [col.lower() for col in columns]\nif \"uni_box_week\" in columns_lower:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_lower.index(\"uni_box_week\")]\n    print(f\"\\n‚úÖ Target variable 'uni_box_week' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable 'uni_box_week' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(\"Target variable 'uni_box_week' is required\")\n\n# Store target column name for later use\nTARGET_COLUMN = target_col\n",
      "id": "73f7ffec-61d6-4d69-98be-e9e6500807b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "bbdc1c2f-92b9-4c3c-a822-10228ef44e96"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\" * 80)\n\ntry:\n    inference_df = session.table(\n        \"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\"\n    )\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_lower = [col.lower() for col in inference_columns]\nif \"uni_box_week\" in inference_columns_lower:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable 'uni_box_week' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "14abdadf-f1fb-4d76-8979-52ee1bc63d6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "cf37ef17-d9ec-4859-943e-01470fc9c8ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\" * 80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\n    \"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN uni_box_week IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN week IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\n    \"\"\"\n    SELECT\n        SUM(CASE WHEN sum_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_sum_past_12_weeks,\n        SUM(CASE WHEN avg_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_avg_past_12_weeks,\n        SUM(CASE WHEN week_of_year IS NULL THEN 1 ELSE 0 END) AS NULL_week_of_year,\n        SUM(CASE WHEN stats_ntile_group IS NULL THEN 1 ELSE 0 END) AS NULL_stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "7b1e6b74-0a9f-4025-8380-18075f9a5f3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "83884b47-9812-478b-a309-6a696c07a2ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\" * 80)\n\ntarget_stats = session.sql(\n    \"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT uni_box_week) AS UNIQUE_VALUES,\n        MIN(uni_box_week) AS MIN_VALUE,\n        MAX(uni_box_week) AS MAX_VALUE,\n        AVG(uni_box_week) AS MEAN_VALUE,\n        STDDEV(uni_box_week) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY uni_box_week) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY uni_box_week) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY uni_box_week) AS Q3\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    WHERE uni_box_week IS NOT NULL\n\"\"\"\n)\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\n    \"\"\"\n    WITH stats AS (\n        SELECT\n            AVG(uni_box_week) AS mean_val,\n            STDDEV(uni_box_week) AS stddev_val\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN(uni_box_week) AS MIN_OUTLIER,\n        MAX(uni_box_week) AS MAX_OUTLIER\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED, stats\n    WHERE uni_box_week IS NOT NULL\n        AND (uni_box_week < mean_val - 3 * stddev_val \n             OR uni_box_week > mean_val + 3 * stddev_val)\n\"\"\"\n)\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "8b4b2f4b-2435-40f1-8399-a67a9cfac534",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "7476b0e3-e50b-44b3-855f-62c3fd8fb9f4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\" * 80)\n\n# Define excluded columns\nexcluded_cols = [\n    \"customer_id\",\n    \"brand_pres_ret\",\n    \"week\",\n    \"group\",\n    \"stats_group\",\n    \"percentile_group\",\n    \"stats_ntile_group\",\n]\n\n# Get feature columns from training (exclude target + excluded)\ntrain_feature_cols = [\n    col for col in columns if col not in excluded_cols and col != TARGET_COLUMN\n]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [col for col in inference_columns if col not in excluded_cols]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "f56a65ea-3c38-4fc9-bfd1-89a9b5f2a728",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "74fcb374-bb6d-4f23-8243-7149ae215612"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\" * 80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\nWHERE uni_box_week IS NOT NULL\n    AND customer_id IS NOT NULL\n    AND week IS NOT NULL\n    -- Remove extreme outliers (optional - adjust threshold as needed)\n    AND uni_box_week >= 0\n    AND uni_box_week <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY uni_box_week)\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(\n    \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n).count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\nWHERE customer_id IS NOT NULL\n    AND week IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(\n    \"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\"\n).count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "c24d76c8-4337-4981-8c3c-79bd0e5cb1ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Summary Statistics\n",
      "id": "4380eb88-f784-4562-90a5-f06259e5ea88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\" * 80)\n\nsummary = session.sql(\n    \"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\n\"\"\"\n)\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\" * 80)\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\n",
      "id": "289748f8-c209-4214-89bd-f04611bf7be0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}