{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "3da2ec7d-20f0-435e-897e-19e0581262a9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "49520f1e-c5c1-46f9-9603-75e5b644dd63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "b132fc1e-7ace-4118-8c3e-5699978d9e19"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\"*80)\n\n# Check if table exists\ntry:\n    train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\")\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_lower = [col.lower() for col in columns]\nif 'uni_box_week' in columns_lower:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_lower.index('uni_box_week')]\n    print(f\"\\n‚úÖ Target variable 'uni_box_week' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable 'uni_box_week' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(\"Target variable 'uni_box_week' is required\")\n\n# Store target column name for later use\nTARGET_COLUMN = target_col\n",
      "id": "269d6dd5-1b43-4114-9d39-9344952b0134",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "d88ea523-2787-421e-9d1b-d86bd1ee05d5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\"*80)\n\ntry:\n    inference_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\")\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_lower = [col.lower() for col in inference_columns]\nif 'uni_box_week' in inference_columns_lower:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable 'uni_box_week' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "45accb1d-2054-4c58-87f8-c0a8e27da34f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "3367dc3d-7422-442b-9038-91a32c6b4489"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\"*80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN uni_box_week IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN week IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\")\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\"\"\"\n    SELECT\n        SUM(CASE WHEN sum_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_sum_past_12_weeks,\n        SUM(CASE WHEN avg_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_avg_past_12_weeks,\n        SUM(CASE WHEN week_of_year IS NULL THEN 1 ELSE 0 END) AS NULL_week_of_year,\n        SUM(CASE WHEN stats_ntile_group IS NULL THEN 1 ELSE 0 END) AS NULL_stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\")\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "cf976315-c3bb-449f-b5e2-bf586d88c9ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "b2ad8c93-2bc1-4b8d-bbb4-abbf2c78ee84"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\"*80)\n\ntarget_stats = session.sql(\"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT uni_box_week) AS UNIQUE_VALUES,\n        MIN(uni_box_week) AS MIN_VALUE,\n        MAX(uni_box_week) AS MAX_VALUE,\n        AVG(uni_box_week) AS MEAN_VALUE,\n        STDDEV(uni_box_week) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY uni_box_week) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY uni_box_week) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY uni_box_week) AS Q3\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    WHERE uni_box_week IS NOT NULL\n\"\"\")\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\"\"\"\n    WITH stats AS (\n        SELECT\n            AVG(uni_box_week) AS mean_val,\n            STDDEV(uni_box_week) AS stddev_val\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN(uni_box_week) AS MIN_OUTLIER,\n        MAX(uni_box_week) AS MAX_OUTLIER\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED, stats\n    WHERE uni_box_week IS NOT NULL\n        AND (uni_box_week < mean_val - 3 * stddev_val \n             OR uni_box_week > mean_val + 3 * stddev_val)\n\"\"\")\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "3b8eac5a-c4fb-4324-9f27-72acf9e8e715",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "625b61b4-244e-46ba-a44f-797641ed6831"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\"*80)\n\n# Define excluded columns\nexcluded_cols = [\n    'customer_id', 'brand_pres_ret', 'week', \n    'group', 'stats_group', 'percentile_group', 'stats_ntile_group'\n]\n\n# Get feature columns from training (exclude target + excluded)\ntrain_feature_cols = [col for col in columns \n                     if col not in excluded_cols and col != TARGET_COLUMN]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [col for col in inference_columns \n                         if col not in excluded_cols]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "f8eb6992-fe03-4882-875a-ef5ed58ec6af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "911b4f22-53e7-4ad8-a16c-9669bb0f67f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\"*80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\nWHERE uni_box_week IS NOT NULL\n    AND customer_id IS NOT NULL\n    AND week IS NOT NULL\n    -- Remove extreme outliers (optional - adjust threshold as needed)\n    AND uni_box_week >= 0\n    AND uni_box_week <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY uni_box_week)\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\").count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\nWHERE customer_id IS NOT NULL\n    AND week IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\").count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "e05af233-63f4-43ae-8abd-29f79c5cace2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Summary Statistics\n",
      "id": "6a0dcb0e-a37f-4838-89d2-b7dac7f26e6f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\"*80)\n\nsummary = session.sql(\"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\n\"\"\")\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\"*80)\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\n",
      "id": "49426f63-0e77-40ff-a2da-5d6074c13291",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}