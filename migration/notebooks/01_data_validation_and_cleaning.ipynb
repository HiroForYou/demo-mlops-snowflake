{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Data Validation and Cleaning\n#\n## Overview\nThis script validates and cleans the training and inference datasets before creating the Feature Store.\n#\n## What We'll Do:\n1. Validate table structures and data quality\n2. Clean data (handle NULLs, outliers)\n3. Verify feature compatibility between train and inference\n4. Generate data quality reports\n",
      "id": "b43133df-ab8b-4deb-816b-03c4c01ffe08"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "ae2837b3-9c5b-48fc-97b0-7eb625f40f94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Validate Training Dataset\n",
      "id": "707dbe28-27f0-4adc-bb27-371727a4d6b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING TRAINING DATASET\")\nprint(\"=\" * 80)\n\n# Check if table exists\ntry:\n    train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\")\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Table exists: TRAIN_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {total_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Get column information\ncolumns = train_df.columns\nprint(f\"\\nüìã Columns ({len(columns)}):\")\nfor col in columns:\n    print(f\"   - {col}\")\n\n# Check for target variable (case-insensitive)\ncolumns_lower = [col.lower() for col in columns]\nif \"uni_box_week\" in columns_lower:\n    # Find the actual column name (preserving case)\n    target_col = columns[columns_lower.index(\"uni_box_week\")]\n    print(f\"\\n‚úÖ Target variable 'uni_box_week' found (as '{target_col}')\")\nelse:\n    print(f\"\\n‚ùå Target variable 'uni_box_week' NOT found!\")\n    print(f\"   Available columns: {', '.join(columns)}\")\n    raise ValueError(\"Target variable 'uni_box_week' is required\")\n\n# Store target column name for later use\nTARGET_COLUMN = target_col\n",
      "id": "8fc4e4a3-be9f-4888-ad7a-a1503c844b70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Validate Inference Dataset\n",
      "id": "7dfd6e9d-5c2a-4246-be78-ef30fa51d8a0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä VALIDATING INFERENCE DATASET\")\nprint(\"=\" * 80)\n\ntry:\n    inference_df = session.table(\n        \"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\"\n    )\n    inference_rows = inference_df.count()\n    print(f\"\\n‚úÖ Table exists: INFERENCE_DATASET_STRUCTURED\")\n    print(f\"   Total rows: {inference_rows:,}\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error accessing table: {str(e)}\")\n    raise\n\n# Verify target is NOT in inference\ninference_columns = inference_df.columns\ninference_columns_lower = [col.lower() for col in inference_columns]\nif \"uni_box_week\" in inference_columns_lower:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Target variable 'uni_box_week' found in inference dataset\")\n    print(f\"   This is expected - inference should not have target values\")\nelse:\n    print(f\"\\n‚úÖ Target variable correctly absent from inference dataset\")\n",
      "id": "d05188bb-84cf-483d-ac17-f6c0acb423d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Check Data Quality - NULLs and Missing Values\n",
      "id": "7a5cd85b-a095-4e57-9c4d-1df9ea163011"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç DATA QUALITY CHECK - NULL VALUES\")\nprint(\"=\" * 80)\n\n# Check NULLs in training data\nnull_check_train = session.sql(\n    \"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_ROWS,\n        SUM(CASE WHEN uni_box_week IS NULL THEN 1 ELSE 0 END) AS NULL_TARGET,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS NULL_CUSTOMER_ID,\n        SUM(CASE WHEN week IS NULL THEN 1 ELSE 0 END) AS NULL_WEEK\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Training Data:\")\nnull_check_train.show()\n\n# Check for NULLs in key features\nfeature_null_check = session.sql(\n    \"\"\"\n    SELECT\n        SUM(CASE WHEN sum_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_sum_past_12_weeks,\n        SUM(CASE WHEN avg_past_12_weeks IS NULL THEN 1 ELSE 0 END) AS NULL_avg_past_12_weeks,\n        SUM(CASE WHEN week_of_year IS NULL THEN 1 ELSE 0 END) AS NULL_week_of_year,\n        SUM(CASE WHEN stats_ntile_group IS NULL THEN 1 ELSE 0 END) AS NULL_stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n\"\"\"\n)\n\nprint(\"\\nüìä NULL Values in Key Features:\")\nfeature_null_check.show()\n",
      "id": "a5f2c08e-9b2c-4060-8dce-0cba39c97715",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Check Target Variable Distribution\n",
      "id": "83d8dac4-1bfe-4ce0-9fe1-f70844fd343c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà TARGET VARIABLE DISTRIBUTION\")\nprint(\"=\" * 80)\n\ntarget_stats = session.sql(\n    \"\"\"\n    SELECT\n        COUNT(*) AS TOTAL_RECORDS,\n        COUNT(DISTINCT uni_box_week) AS UNIQUE_VALUES,\n        MIN(uni_box_week) AS MIN_VALUE,\n        MAX(uni_box_week) AS MAX_VALUE,\n        AVG(uni_box_week) AS MEAN_VALUE,\n        STDDEV(uni_box_week) AS STDDEV_VALUE,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY uni_box_week) AS Q1,\n        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY uni_box_week) AS MEDIAN,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY uni_box_week) AS Q3\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    WHERE uni_box_week IS NOT NULL\n\"\"\"\n)\n\nprint(\"\\nüìä Target Variable (uni_box_week) Statistics:\")\ntarget_stats.show()\n\n# Check for outliers (values beyond 3 standard deviations)\noutlier_check = session.sql(\n    \"\"\"\n    WITH stats AS (\n        SELECT\n            AVG(uni_box_week) AS mean_val,\n            STDDEV(uni_box_week) AS stddev_val\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n    SELECT\n        COUNT(*) AS OUTLIER_COUNT,\n        MIN(uni_box_week) AS MIN_OUTLIER,\n        MAX(uni_box_week) AS MAX_OUTLIER\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED, stats\n    WHERE uni_box_week IS NOT NULL\n        AND (uni_box_week < mean_val - 3 * stddev_val \n             OR uni_box_week > mean_val + 3 * stddev_val)\n\"\"\"\n)\n\nprint(\"\\nüìä Outliers (>3 std dev):\")\noutlier_check.show()\n",
      "id": "72f56191-4843-4e7a-aac5-85f3317397d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Verify Feature Compatibility\n",
      "id": "143bf063-765a-4e68-8435-dd96a295dfbd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîó FEATURE COMPATIBILITY CHECK\")\nprint(\"=\" * 80)\n\n# Define excluded columns\nexcluded_cols = [\n    \"customer_id\",\n    \"brand_pres_ret\",\n    \"week\",\n    \"group\",\n    \"stats_group\",\n    \"percentile_group\",\n    \"stats_ntile_group\",\n]\n\n# Get feature columns from training (exclude target + excluded)\ntrain_feature_cols = [\n    col for col in columns if col not in excluded_cols and col != TARGET_COLUMN\n]\n\n# Get feature columns from inference (exclude excluded)\ninference_feature_cols = [col for col in inference_columns if col not in excluded_cols]\n\nprint(f\"\\nüìã Training Features ({len(train_feature_cols)}):\")\nfor col in sorted(train_feature_cols):\n    print(f\"   - {col}\")\n\nprint(f\"\\nüìã Inference Features ({len(inference_feature_cols)}):\")\nfor col in sorted(inference_feature_cols):\n    print(f\"   - {col}\")\n\n# Check if features match\nmissing_in_inference = set(train_feature_cols) - set(inference_feature_cols)\nmissing_in_train = set(inference_feature_cols) - set(train_feature_cols)\n\nif missing_in_inference:\n    print(f\"\\n‚ö†Ô∏è  Features in training but NOT in inference:\")\n    for col in missing_in_inference:\n        print(f\"   - {col}\")\n\nif missing_in_train:\n    print(f\"\\n‚ö†Ô∏è  Features in inference but NOT in training:\")\n    for col in missing_in_train:\n        print(f\"   - {col}\")\n\nif not missing_in_inference and not missing_in_train:\n    print(f\"\\n‚úÖ All features match between training and inference!\")\n",
      "id": "f7eccb82-2101-42af-bd5a-c8cc2e484a87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Create Cleaned Tables\n",
      "id": "4bb65e55-3f75-4def-b03f-728950d5f5fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üßπ CREATING CLEANED TABLES\")\nprint(\"=\" * 80)\n\n# Create cleaned training table\nprint(\"\\nüìù Creating cleaned training table...\")\n\ncleaned_train_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\nWHERE uni_box_week IS NOT NULL\n    AND customer_id IS NOT NULL\n    AND week IS NOT NULL\n    -- Remove extreme outliers (optional - adjust threshold as needed)\n    AND uni_box_week >= 0\n    AND uni_box_week <= (\n        SELECT PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY uni_box_week)\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n        WHERE uni_box_week IS NOT NULL\n    )\n\"\"\"\n\nsession.sql(cleaned_train_sql).collect()\n\ncleaned_train_count = session.table(\n    \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n).count()\nprint(f\"‚úÖ Cleaned training table created: {cleaned_train_count:,} rows\")\n\n# Create cleaned inference table\nprint(\"\\nüìù Creating cleaned inference table...\")\n\ncleaned_inference_sql = \"\"\"\nCREATE OR REPLACE TABLE BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED AS\nSELECT *\nFROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\nWHERE customer_id IS NOT NULL\n    AND week IS NOT NULL\n\"\"\"\n\nsession.sql(cleaned_inference_sql).collect()\n\ncleaned_inference_count = session.table(\n    \"BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\"\n).count()\nprint(f\"‚úÖ Cleaned inference table created: {cleaned_inference_count:,} rows\")\n",
      "id": "91ad002a-9206-4ec5-b8a2-8bb743c7e5b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Validate stats_ntile_group Segmentation\n",
      "id": "fa3743f3-c4c7-4abf-bee6-9afb11cef3da"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç VALIDATING stats_ntile_group SEGMENTATION\")\nprint(\"=\" * 80)\n\n# Check if stats_ntile_group exists\nif \"stats_ntile_group\" not in columns_lower:\n    print(\"\\n‚ùå ERROR: Column 'stats_ntile_group' NOT found in training dataset!\")\n    print(\"   This column is required for 16-group model training.\")\n    raise ValueError(\"stats_ntile_group column is required\")\n\n# Find actual column name (preserving case)\nstats_ntile_col = columns[columns_lower.index(\"stats_ntile_group\")]\n\n# Get unique groups\ngroups_df = session.sql(\n    f\"\"\"\n    SELECT \n        {stats_ntile_col} AS GROUP_NAME,\n        COUNT(*) AS RECORD_COUNT,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        AVG(uni_box_week) AS AVG_TARGET,\n        MIN(uni_box_week) AS MIN_TARGET,\n        MAX(uni_box_week) AS MAX_TARGET\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE {stats_ntile_col} IS NOT NULL\n    GROUP BY {stats_ntile_col}\n    ORDER BY {stats_ntile_col}\n\"\"\"\n)\n\nprint(\"\\nüìä Group Distribution:\")\ngroups_df.show()\n\n# Get group count\ngroup_count = groups_df.count()\nprint(f\"\\nüìä Total unique groups: {group_count}\")\n\n# Validate we have exactly 16 groups\nif group_count != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {group_count}\")\n    print(\"   This may affect model training. Please verify segmentation logic.\")\nelse:\n    print(f\"\\n‚úÖ Validation passed: Exactly 16 groups found\")\n\n# Check minimum records per group (recommend at least 100)\nmin_records_check = session.sql(\n    f\"\"\"\n    SELECT \n        MIN(RECORD_COUNT) AS MIN_RECORDS,\n        MAX(RECORD_COUNT) AS MAX_RECORDS,\n        AVG(RECORD_COUNT) AS AVG_RECORDS\n    FROM (\n        SELECT \n            {stats_ntile_col},\n            COUNT(*) AS RECORD_COUNT\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n        WHERE {stats_ntile_col} IS NOT NULL\n        GROUP BY {stats_ntile_col}\n    )\n\"\"\"\n)\n\nprint(\"\\nüìä Records per Group Statistics:\")\nmin_records_check.show()\n\nmin_records_result = min_records_check.collect()[0]\nmin_records = min_records_result[\"MIN_RECORDS\"]\n\nif min_records < 100:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Some groups have less than 100 records (minimum: {min_records})\")\n    print(\"   This may affect model training quality.\")\nelse:\n    print(f\"\\n‚úÖ All groups have sufficient data (minimum: {min_records} records)\")\n",
      "id": "a323b8ac-1f82-4fdd-a300-3ef234c63963",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Summary Statistics\n",
      "id": "d6f9f178-e01d-45df-8c8e-70dbf738a2e3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY STATISTICS\")\nprint(\"=\" * 80)\n\nsummary = session.sql(\n    \"\"\"\n    SELECT\n        'Training (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Training (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    UNION ALL\n    SELECT\n        'Inference (Original)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_STRUCTURED\n    UNION ALL\n    SELECT\n        'Inference (Cleaned)' AS DATASET,\n        COUNT(*) AS TOTAL_ROWS,\n        COUNT(DISTINCT customer_id) AS UNIQUE_CUSTOMERS,\n        COUNT(DISTINCT week) AS UNIQUE_WEEKS\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.INFERENCE_DATASET_CLEANED\n\"\"\"\n)\n\nprint(\"\\nüìä Dataset Comparison:\")\nsummary.show()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ DATA VALIDATION AND CLEANING COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Validation Summary:\")\nprint(f\"   ‚úÖ Training data validated: {cleaned_train_count:,} rows\")\nprint(f\"   ‚úÖ Inference data validated: {cleaned_inference_count:,} rows\")\nprint(f\"   ‚úÖ stats_ntile_group validated: {group_count} groups\")\nprint(f\"   ‚úÖ Minimum records per group: {min_records}\")\n\nprint(\"\\nüìã Next Steps:\")\nprint(\"   1. Review cleaned tables and group distribution\")\nprint(\"   2. Run 02_feature_store_setup.py to create Feature Store\")\nprint(\"   3. Run 03_hyperparameter_search.py to find optimal hyperparameters per group\")\n",
      "id": "d9080eec-370f-4350-a876-79cc0ad628fe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}