{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Hyperparameter Search (LGBM/XGB/SGD + Snowflake ML Tune) - Per Group\n#\n## Overview\nThis script performs hyperparameter optimization using Snowflake ML's tune.search for regression (LGBM, XGBoost, SGD per group).\n**Runs HPO per group in a sequential loop (no MMT) to avoid Ray serialization issues; each group runs its own Random Search.**\n#\n## What We'll Do:\n1. Load cleaned training data with stats_ntile_group\n2. Get all 16 unique groups\n3. For each group (loop): load group data, run Random Search with snowflake.ml.modeling.tune, save best hyperparameters to SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n4. Generate summary of all hyperparameter results\n",
      "id": "db814738-0364-46ec-87a8-718586543b39"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.modeling.tune import (\n    Tuner,\n    TunerConfig,\n    get_tuner_context,\n    randint,\n    uniform,\n)\nfrom snowflake.ml.modeling.tune.search import RandomSearch\nfrom snowflake.ml.data.data_connector import DataConnector\nfrom snowflake.ml.experiment import ExperimentTracking\nimport numpy as np\nfrom datetime import datetime\nimport json\nimport time\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n\n# Configuraci√≥n:\n# - Si no tienes permisos para FeatureView/Dynamic Tables, usa tablas limpias.\n# - Mantengo el flag pero agrego fallback autom√°tico si el modo Feature Store falla.\nUSE_CLEANED_TABLES = (\n    False  # True = TRAIN_DATASET_CLEANED, False = intentar Feature Store\n)\n\n# Un solo objeto: grupo -> nombre de clase Snowflake ML (snowflake.ml.modeling.*)\nGROUP_MODEL = {\n    \"group_stat_0_1\": \"LGBMRegressor\",\n    \"group_stat_0_2\": \"LGBMRegressor\",\n    \"group_stat_0_3\": \"LGBMRegressor\",\n    \"group_stat_0_4\": \"LGBMRegressor\",\n    \"group_stat_1_1\": \"LGBMRegressor\",\n    \"group_stat_1_2\": \"LGBMRegressor\",\n    \"group_stat_1_3\": \"XGBRegressor\",\n    \"group_stat_1_4\": \"SGDRegressor\",\n    \"group_stat_2_1\": \"LGBMRegressor\",\n    \"group_stat_2_2\": \"LGBMRegressor\",\n    \"group_stat_2_3\": \"XGBRegressor\",\n    \"group_stat_2_4\": \"XGBRegressor\",\n    \"group_stat_3_1\": \"LGBMRegressor\",\n    \"group_stat_3_2\": \"LGBMRegressor\",\n    \"group_stat_3_3\": \"LGBMRegressor\",\n    \"group_stat_3_4\": \"SGDRegressor\",\n}\n_DEFAULT_MODEL = \"XGBRegressor\"\n",
      "id": "eebddbb8-7fb0-434e-9272-4ba5d99e08c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Get All Groups and Load Training Data\n",
      "id": "30ef68fd-a9c0-4575-ab31-4ef5e7120e5f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä GETTING ALL GROUPS\")\nprint(\"=\" * 80)\n\n# Get all unique groups from cleaned training table\ngroups_df = session.sql(\n    \"\"\"\n    SELECT DISTINCT stats_ntile_group AS GROUP_NAME\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE stats_ntile_group IS NOT NULL\n    ORDER BY stats_ntile_group\n\"\"\"\n)\n\ngroups_list = [row[\"GROUP_NAME\"] for row in groups_df.collect()]\nprint(f\"\\n‚úÖ Found {len(groups_list)} groups:\")\nfor i, group in enumerate(groups_list, 1):\n    print(f\"   {i:2d}. {group}\")\n\nif len(groups_list) != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {len(groups_list)}\")\n    print(\"   Continuing with available groups...\")\n",
      "id": "43f33b62-21e2-4c90-bdf8-604417ed30f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Load Training Data (Feature Store or Cleaned Tables)\n",
      "id": "c2a91f2f-5634-4592-a86a-16286c39684c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "if USE_CLEANED_TABLES:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üìä LOADING DATA FROM CLEANED TABLES (TESTING MODE)\")\n    print(\"=\" * 80)\n\n    # Load directly from cleaned training table (for testing purposes)\n    print(\"‚è≥ Loading data from TRAIN_DATASET_CLEANED...\")\n    train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Training data loaded from cleaned table\")\n    print(f\"   Total rows: {total_rows:,}\")\n    print(f\"   ‚ö†Ô∏è  TESTING MODE: Using cleaned tables directly (not Feature Store)\")\nelse:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üè™ FEATURE STORE MODE (SIN FEATUREVIEW)\")\n    print(\"=\" * 80)\n\n    # En lugar de FeatureView, soportamos una tabla de features materializada por `02_feature_store_setup.py`\n    # (sin Dynamic Tables). Si no existe o falla, hacemos fallback a tablas limpias.\n    FEATURES_TABLE = \"BD_AA_DEV.SC_FEATURES_BMX.UNI_BOX_FEATURES\"\n\n    try:\n        # Mantener inicializaci√≥n del Feature Store (aunque no usemos FeatureView)\n        _fs = FeatureStore(\n            session=session,\n            database=\"BD_AA_DEV\",\n            name=\"SC_FEATURES_BMX\",\n            default_warehouse=\"WH_AA_DEV_DS_SQL\",\n        )\n        print(\"‚úÖ Feature Store inicializado (sin FeatureView)\")\n\n        print(f\"‚è≥ Loading features from table: {FEATURES_TABLE} ...\")\n        features_df = session.table(FEATURES_TABLE)\n\n        print(\"‚è≥ Loading target variable and stats_ntile_group from training table...\")\n        target_df = session.table(\n            \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n        ).select(\n            \"customer_id\", \"brand_pres_ret\", \"week\", \"uni_box_week\", \"stats_ntile_group\"\n        )\n\n        print(\"‚è≥ Joining features with target...\")\n        train_df = features_df.join(\n            target_df, on=[\"customer_id\", \"brand_pres_ret\", \"week\"], how=\"inner\"\n        )\n\n        total_rows = train_df.count()\n        print(f\"\\n‚úÖ Training data loaded from features table + target\")\n        print(f\"   Total rows: {total_rows:,}\")\n    except Exception as e:\n        print(\n            f\"‚ö†Ô∏è  Could not load/join features table ({FEATURES_TABLE}): {str(e)[:200]}\"\n        )\n        print(\n            \"   Falling back to TRAIN_DATASET_CLEANED (USE_CLEANED_TABLES=True behavior)\"\n        )\n        train_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n        total_rows = train_df.count()\n        print(f\"\\n‚úÖ Training data loaded from cleaned table (fallback)\")\n        print(f\"   Total rows: {total_rows:,}\")\n",
      "id": "c5ea412d-e23a-40bf-a6b4-9594404693c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Define Hyperparameter Search Space\n",
      "id": "2d3646bf-abd1-480d-8c50-e707b9e7fd54"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üéØ DEFINING HYPERPARAMETER SEARCH SPACE\")\nprint(\"=\" * 80)\n\n# Espacios de b√∫squeda por tipo de modelo (LGBM, XGBoost, SGD)\nSEARCH_SPACES = {\n    \"XGBRegressor\": {\n        \"n_estimators\": randint(50, 300),\n        \"max_depth\": randint(3, 10),\n        \"learning_rate\": uniform(0.01, 0.3),\n        \"subsample\": uniform(0.6, 1.0),\n        \"colsample_bytree\": uniform(0.6, 1.0),\n        \"min_child_weight\": randint(1, 7),\n        \"gamma\": uniform(0, 0.5),\n        \"reg_alpha\": uniform(0, 1),\n        \"reg_lambda\": uniform(0, 1),\n    },\n    \"LGBMRegressor\": {\n        \"n_estimators\": randint(50, 300),\n        \"max_depth\": randint(3, 10),\n        \"learning_rate\": uniform(0.01, 0.3),\n        \"num_leaves\": randint(20, 150),\n        \"subsample\": uniform(0.6, 1.0),\n        \"colsample_bytree\": uniform(0.6, 1.0),\n        \"reg_alpha\": uniform(0, 1),\n        \"reg_lambda\": uniform(0, 1),\n        \"min_child_samples\": randint(5, 50),\n    },\n    \"SGDRegressor\": {\n        \"alpha\": uniform(1e-5, 1e-2),\n        \"max_iter\": randint(1000, 5000),\n        \"tol\": uniform(1e-5, 1e-2),\n        \"eta0\": uniform(1e-4, 0.01),\n    },\n}\n\nprint(\"\\nüìã Hyperparameter Search Spaces (per model type):\")\nfor model_type, search_space in SEARCH_SPACES.items():\n    print(f\"   {model_type}: {list(search_space.keys())}\")\nprint(\"\\nüìã XGBRegressor search space (example):\")\nfor param, dist in SEARCH_SPACES[\"XGBRegressor\"].items():\n    if hasattr(dist, \"low\") and hasattr(dist, \"high\"):\n        dist_name = dist.__class__.__name__.lower()\n        if \"rand\" in dist_name and \"int\" in dist_name:\n            print(f\"   {param}: randint({dist.low}, {dist.high})\")\n        else:\n            print(f\"   {param}: uniform({dist.low:.2f}, {dist.high:.2f})\")\n\n# Number of trials for Random Search\nnum_trials = 10\nmax_concurrent_trials = 4\nprint(f\"\\nüî¢ Random Search trials per group: {num_trials}\")\nprint(f\"   Max concurrent trials per group: {max_concurrent_trials}\")\n\n# Sample rate per group for hyperparameter search\nSAMPLE_RATE_PER_GROUP = 0.1\nprint(f\"üìä Sample rate per group: {SAMPLE_RATE_PER_GROUP*100:.0f}%\")\nif SAMPLE_RATE_PER_GROUP < 1.0:\n    print(\n        f\"   ‚ö†Ô∏è  Using {SAMPLE_RATE_PER_GROUP*100:.0f}% of data - consider using 1.0 (full group) for better results\"\n    )\nelse:\n    print(f\"   ‚úÖ Using full group data for optimal hyperparameter search\")\n",
      "id": "fc9ed6ad-5c37-4a16-bf91-f29b1fbef7f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 3c. (Opcional) Ver Ray Dashboard\n#\nUsa esta celda en Notebooks de Snowflake para obtener la URL del Ray Dashboard\nasociado al runtime actual. Copia y pega la URL en tu navegador.\n",
      "id": "f3c32907-34c0-4d47-9ef7-2c1d32c450d5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "try:\n    from snowflake.ml.runtime_cluster import get_ray_dashboard_url\n\n    dashboard_url = get_ray_dashboard_url()\n    print(f\"‚úÖ Access the Ray Dashboard here: {dashboard_url}\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è No se pudo obtener la URL del Ray Dashboard.\")\n    print(f\"   Detalle: {str(e)[:200]}\")\n",
      "id": "5bf16e6a-69dc-458d-afe7-260762e3abd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Scale Cluster for HPO\n",
      "id": "1ecc7895-b50f-4368-9d8f-e07ab94bf706"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà SCALING CLUSTER FOR HPO\")\nprint(\"=\" * 80)\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n\n    print(\"‚è≥ Escalando cluster a 4 contenedores...\")\n    scale_cluster(\n        expected_cluster_size=4,\n        options={\n            \"block_until_min_cluster_size\": 2  # Return when at least 2 nodes ready\n        }\n    )\n    print(\"‚úÖ Cluster escalado a 4 contenedores\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Error al escalar cluster: {str(e)[:200]}\")\n    print(\"   Continuando con el cluster actual...\")\n",
      "id": "f9f2ad54-1b11-4d45-93b4-efe83fb0b5a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Perform Hyperparameter Search Per Group\n",
      "id": "12a023b8-a20b-4760-a485-a1c61d311852"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5a. Setup: ML Experiments, tabla (si aplica), features y all_results\n",
      "id": "bbc70428-cdac-4004-80c9-8bfce3020e88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç PERFORMING HYPERPARAMETER SEARCH PER GROUP\")\nprint(\"=\" * 80)\n\n# Initialize ML Experiments for hyperparameter tracking FIRST\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üî¨ INITIALIZING ML EXPERIMENTS\")\nprint(\"=\" * 80)\n\n# Initialize experiment_name in global scope (multi-model: LGBMRegressor, XGBRegressor, SGDRegressor)\nexperiment_name = (\n    f\"hyperparameter_search_regression_{datetime.now().strftime('%Y%m%d')}\"\n)\n\ntry:\n    exp_tracking = ExperimentTracking(session)\n    exp_tracking.set_experiment(experiment_name)\n    print(f\"‚úÖ Experiment created: {experiment_name}\")\n    experiments_available = True\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will continue with table-based storage only\")\n    exp_tracking = None\n    experiments_available = False\n\n# Create results table ONLY if ML Experiments is not available\n# If Experiments is available, we don't need this table\nif not experiments_available:\n    print(\"\\nüìã Creating HYPERPARAMETER_RESULTS table (Experiments not available)\")\n    session.sql(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS (\n            search_id VARCHAR,\n            group_name VARCHAR,\n            algorithm VARCHAR,\n            best_params VARIANT,\n            best_cv_rmse FLOAT,\n            best_cv_mae FLOAT,\n            val_rmse FLOAT,\n            val_mae FLOAT,\n            n_iter INTEGER,\n            sample_size INTEGER,\n            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\n    \"\"\"\n    ).collect()\n    print(\"   ‚úÖ Table created (will be used as primary storage)\")\nelse:\n    print(\"\\nüìã Skipping table creation (using ML Experiments as primary storage)\")\n\n# Define excluded columns (metadata columns, not features)\n# Note: FEATURE_TIMESTAMP may not exist if using cleaned tables directly\nexcluded_cols = [\n    \"customer_id\",\n    \"brand_pres_ret\",\n    \"week\",\n    \"FEATURE_TIMESTAMP\",  # Feature Store timestamp column (may not exist in cleaned tables)\n    \"stats_ntile_group\",  # Group column - not a feature\n]\n\n\ndef _get_target_column(df):\n    \"\"\"Return the target column name in df (case-insensitive match for uni_box_week).\"\"\"\n    for c in df.columns:\n        if str(c).upper() == \"UNI_BOX_WEEK\":\n            return c\n    return \"uni_box_week\"\n\n\ndef _get_feature_cols_numeric(df, excluded_cols, target_col):\n    \"\"\"Columnas que son features num√©ricas: excluye metadata/target (case-insensitive) y solo int/float/bool (Snowflake ML).\"\"\"\n    excluded_upper = {str(x).upper() for x in list(excluded_cols) + [target_col]}\n    return [\n        col\n        for col in df.columns\n        if str(col).upper() not in excluded_upper\n        and getattr(df[col].dtype, \"kind\", \"O\") in \"iufb\"\n    ]\n\n\n# Get feature columns from first group (all groups should have same features)\nsample_group_df = train_df.filter(train_df[\"stats_ntile_group\"] == groups_list[0])\nsample_pandas = sample_group_df.limit(1).to_pandas()\ntarget_col_sample = _get_target_column(sample_pandas)\nfeature_cols = _get_feature_cols_numeric(sample_pandas, excluded_cols, target_col_sample)\n\nprint(f\"\\nüìã Features ({len(feature_cols)}):\")\nfor col in sorted(feature_cols):\n    print(f\"   - {col}\")\n\n# Dictionary to store all results (se llena en el loop de 4c)\nall_results = {}\n",
      "id": "d4667a18-c600-48da-9fa6-42d3f2b161d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4b. Definiciones: funci√≥n de entrenamiento para el Tuner y HPO por grupo\n",
      "id": "ef8f1da3-77a3-4dc7-bdd9-0f2333a0b86e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def create_train_func_for_tuner(feature_cols, model_type, target_col):\n    \"\"\"\n    Create a training function for the Tuner (XGBRegressor, LGBMRegressor or SGDRegressor).\n    Called for each trial with different hyperparameters; model_type selects the regressor class.\n\n    Args:\n        feature_cols: List of feature column names\n        model_type: \"XGBRegressor\", \"LGBMRegressor\", or \"SGDRegressor\"\n        target_col: Actual target column name in dataset (e.g. uni_box_week or UNI_BOX_WEEK)\n\n    Returns:\n        train_func: Function that can be used by Tuner\n    \"\"\"\n\n    def train_func():\n        from sklearn.metrics import mean_squared_error, mean_absolute_error\n        import numpy as np\n\n        tuner_context = get_tuner_context()\n        params = tuner_context.get_hyper_params()\n        dm = tuner_context.get_dataset_map()\n\n        train_pd = dm[\"train\"].to_pandas()\n        test_pd = dm[\"test\"].to_pandas()\n        # Snowflake ML usa fit(dataset) con un solo DataFrame; input_cols y label_cols en el modelo\n        train_dataset = train_pd[feature_cols + [target_col]].fillna(0)\n        test_features = test_pd[feature_cols].fillna(0)\n        y_val = test_pd[target_col].fillna(0).values\n\n        model_params = params.copy()\n        model_params[\"random_state\"] = 42\n\n        if model_type == \"XGBRegressor\":\n            from snowflake.ml.modeling.xgboost import XGBRegressor\n\n            model_params[\"n_jobs\"] = -1\n            model_params[\"objective\"] = \"reg:squarederror\"\n            model_params[\"eval_metric\"] = \"rmse\"\n            model = XGBRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n        elif model_type == \"LGBMRegressor\":\n            from snowflake.ml.modeling.lightgbm import LGBMRegressor\n\n            model_params[\"n_jobs\"] = -1\n            model_params[\"verbosity\"] = -1\n            model = LGBMRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n        elif model_type == \"SGDRegressor\":\n            from snowflake.ml.modeling.linear_model import SGDRegressor\n\n            model_params.setdefault(\"penalty\", \"l2\")\n            model_params.setdefault(\"learning_rate\", \"invscaling\")\n            model = SGDRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n        else:\n            from snowflake.ml.modeling.xgboost import XGBRegressor\n\n            model_params[\"n_jobs\"] = -1\n            model_params[\"objective\"] = \"reg:squarederror\"\n            model_params[\"eval_metric\"] = \"rmse\"\n            model = XGBRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n\n        model.fit(train_dataset)\n        pred_result = model.predict(test_features)\n        pred_df = pred_result.to_pandas() if hasattr(pred_result, \"to_pandas\") else pred_result\n        out_col = model.get_output_cols()[0]\n        y_val_pred = np.asarray(pred_df[out_col])\n        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        val_mae = mean_absolute_error(y_val, y_val_pred)\n        tuner_context.report(metrics={\"rmse\": val_rmse, \"mae\": val_mae}, model=model)\n\n    return train_func\n\n\ndef run_hyperparameter_search_for_one_group(group_name, group_df):\n    \"\"\"\n    Run hyperparameter search for one group (HPO only, no MMT).\n    Called from a loop in the main script to avoid Ray serialization issues.\n\n    Args:\n        group_name: stats_ntile_group name (e.g. \"GROUP_01\").\n        group_df: pandas DataFrame with data for this group only.\n\n    Returns:\n        HyperparameterResult (best params, metrics) or DummyResult (skipped/failed).\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    import numpy as np\n\n    print(f\"\\n{'='*80}\")\n    print(f\"üîç Hyperparameter Search for Group: {group_name}\")\n    print(f\"{'='*80}\")\n\n    class DummyResult:\n        def __init__(self):\n            self.group_name = group_name\n            self.skipped = True\n\n    df = group_df\n    group_count = len(df)\n    print(f\"üìä Group data: {group_count:,} records\")\n\n    if group_count < 50:\n        print(\n            f\"‚ö†Ô∏è  WARNING: Group has less than 50 records. Skipping hyperparameter search.\"\n        )\n        print(f\"   Will use default hyperparameters for this group.\")\n        return DummyResult()\n\n    # Sample data for this group (or use full group if SAMPLE_RATE_PER_GROUP = 1.0)\n    if SAMPLE_RATE_PER_GROUP < 1.0:\n        sampled_df = df.sample(frac=SAMPLE_RATE_PER_GROUP, random_state=42)\n        sampled_count = len(sampled_df)\n        print(\n            f\"   Sampled: {sampled_count:,} records ({SAMPLE_RATE_PER_GROUP*100:.0f}% of {group_count:,} total)\"\n        )\n    else:\n        # Use full group for better hyperparameter search\n        sampled_df = df\n        sampled_count = group_count\n        print(f\"   Using full group: {sampled_count:,} records (100% of group)\")\n\n    # Define excluded columns (metadata columns, not features)\n    # Note: FEATURE_TIMESTAMP may not exist if using cleaned tables directly\n    excluded_cols = [\n        \"customer_id\",\n        \"brand_pres_ret\",\n        \"week\",\n        \"FEATURE_TIMESTAMP\",  # Feature Store timestamp column (may not exist in cleaned tables)\n        \"stats_ntile_group\",  # Group column - not a feature\n    ]\n\n    target_col = _get_target_column(sampled_df)\n    # Get feature columns (solo num√©ricas; Snowflake ML exige int/float/bool)\n    feature_cols_list = _get_feature_cols_numeric(sampled_df, excluded_cols, target_col)\n\n    # Prepare X and y\n    X = sampled_df[feature_cols_list].fillna(0)\n    y = sampled_df[target_col].fillna(0)\n\n    print(f\"   Data shape: X={X.shape}, y={y.shape}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}], mean: {y.mean():.2f}\")\n\n    # Split into train and validation sets\n    if len(X) < 20:\n        print(f\"‚ö†Ô∏è  WARNING: Not enough data for train/val split. Skipping.\")\n        return DummyResult()\n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    print(f\"   Train: {X_train.shape[0]:,} samples, Val: {X_val.shape[0]:,} samples\")\n\n    # Prepare data for Tuner using DataConnector\n    # DataConnector.from_dataframe() expects Snowpark DataFrames, not Pandas\n    # Usar copia expl√≠cita e √≠ndices reseteados para evitar advertencias de Pandas\n    train_data = X_train.reset_index(drop=True).copy()\n    train_data[target_col] = np.asarray(y_train)\n\n    val_data = X_val.reset_index(drop=True).copy()\n    val_data[target_col] = np.asarray(y_val)\n\n    train_snowpark = session.create_dataframe(train_data)\n    val_snowpark = session.create_dataframe(val_data)\n\n    train_dc = DataConnector.from_dataframe(train_snowpark)\n    val_dc = DataConnector.from_dataframe(val_snowpark)\n\n    # dataset_map must include both \"train\" and \"test\" keys (following HPO documentation)\n    dataset_map = {\"train\": train_dc, \"test\": val_dc}\n\n    # Modelo para este grupo (nombre clase Snowflake ML)\n    model_type = GROUP_MODEL.get(group_name, _DEFAULT_MODEL)\n    search_space = SEARCH_SPACES.get(model_type, SEARCH_SPACES[\"XGBRegressor\"])\n    print(f\"   Model: {model_type}\")\n\n    # Create training function for Tuner (model_type selects XGB/LGBM/SGD)\n    train_func = create_train_func_for_tuner(feature_cols_list, model_type, target_col)\n\n    tuner_config = TunerConfig(\n        metric=\"rmse\",\n        mode=\"min\",\n        search_alg=RandomSearch(),\n        num_trials=num_trials,\n        max_concurrent_trials=max_concurrent_trials,\n    )\n\n    # Create and run Tuner\n    print(f\"   ‚è≥ Starting Random Search ({model_type}, {num_trials} trials)...\")\n    start_time = time.time()\n\n    try:\n        tuner = Tuner(train_func, search_space, tuner_config)\n        results = tuner.run(dataset_map=dataset_map)\n\n        elapsed_time = time.time() - start_time\n\n        # Get best results (best_result is a single-row pd.DataFrame: config/* = hyperparams, other cols = metrics)\n        best_result = results.best_result\n        config_cols = [c for c in best_result.columns if str(c).startswith(\"config/\")]\n\n        def _to_native(v):\n            if hasattr(v, \"item\"):\n                return v.item()\n            return v\n\n        best_params = {\n            str(c).replace(\"config/\", \"\"): _to_native(best_result[c].iloc[0])\n            for c in config_cols\n        }\n        best_rmse = float(best_result[\"rmse\"].iloc[0]) if \"rmse\" in best_result.columns else None\n        best_mae = float(best_result[\"mae\"].iloc[0]) if \"mae\" in best_result.columns else None\n        if best_rmse is None:\n            raise ValueError(\"TunerResults best_result has no 'rmse' metric column\")\n        best_model = results.best_model\n\n        # Evaluate best model on validation set again for consistency\n        pred_result = best_model.predict(X_val)\n        pred_df = pred_result.to_pandas() if hasattr(pred_result, \"to_pandas\") else pred_result\n        out_col = best_model.get_output_cols()[0]\n        y_val_pred = np.asarray(pred_df[out_col])\n        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        val_mae = mean_absolute_error(y_val, y_val_pred)\n\n        print(f\"   ‚úÖ Completed in {elapsed_time:.1f}s\")\n        print(f\"      Best RMSE: {best_rmse:.4f}\")\n        print(f\"      Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n\n        # Store results in global dictionary (for summary later)\n        all_results[group_name] = {\n            \"best_params\": best_params,\n            \"best_cv_rmse\": best_rmse,\n            \"val_rmse\": val_rmse,\n            \"val_mae\": val_mae,\n            \"sample_size\": sampled_count,\n            \"algorithm\": model_type,\n        }\n\n        # Save to ML Experiments\n        search_id = f\"tune_{group_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        experiments_success = False\n\n        if experiments_available:\n            try:\n                # Create a run for this group's best hyperparameters\n                run_name = f\"best_{group_name}_{datetime.now().strftime('%H%M%S')}\"\n                with exp_tracking.start_run(run_name):\n                    # Log all hyperparameters\n                    exp_tracking.log_params(best_params)\n\n                    # Log metrics\n                    exp_tracking.log_metrics(\n                        {\n                            \"best_rmse\": float(best_rmse),\n                            \"val_rmse\": float(val_rmse),\n                            \"val_mae\": float(val_mae),\n                            \"sample_size\": int(sampled_count),\n                            \"num_trials\": int(num_trials),\n                        }\n                    )\n\n                    # Log group identifier as a tag/parameter\n                    exp_tracking.log_param(\"group_name\", group_name)\n                    exp_tracking.log_param(\"search_id\", search_id)\n                    exp_tracking.log_param(\"algorithm\", model_type)\n\n                print(f\"   ‚úÖ Results logged to ML Experiments (run: {run_name})\")\n                experiments_success = True\n            except Exception as e:\n                print(f\"   ‚ö†Ô∏è  Error logging to Experiments: {str(e)[:200]}\")\n                experiments_success = False\n\n        # Save to database ONLY if ML Experiments is not available or failed\n        # If Experiments works, we don't need the table\n        if not experiments_available or not experiments_success:\n            print(\n                f\"   üìã Saving to table (Experiments {'not available' if not experiments_available else 'failed'})\"\n            )\n\n            # Ensure table exists\n            session.sql(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS (\n                    search_id VARCHAR,\n                    group_name VARCHAR,\n                    algorithm VARCHAR,\n                    best_params VARIANT,\n                    best_cv_rmse FLOAT,\n                    best_cv_mae FLOAT,\n                    val_rmse FLOAT,\n                    val_mae FLOAT,\n                    n_iter INTEGER,\n                    sample_size INTEGER,\n                    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n                )\n            \"\"\"\n            ).collect()\n\n            best_params_json = json.dumps(\n                {\n                    k: float(v) if isinstance(v, (np.integer, np.floating)) else v\n                    for k, v in best_params.items()\n                }\n            )\n\n            best_mae_value = best_mae if best_mae is not None else None\n            best_mae_sql = (\n                f\"{best_mae_value:.6f}\" if best_mae_value is not None else \"NULL\"\n            )\n            best_params_escaped = best_params_json.replace(\"'\", \"''\")\n            search_id_escaped = search_id.replace(\"'\", \"''\")\n            group_name_escaped = group_name.replace(\"'\", \"''\")\n            algorithm_escaped = model_type.replace(\"'\", \"''\")\n\n            insert_sql = f\"\"\"\n                INSERT INTO BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n                (search_id, group_name, algorithm, best_params, best_cv_rmse, best_cv_mae, val_rmse, val_mae, n_iter, sample_size)\n                VALUES (\n                    '{search_id_escaped}',\n                    '{group_name_escaped}',\n                    '{algorithm_escaped}',\n                    PARSE_JSON('{best_params_escaped}'),\n                    {best_rmse:.6f},\n                    {best_mae_sql},\n                    {val_rmse:.6f},\n                    {val_mae:.6f},\n                    {num_trials},\n                    {sampled_count}\n                )\n            \"\"\"\n            session.sql(insert_sql).collect()\n            print(f\"   ‚úÖ Results saved to table\")\n        else:\n            print(f\"   ‚úÖ Results stored in ML Experiments only (table not needed)\")\n\n        # Create result object to return\n        class HyperparameterResult:\n            def __init__(self):\n                self.group_name = group_name\n                self.best_params = best_params\n                self.best_rmse = best_rmse\n                self.val_rmse = val_rmse\n                self.val_mae = val_mae\n                self.skipped = False\n\n        print(f\"{'='*80}\\n\")\n        return HyperparameterResult()\n\n    except Exception as e:\n        print(f\"   ‚ùå Error during hyperparameter search: {str(e)[:200]}\")\n        import traceback\n\n        print(f\"   Traceback: {traceback.format_exc()[:300]}\")\n        print(f\"   Will use default hyperparameters for this group.\")\n        print(f\"{'='*80}\\n\")\n\n        # Return a dummy object indicating failure\n        return DummyResult()\n\n",
      "id": "b178cfb3-bb4f-4cc2-ab1b-4ffde37c64b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5c. Ejecutar b√∫squeda de hiperpar√°metros (loop por grupo)\n",
      "id": "37349ac3-e168-49ad-a2c0-4e0701324ca7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# Run hyperparameter search per group (loop; no MMT to avoid Ray serialization)\n# Results are saved to BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS and/or ML Experiments.\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üöÄ HYPERPARAMETER SEARCH PER GROUP (sequential loop)\")\nprint(\"=\" * 80)\nprint(\"\\nRunning Random Search with Tuner for each group (no MMT).\\n\")\n\nstart_time = time.time()\ngroup_results = {}\n\nfor idx, group_name in enumerate(groups_list, 1):\n    print(f\"\\n[{idx}/{len(groups_list)}] Processing group: {group_name}\")\n    group_snowpark = train_df.filter(train_df[\"stats_ntile_group\"] == group_name)\n    group_df = group_snowpark.to_pandas()\n    result = run_hyperparameter_search_for_one_group(group_name, group_df)\n    group_results[group_name] = result\n\nend_time = time.time()\nelapsed_minutes = (end_time - start_time) / 60\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE\")\nprint(\"=\" * 80)\nprint(f\"\\n‚è±Ô∏è  Total search time: {elapsed_minutes:.2f} minutes\")\n\n# Review results by group\nprint(\"\\nüìä Hyperparameter Search Results by Group:\\n\")\nsuccessful_searches = 0\nfor group_name in groups_list:\n    result = group_results.get(group_name)\n    if result is None:\n        print(f\"‚ö†Ô∏è  {group_name}: No result\")\n        continue\n    if getattr(result, \"skipped\", True):\n        print(f\"‚ö†Ô∏è  {group_name}: Skipped (insufficient data or error)\")\n    else:\n        print(f\"‚úÖ {group_name}:\")\n        print(f\"   Best RMSE: {result.best_rmse:.4f}\")\n        print(f\"   Val RMSE: {result.val_rmse:.4f}, Val MAE: {result.val_mae:.4f}\")\n        successful_searches += 1\n\nprint(\n    f\"\\n‚úÖ Completed hyperparameter search for {successful_searches}/{len(groups_list)} groups\"\n)\n",
      "id": "44e6dfab-ea3f-4cb9-98ac-2ac15518f8a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Summary of All Results\n",
      "id": "7e7fb347-2057-498e-9a1e-b7b6c94ba22d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY OF ALL HYPERPARAMETER SEARCHES\")\nprint(\"=\" * 80)\n\n# Summary: Show results from Experiments or table\nif experiments_available:\n    print(\"\\nüìä Results Summary:\")\n    print(f\"   ‚úÖ All results stored in ML Experiments\")\n    print(f\"   ‚úÖ Experiment: {experiment_name}\")\n    print(f\"   ‚úÖ Groups processed: {len(all_results)}\")\n    print(f\"\\nüí° View results in Snowsight: AI & ML ‚Üí Experiments ‚Üí {experiment_name}\")\n\n    # Try to show summary from Experiments if possible\n    try:\n        # This is a conceptual query - actual API may vary\n        print(\"\\nüìä Sample results from Experiments:\")\n        for group_name, result in list(all_results.items())[:5]:\n            print(\n                f\"   {group_name}: RMSE={result['val_rmse']:.4f}, MAE={result['val_mae']:.4f}\"\n            )\n        if len(all_results) > 5:\n            print(f\"   ... and {len(all_results) - 5} more groups\")\n    except:\n        pass\nelse:\n    # Fallback to table summary if Experiments not available\n    print(\"\\nüìä Results from Table:\")\n    summary_df = session.sql(\n        \"\"\"\n        SELECT \n            group_name,\n            best_cv_rmse,\n            val_rmse,\n            val_mae,\n            sample_size,\n            created_at\n        FROM BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n        WHERE created_at >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n        ORDER BY group_name\n    \"\"\"\n    )\n    summary_df.show()\n\n    # Overall statistics\n    overall_stats = session.sql(\n        \"\"\"\n        SELECT \n            COUNT(*) AS TOTAL_SEARCHES,\n            AVG(best_cv_rmse) AS AVG_CV_RMSE,\n            AVG(val_rmse) AS AVG_VAL_RMSE,\n            MIN(val_rmse) AS MIN_VAL_RMSE,\n            MAX(val_rmse) AS MAX_VAL_RMSE\n        FROM BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n        WHERE created_at >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n    \"\"\"\n    )\n    print(\"\\nüìä Overall Statistics:\")\n    overall_stats.show()\n",
      "id": "314d5313-17e4-4daa-9df5-710ce4f90cb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Scale Cluster Down\n",
      "id": "e8ccd83c-2a8b-4e5a-a392-220c036fa79d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìâ SCALING CLUSTER DOWN\")\nprint(\"=\" * 80)\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n\n    print(\"‚è≥ Reduciendo cluster a 1 contenedor...\")\n    scale_cluster(\n        expected_cluster_size=1\n    )\n    print(\"‚úÖ Cluster reducido a 1 contenedor\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Error al reducir cluster: {str(e)[:200]}\")\n    print(\"   El cluster puede seguir escalado...\")\n",
      "id": "547e1fda-7dcb-41bf-ab33-a84b2c85fe5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Summary\n",
      "id": "5012aa00-21df-4525-b4af-38a256e3825f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úÖ Models: LGBMRegressor, XGBRegressor, SGDRegressor (per group)\")\nprint(\n    f\"   ‚úÖ Search method: Snowflake ML tune.search RandomSearch (per-group loop, no MMT)\"\n)\nprint(f\"   ‚úÖ Execution: Sequential loop over groups (avoids Ray serialization)\")\nprint(f\"   ‚úÖ Groups processed: {successful_searches}/{len(groups_list)}\")\nprint(f\"   ‚úÖ Trials per group: {num_trials}\")\nprint(f\"   ‚úÖ Sample rate per group: {SAMPLE_RATE_PER_GROUP*100:.0f}%\")\nprint(f\"   ‚è±Ô∏è  Total time: {elapsed_minutes:.2f} minutes\")\n\n# Calculate statistics from all_results if available\nif all_results:\n    avg_val_rmse = np.mean([r[\"val_rmse\"] for r in all_results.values()])\n    min_val_rmse = min([r[\"val_rmse\"] for r in all_results.values()])\n    max_val_rmse = max([r[\"val_rmse\"] for r in all_results.values()])\n\n    print(f\"   ‚úÖ Average Validation RMSE: {avg_val_rmse:.4f}\")\n    print(f\"   ‚úÖ Best Group RMSE: {min_val_rmse:.4f}\")\n    print(f\"   ‚úÖ Worst Group RMSE: {max_val_rmse:.4f}\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review hyperparameter results by group\")\nprint(\"   2. Run 04_many_model_training.py to train 16 models (one per group)\")\nprint(\"   3. Each model will use its group-specific hyperparameters\")\n\nprint(\"\\n\" + \"=\" * 80)\n",
      "id": "960c3f29-6311-435d-aba1-8bcceb0831c7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}