{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Hyperparameter Search (XGBoost + Snowflake ML Tune) - Per Group\n#\n## Overview\nThis script performs hyperparameter optimization using Snowflake ML's tune.search for XGBoost regression.\n**Performs separate hyperparameter search for each of the 16 stats_ntile_group groups.**\n#\n## What We'll Do:\n1. Load cleaned training data with stats_ntile_group\n2. Get all 16 unique groups\n3. For each group:\n   - Load group-specific data (sampled for efficiency)\n   - Prepare features and target\n   - Perform Random Search using snowflake.ml.modeling.tune.search\n   - Save best hyperparameters per group\n4. Generate summary of all hyperparameter results\n",
      "id": "02d4b8c2-7761-4d32-bab8-59472a402e57"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.modeling.tune import Tuner, TunerConfig, get_tuner_context\nfrom snowflake.ml.modeling.tune.search import RandomSearch, randint, uniform\nfrom snowflake.ml.data.data_connector import DataConnector\nfrom snowflake.ml.experiment import ExperimentTracking\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom datetime import datetime\nimport json\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "0daccd8c-0264-4f79-85df-d0e33eacba45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Get All Groups and Load Training Data\n",
      "id": "bc014d62-d70a-4c81-9588-0dbe695d848c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä GETTING ALL GROUPS\")\nprint(\"=\" * 80)\n\n# Get all unique groups from cleaned training table\ngroups_df = session.sql(\n    \"\"\"\n    SELECT DISTINCT stats_ntile_group AS GROUP_NAME\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE stats_ntile_group IS NOT NULL\n    ORDER BY stats_ntile_group\n\"\"\"\n)\n\ngroups_list = [row[\"GROUP_NAME\"] for row in groups_df.collect()]\nprint(f\"\\n‚úÖ Found {len(groups_list)} groups:\")\nfor i, group in enumerate(groups_list, 1):\n    print(f\"   {i:2d}. {group}\")\n\nif len(groups_list) != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {len(groups_list)}\")\n    print(\"   Continuing with available groups...\")\n",
      "id": "bf86f882-b8b1-494c-994b-2ab9f5604164",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Load Features from Feature Store\n",
      "id": "8dc00713-fbbb-4fda-ad16-1d05dbe3f346"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üè™ LOADING FEATURES FROM FEATURE STORE\")\nprint(\"=\" * 80)\n\n# Initialize Feature Store\nfs = FeatureStore(session=session, database=\"BD_AA_DEV\", name=\"SC_FEATURES_BMX\")\n\nprint(\"‚úÖ Feature Store initialized\")\n\n# Get FeatureView\ntry:\n    feature_view = fs.get_feature_view(\"UNI_BOX_FEATURES\", version=\"v1\")\n    print(\"‚úÖ FeatureView 'UNI_BOX_FEATURES' v1 loaded\")\nexcept Exception as e:\n    # Try v2 if v1 doesn't exist\n    try:\n        feature_view = fs.get_feature_view(\"UNI_BOX_FEATURES\", version=\"v2\")\n        print(\"‚úÖ FeatureView 'UNI_BOX_FEATURES' v2 loaded\")\n    except:\n        print(f\"‚ùå Error loading FeatureView: {str(e)}\")\n        print(\"   Please run 02_feature_store_setup.py first\")\n        raise\n\n# Materialize features (get the feature data)\nprint(\"\\n‚è≥ Materializing features from Feature Store...\")\nfeatures_df = feature_view.get_features()\n\n# Get target and stats_ntile_group from cleaned training table\nprint(\"‚è≥ Loading target variable and stats_ntile_group from training table...\")\ntarget_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\").select(\n    \"customer_id\", \"brand_pres_ret\", \"week\", \"uni_box_week\", \"stats_ntile_group\"\n)\n\n# Join features with target\nprint(\"‚è≥ Joining features with target...\")\ntrain_df = features_df.join(\n    target_df, on=[\"customer_id\", \"brand_pres_ret\", \"week\"], how=\"inner\"\n)\n\ntotal_rows = train_df.count()\nprint(f\"\\n‚úÖ Training data loaded from Feature Store\")\nprint(f\"   Total rows: {total_rows:,}\")\n",
      "id": "0ced9938-eb5f-4395-a4c7-a3f1b6882418",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Define Hyperparameter Search Space\n",
      "id": "ab1f4f8d-41e0-445f-bddb-0ff168a31530"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üéØ DEFINING HYPERPARAMETER SEARCH SPACE\")\nprint(\"=\" * 80)\n\n# Define parameter distributions for Random Search using Snowflake ML tune.search\nsearch_space = {\n    \"n_estimators\": randint(50, 300),\n    \"max_depth\": randint(3, 10),\n    \"learning_rate\": uniform(0.01, 0.3),\n    \"subsample\": uniform(0.6, 1.0),\n    \"colsample_bytree\": uniform(0.6, 1.0),\n    \"min_child_weight\": randint(1, 7),\n    \"gamma\": uniform(0, 0.5),\n    \"reg_alpha\": uniform(0, 1),\n    \"reg_lambda\": uniform(0, 1),\n}\n\nprint(\"\\nüìã Hyperparameter Search Space (using snowflake.ml.modeling.tune.search):\")\nfor param, dist in search_space.items():\n    if hasattr(dist, \"low\") and hasattr(dist, \"high\"):\n        if hasattr(dist, \"base\"):  # loguniform\n            print(f\"   {param}: loguniform({dist.low:.2f}, {dist.high:.2f})\")\n        else:  # uniform or randint\n            if isinstance(dist, randint):\n                print(f\"   {param}: randint({dist.low}, {dist.high})\")\n            else:\n                print(f\"   {param}: uniform({dist.low:.2f}, {dist.high:.2f})\")\n\n# Number of trials for Random Search (reduced per group for efficiency)\nnum_trials = 30  # Reduced from 50 since we're doing 16 groups\nprint(f\"\\nüî¢ Random Search trials per group: {num_trials}\")\n\n# Sample rate per group for hyperparameter search\n# Options:\n#   - 1.0 = Use full group (most accurate but slower)\n#   - 0.5 = Use 50% of group (balanced)\n#   - 0.1 = Use 10% of group (faster, less accurate)\n# Note: For large datasets, using full group (1.0) is recommended for better hyperparameter tuning\nsample_rate = 0.1\nprint(f\"üìä Sample rate per group: {sample_rate*100:.0f}%\")\nif sample_rate < 1.0:\n    print(f\"   ‚ö†Ô∏è  Using {sample_rate*100:.0f}% of data - consider using 1.0 (full group) for better results\")\nelse:\n    print(f\"   ‚úÖ Using full group data for optimal hyperparameter search\")\n",
      "id": "b6bb07f0-2e6f-4ea1-a6e5-2180f99ccd83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Perform Hyperparameter Search Per Group\n",
      "id": "7ddb73c1-e024-4901-b5b4-6ac183357460"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç PERFORMING HYPERPARAMETER SEARCH PER GROUP\")\nprint(\"=\" * 80)\n\n# Create results table ONLY if ML Experiments is not available\n# If Experiments is available, we don't need this table\nif not experiments_available:\n    print(\"\\nüìã Creating HYPERPARAMETER_RESULTS table (Experiments not available)\")\n    session.sql(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS (\n            search_id VARCHAR,\n            group_name VARCHAR,\n            algorithm VARCHAR,\n            best_params VARIANT,\n            best_cv_rmse FLOAT,\n            best_cv_mae FLOAT,\n            val_rmse FLOAT,\n            val_mae FLOAT,\n            n_iter INTEGER,\n            sample_size INTEGER,\n            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\n    \"\"\"\n    ).collect()\n    print(\"   ‚úÖ Table created (will be used as primary storage)\")\nelse:\n    print(\"\\nüìã Skipping table creation (using ML Experiments as primary storage)\")\n\n# Define excluded columns (metadata columns from Feature Store)\nexcluded_cols = [\n    \"customer_id\",\n    \"brand_pres_ret\",\n    \"week\",\n    \"FEATURE_TIMESTAMP\",  # Feature Store timestamp column\n    \"stats_ntile_group\",  # Group column - not a feature\n]\n\n# Get feature columns from first group (all groups should have same features)\nsample_group_df = train_df.filter(train_df[\"stats_ntile_group\"] == groups_list[0])\nsample_pandas = sample_group_df.limit(1).to_pandas()\nfeature_cols = [\n    col for col in sample_pandas.columns \n    if col not in excluded_cols + [\"uni_box_week\"]\n]\n\nprint(f\"\\nüìã Features ({len(feature_cols)}):\")\nfor col in sorted(feature_cols):\n    print(f\"   - {col}\")\n\n# Dictionary to store all results\nall_results = {}\n\n# Initialize ML Experiments for hyperparameter tracking\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üî¨ INITIALIZING ML EXPERIMENTS\")\nprint(\"=\" * 80)\n\ntry:\n    exp_tracking = ExperimentTracking(session)\n    experiment_name = f\"hyperparameter_search_xgboost_{datetime.now().strftime('%Y%m%d')}\"\n    exp_tracking.set_experiment(experiment_name)\n    print(f\"‚úÖ Experiment created: {experiment_name}\")\n    experiments_available = True\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will continue with table-based storage only\")\n    exp_tracking = None\n    experiments_available = False\n\n\ndef create_train_func(group_name, feature_cols, X_val, y_val):\n    \"\"\"\n    Create a training function for the Tuner.\n    This function will be called for each trial with different hyperparameters.\n    \"\"\"\n    def train_func():\n        tuner_context = get_tuner_context()\n        params = tuner_context.get_hyper_params()\n        dm = tuner_context.get_dataset_map()\n        \n        # Load data from DataConnector\n        train_pd = dm[\"train\"].to_pandas()\n        \n        # Prepare features and target\n        X_train = train_pd[feature_cols].fillna(0)\n        y_train = train_pd[\"uni_box_week\"].fillna(0)\n        \n        # Build model with hyperparameters from tuner\n        xgb_params = params.copy()\n        xgb_params[\"random_state\"] = 42\n        xgb_params[\"n_jobs\"] = -1\n        xgb_params[\"objective\"] = \"reg:squarederror\"\n        xgb_params[\"eval_metric\"] = \"rmse\"\n        \n        # Train model\n        model = XGBRegressor(**xgb_params)\n        model.fit(X_train, y_train)\n        \n        # Evaluate on validation set (using outer scope variables)\n        y_val_pred = model.predict(X_val)\n        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        val_mae = mean_absolute_error(y_val, y_val_pred)\n        \n        # Report metrics (negative RMSE for minimization)\n        tuner_context.report(metrics={\"rmse\": val_rmse, \"mae\": val_mae}, model=model)\n    \n    return train_func\n\n\n# Process each group\n# IMPORTANTE: Cada iteraci√≥n procesa UN grupo espec√≠fico y guarda sus hiperpar√°metros\n# La conexi√≥n grupo-hiperpar√°metros se hace mediante el campo 'group_name' que se guarda\n# en la tabla HYPERPARAMETER_RESULTS. Luego en el script 04, se usa este 'group_name'\n# para cargar los hiperpar√°metros correctos para cada modelo.\nfor group_idx, group_name in enumerate(groups_list, 1):\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"üîç Processing Group {group_idx}/{len(groups_list)}: {group_name}\")\n    print(\"=\" * 80)\n    \n    # Filter data for this group\n    # FILTRO CLAVE: Solo procesamos datos donde stats_ntile_group == group_name\n    # Esto asegura que cada b√∫squeda de hiperpar√°metros usa SOLO datos de ese grupo\n    group_df = train_df.filter(train_df[\"stats_ntile_group\"] == group_name)\n    group_count = group_df.count()\n    \n    print(f\"\\nüìä Group data: {group_count:,} records\")\n    \n    if group_count < 50:\n        print(f\"‚ö†Ô∏è  WARNING: Group has less than 50 records. Skipping hyperparameter search.\")\n        print(f\"   Will use default hyperparameters for this group.\")\n        continue\n    \n    # Sample data for this group (or use full group if sample_rate = 1.0)\n    if sample_rate < 1.0:\n        sampled_group_df = group_df.sample(fraction=sample_rate, seed=42)\n        sampled_count = sampled_group_df.count()\n        print(f\"   Sampled: {sampled_count:,} records ({sample_rate*100:.0f}% of {group_count:,} total)\")\n    else:\n        # Use full group for better hyperparameter search\n        sampled_group_df = group_df\n        sampled_count = group_count\n        print(f\"   Using full group: {sampled_count:,} records (100% of group)\")\n    \n    # Convert to pandas\n    print(\"   Converting to pandas...\")\n    df_group = sampled_group_df.to_pandas()\n    \n    # Prepare X and y\n    X = df_group[feature_cols].fillna(0)\n    y = df_group[\"uni_box_week\"].fillna(0)\n    \n    print(f\"   Data shape: X={X.shape}, y={y.shape}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}], mean: {y.mean():.2f}\")\n    \n    # Split into train and validation sets\n    if len(X) < 20:\n        print(f\"‚ö†Ô∏è  WARNING: Not enough data for train/val split. Skipping.\")\n        continue\n        \n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    print(f\"   Train: {X_train.shape[0]:,} samples, Val: {X_val.shape[0]:,} samples\")\n    \n    # Prepare data for Tuner using DataConnector\n    # Combine features and target for train set\n    train_data = X_train.copy()\n    train_data[\"uni_box_week\"] = y_train.values\n    \n    train_dc = DataConnector.from_dataframe(train_data)\n    dataset_map = {\"train\": train_dc}\n    \n    # Create training function for this group\n    train_func = create_train_func(group_name, feature_cols, X_val, y_val)\n    \n    # Configure Tuner\n    tuner_config = TunerConfig(\n        metric=\"rmse\",\n        mode=\"min\",  # Minimize RMSE\n        search_alg=RandomSearch(),\n        num_trials=num_trials,\n        max_concurrent_trials=1,  # Run sequentially per group\n    )\n    \n    # Create and run Tuner\n    print(f\"   ‚è≥ Starting Random Search using snowflake.ml.modeling.tune ({num_trials} trials)...\")\n    import time\n    start_time = time.time()\n    \n    try:\n        tuner = Tuner(train_func, search_space, tuner_config)\n        results = tuner.run(dataset_map=dataset_map)\n        \n        elapsed_time = time.time() - start_time\n        \n        # Get best results\n        best_result = results.best_result\n        best_params = best_result.hyperparameters\n        best_rmse = best_result.metrics[\"rmse\"]\n        best_mae = best_result.metrics.get(\"mae\", None)\n        best_model = results.best_model\n        \n        # Evaluate best model on validation set again for consistency\n        y_val_pred = best_model.predict(X_val)\n        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        val_mae = mean_absolute_error(y_val, y_val_pred)\n        \n        print(f\"   ‚úÖ Completed in {elapsed_time:.1f}s\")\n        print(f\"      Best RMSE: {best_rmse:.4f}\")\n        print(f\"      Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n        \n        # Store results\n        all_results[group_name] = {\n            \"best_params\": best_params,\n            \"best_cv_rmse\": best_rmse,\n            \"val_rmse\": val_rmse,\n            \"val_mae\": val_mae,\n            \"sample_size\": sampled_count,\n        }\n        \n        # Save to ML Experiments\n        search_id = f\"xgb_snowflake_tune_{group_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        experiments_success = False\n        \n        if experiments_available:\n            try:\n                # Create a run for this group's best hyperparameters\n                run_name = f\"best_{group_name}_{datetime.now().strftime('%H%M%S')}\"\n                with exp_tracking.start_run(run_name):\n                    # Log all hyperparameters\n                    exp_tracking.log_params(best_params)\n                    \n                    # Log metrics\n                    exp_tracking.log_metrics({\n                        \"best_rmse\": float(best_rmse),\n                        \"val_rmse\": float(val_rmse),\n                        \"val_mae\": float(val_mae),\n                        \"sample_size\": int(sampled_count),\n                        \"num_trials\": int(num_trials),\n                    })\n                    \n                    # Log group identifier as a tag/parameter\n                    exp_tracking.log_param(\"group_name\", group_name)\n                    exp_tracking.log_param(\"search_id\", search_id)\n                    exp_tracking.log_param(\"algorithm\", \"XGBoost\")\n                    \n                    # Optionally log the best model (if you want to keep it in experiments)\n                    # exp_tracking.log_model(best_model, model_name=f\"best_model_{group_name}\")\n                \n                print(f\"   ‚úÖ Results logged to ML Experiments (run: {run_name})\")\n                experiments_success = True\n            except Exception as e:\n                print(f\"   ‚ö†Ô∏è  Error logging to Experiments: {str(e)[:200]}\")\n                experiments_success = False\n        \n        # Save to database ONLY if ML Experiments is not available or failed\n        # If Experiments works, we don't need the table\n        if not experiments_available or not experiments_success:\n            print(f\"   üìã Saving to table (Experiments {'not available' if not experiments_available else 'failed'})\")\n            \n            # Ensure table exists\n            session.sql(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS (\n                    search_id VARCHAR,\n                    group_name VARCHAR,\n                    algorithm VARCHAR,\n                    best_params VARIANT,\n                    best_cv_rmse FLOAT,\n                    best_cv_mae FLOAT,\n                    val_rmse FLOAT,\n                    val_mae FLOAT,\n                    n_iter INTEGER,\n                    sample_size INTEGER,\n                    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n                )\n            \"\"\"\n            ).collect()\n            \n            best_params_json = json.dumps(\n                {\n                    k: float(v) if isinstance(v, (np.integer, np.floating)) else v\n                    for k, v in best_params.items()\n                }\n            )\n            \n            best_mae_value = best_mae if best_mae is not None else None\n            best_mae_sql = f\"{best_mae_value:.6f}\" if best_mae_value is not None else \"NULL\"\n            \n            # Guardar resultados con group_name como identificador del grupo\n            insert_sql = f\"\"\"\n                INSERT INTO BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n                (search_id, group_name, algorithm, best_params, best_cv_rmse, best_cv_mae, val_rmse, val_mae, n_iter, sample_size)\n                VALUES (\n                    '{search_id}',\n                    '{group_name}',\n                    'XGBoost',\n                    PARSE_JSON('{best_params_json}'),\n                    {best_rmse:.6f},\n                    {best_mae_sql},\n                    {val_rmse:.6f},\n                    {val_mae:.6f},\n                    {num_trials},\n                    {sampled_count}\n                )\n            \"\"\"\n            \n            session.sql(insert_sql).collect()\n            print(f\"   ‚úÖ Results saved to table\")\n        else:\n            print(f\"   ‚úÖ Results stored in ML Experiments only (table not needed)\")\n        \n    except Exception as e:\n        print(f\"   ‚ùå Error during hyperparameter search: {str(e)[:200]}\")\n        import traceback\n        print(f\"   Traceback: {traceback.format_exc()[:300]}\")\n        print(f\"   Will use default hyperparameters for this group.\")\n        continue\n\nprint(f\"\\n‚úÖ Completed hyperparameter search for {len(all_results)}/{len(groups_list)} groups\")\n",
      "id": "659b79bb-a574-4c0e-a1a7-256862ac5905",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Summary of All Results\n",
      "id": "3e618810-7491-4075-9d03-4e85443fd6ed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY OF ALL HYPERPARAMETER SEARCHES\")\nprint(\"=\" * 80)\n\n# Summary: Show results from Experiments or table\nif experiments_available:\n    print(\"\\nüìä Results Summary:\")\n    print(f\"   ‚úÖ All results stored in ML Experiments\")\n    print(f\"   ‚úÖ Experiment: {experiment_name}\")\n    print(f\"   ‚úÖ Groups processed: {len(all_results)}\")\n    print(f\"\\nüí° View results in Snowsight: AI & ML ‚Üí Experiments ‚Üí {experiment_name}\")\n    \n    # Try to show summary from Experiments if possible\n    try:\n        # This is a conceptual query - actual API may vary\n        print(\"\\nüìä Sample results from Experiments:\")\n        for group_name, result in list(all_results.items())[:5]:\n            print(f\"   {group_name}: RMSE={result['val_rmse']:.4f}, MAE={result['val_mae']:.4f}\")\n        if len(all_results) > 5:\n            print(f\"   ... and {len(all_results) - 5} more groups\")\n    except:\n        pass\nelse:\n    # Fallback to table summary if Experiments not available\n    print(\"\\nüìä Results from Table:\")\n    summary_df = session.sql(\n        \"\"\"\n        SELECT \n            group_name,\n            best_cv_rmse,\n            val_rmse,\n            val_mae,\n            sample_size,\n            created_at\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n        WHERE created_at >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n        ORDER BY group_name\n    \"\"\"\n    )\n    summary_df.show()\n    \n    # Overall statistics\n    overall_stats = session.sql(\n        \"\"\"\n        SELECT \n            COUNT(*) AS TOTAL_SEARCHES,\n            AVG(best_cv_rmse) AS AVG_CV_RMSE,\n            AVG(val_rmse) AS AVG_VAL_RMSE,\n            MIN(val_rmse) AS MIN_VAL_RMSE,\n            MAX(val_rmse) AS MAX_VAL_RMSE\n        FROM BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n        WHERE created_at >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n    \"\"\"\n    )\n    print(\"\\nüìä Overall Statistics:\")\n    overall_stats.show()\n",
      "id": "911ac7c9-448f-48b6-8c85-9aa37b522c4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Summary\n",
      "id": "cc7a2701-7886-40b1-b20e-cc3dda7133f8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úÖ Algorithm: XGBoost\")\nprint(f\"   ‚úÖ Search method: Snowflake ML tune.search RandomSearch (per group)\")\nprint(f\"   ‚úÖ Groups processed: {len(all_results)}/{len(groups_list)}\")\nprint(f\"   ‚úÖ Trials per group: {num_trials}\")\nprint(f\"   ‚úÖ Sample rate per group: {sample_rate*100:.0f}%\")\n\nif all_results:\n    avg_val_rmse = np.mean([r[\"val_rmse\"] for r in all_results.values()])\n    min_val_rmse = min([r[\"val_rmse\"] for r in all_results.values()])\n    max_val_rmse = max([r[\"val_rmse\"] for r in all_results.values()])\n    \n    print(f\"   ‚úÖ Average Validation RMSE: {avg_val_rmse:.4f}\")\n    print(f\"   ‚úÖ Best Group RMSE: {min_val_rmse:.4f}\")\n    print(f\"   ‚úÖ Worst Group RMSE: {max_val_rmse:.4f}\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review hyperparameter results by group\")\nprint(\"   2. Run 04_many_model_training.py to train 16 models (one per group)\")\nprint(\"   3. Each model will use its group-specific hyperparameters\")\n\nprint(\"\\n\" + \"=\" * 80)\n",
      "id": "c35c452b-19f8-44f4-8937-7a6643407ba9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}