{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Hyperparameter Search (LGBM/XGB + Snowflake ML Tune) - Per Group\n#\n## Overview\nThis script performs hyperparameter optimization using Snowflake ML's tune.search for regression (LGBM, XGBoost per group).\n**Runs HPO per group in a sequential loop (no MMT) to avoid Ray serialization issues; each group runs its own Random Search.**\n#\n## What We'll Do:\n1. Load cleaned training data with stats_ntile_group\n2. Get all 16 unique groups\n3. For each group (loop): load group data, run Random Search with snowflake.ml.modeling.tune, save best hyperparameters to SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n4. Generate summary of all hyperparameter results\n",
      "id": "5a78a0d4-6903-4442-b1ec-15aeb5a06a2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.modeling.tune import (\n    Tuner,\n    TunerConfig,\n    get_tuner_context,\n    randint,\n    uniform,\n)\nfrom snowflake.ml.modeling.tune.search import RandomSearch\nfrom snowflake.ml.data.data_connector import DataConnector\nfrom snowflake.ml.experiment import ExperimentTracking\nimport numpy as np\nfrom datetime import datetime\nimport json\nimport time\n\nsession = get_active_session()\n\n# Configuration: Database, schemas, and tables\nDATABASE = \"BD_AA_DEV\"\nSTORAGE_SCHEMA = \"SC_STORAGE_BMX_PS\"\nFEATURES_SCHEMA = \"SC_FEATURES_BMX\"\nMODELS_SCHEMA = \"SC_MODELS_BMX\"\nTRAIN_TABLE_CLEANED = f\"{DATABASE}.{STORAGE_SCHEMA}.TRAIN_DATASET_CLEANED\"\nFEATURES_TABLE = f\"{DATABASE}.{FEATURES_SCHEMA}.UNI_BOX_FEATURES\"\nFEATURE_VERSIONS_TABLE = f\"{DATABASE}.{FEATURES_SCHEMA}.FEATURE_VERSIONS\"\nHYPERPARAMETER_RESULTS_TABLE = f\"{DATABASE}.{MODELS_SCHEMA}.HYPERPARAMETER_RESULTS\"\nDEFAULT_WAREHOUSE = \"WH_AA_DEV_DS_SQL\"\n\n# Column constants\nTARGET_COLUMN = \"UNI_BOX_WEEK\"\nSTATS_NTILE_GROUP_COL = \"STATS_NTILE_GROUP\"\n\n# Excluded columns (metadata columns, not features) - defined once at the beginning\nEXCLUDED_COLS = [\n    \"CUSTOMER_ID\",\n    \"BRAND_PRES_RET\",\n    \"PROD_KEY\",\n    \"WEEK\",\n    \"FEATURE_TIMESTAMP\",\n    STATS_NTILE_GROUP_COL,\n]\n\n# Hyperparameter search configuration\nNUM_TRIALS = 15\nMAX_CONCURRENT_TRIALS = 4\nSAMPLE_RATE_PER_GROUP = 0.2\n\n# Cluster scaling configuration\nCLUSTER_SIZE_HPO = 5\nCLUSTER_SIZE_MIN_HPO = 2\nCLUSTER_SIZE_DOWN = 1\n\n# Set context\nsession.sql(f\"USE DATABASE {DATABASE}\").collect()\nsession.sql(f\"USE SCHEMA {STORAGE_SCHEMA}\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n\n# Resolve active feature version (for traceability)\nfeature_version_id = None\nfeature_snapshot_at = None\ntry:\n    version_rows = session.sql(\n        f\"\"\"\n        SELECT FEATURE_VERSION_ID, FEATURE_SNAPSHOT_AT, CREATED_AT\n        FROM {FEATURE_VERSIONS_TABLE}\n        WHERE FEATURE_TABLE_NAME = 'UNI_BOX_FEATURES'\n          AND IS_ACTIVE = TRUE\n        ORDER BY CREATED_AT DESC\n        LIMIT 1\n    \"\"\"\n    ).collect()\n    if version_rows:\n        feature_version_id = version_rows[0][\"FEATURE_VERSION_ID\"]\n        feature_snapshot_at = version_rows[0][\"FEATURE_SNAPSHOT_AT\"]\n        print(\"\\nüìå Active feature version for training:\")\n        print(f\"   FEATURE_VERSION_ID: {feature_version_id}\")\n        print(f\"   FEATURE_SNAPSHOT_AT: {feature_snapshot_at}\")\n    else:\n        print(\"\\n‚ö†Ô∏è  No active feature version found in FEATURE_VERSIONS; proceeding without version metadata\")\nexcept Exception as e:\n    print(f\"\\n‚ö†Ô∏è  Could not resolve feature version: {str(e)[:200]}\")\n\n# Configuration:\n# - If you don't have permissions for FeatureView/Dynamic Tables, use cleaned tables.\n# - Flag is kept with automatic fallback if Feature Store mode fails.\nUSE_CLEANED_TABLES = (\n    False  # True = TRAIN_DATASET_CLEANED, False = try Feature Store\n)\n\n# Single object: group -> Snowflake ML class name (snowflake.ml.modeling.*).\n# All groups use boosting models (XGB/LGBM) for regression.\nGROUP_MODEL = {\n    \"group_stat_0_1\": \"LGBMRegressor\",\n    \"group_stat_0_2\": \"LGBMRegressor\",\n    \"group_stat_0_3\": \"LGBMRegressor\",\n    \"group_stat_0_4\": \"LGBMRegressor\",\n    \"group_stat_1_1\": \"LGBMRegressor\",\n    \"group_stat_1_2\": \"LGBMRegressor\",\n    \"group_stat_1_3\": \"XGBRegressor\",\n    \"group_stat_1_4\": \"XGBRegressor\",\n    \"group_stat_2_1\": \"LGBMRegressor\",\n    \"group_stat_2_2\": \"LGBMRegressor\",\n    \"group_stat_2_3\": \"XGBRegressor\",\n    \"group_stat_2_4\": \"XGBRegressor\",\n    \"group_stat_3_1\": \"LGBMRegressor\",\n    \"group_stat_3_2\": \"LGBMRegressor\",\n    \"group_stat_3_3\": \"LGBMRegressor\",\n    \"group_stat_3_4\": \"XGBRegressor\",\n}\n_DEFAULT_MODEL = \"XGBRegressor\"\n",
      "id": "45038b70-036e-47a8-8124-e4ea26b910e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Get All Groups and Load Training Data\n",
      "id": "ef5ad7f2-7c96-4e6c-95fe-1f4983b0b9b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä GETTING ALL GROUPS\")\nprint(\"=\" * 80)\n\n# Get all unique groups from cleaned training table\ngroups_df = session.sql(\n    f\"\"\"\n    SELECT DISTINCT {STATS_NTILE_GROUP_COL} AS GROUP_NAME\n    FROM {TRAIN_TABLE_CLEANED}\n    WHERE {STATS_NTILE_GROUP_COL} IS NOT NULL\n    ORDER BY {STATS_NTILE_GROUP_COL}\n\"\"\"\n)\n\ngroups_list = [row[\"GROUP_NAME\"] for row in groups_df.collect()]\nprint(f\"\\n‚úÖ Found {len(groups_list)} groups:\")\nfor i, group in enumerate(groups_list, 1):\n    print(f\"   {i:2d}. {group}\")\n\nif len(groups_list) != 16:\n    print(f\"\\n‚ö†Ô∏è  WARNING: Expected 16 groups, found {len(groups_list)}\")\n    print(\"   Continuing with available groups...\")\n",
      "id": "0dbe4b75-9729-4149-a7ba-d4b6b0acfff1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Load Training Data (Feature Store or Cleaned Tables)\n",
      "id": "6b05c197-ebb4-4245-b155-3296e7a5031a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "if USE_CLEANED_TABLES:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üìä LOADING DATA FROM CLEANED TABLES (TESTING MODE)\")\n    print(\"=\" * 80)\n\n    # Load directly from cleaned training table (for testing purposes)\n    print(\"‚è≥ Loading data from TRAIN_DATASET_CLEANED...\")\n    train_df = session.table(TRAIN_TABLE_CLEANED)\n\n    total_rows = train_df.count()\n    print(f\"\\n‚úÖ Training data loaded from cleaned table\")\n    print(f\"   Total rows: {total_rows:,}\")\n    print(f\"   ‚ö†Ô∏è  TESTING MODE: Using cleaned tables directly (not Feature Store)\")\nelse:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üè™ FEATURE STORE MODE (WITHOUT FEATUREVIEW)\")\n    print(\"=\" * 80)\n\n    # Instead of FeatureView, we support a materialized features table created by `02_feature_store_setup.py`\n    # (without Dynamic Tables). If it doesn't exist or fails, we fallback to cleaned tables.\n    try:\n        # Initialize Feature Store (even though we don't use FeatureView)\n        _fs = FeatureStore(\n            session=session,\n            database=DATABASE,\n            name=FEATURES_SCHEMA,\n            default_warehouse=DEFAULT_WAREHOUSE,\n        )\n        print(\"‚úÖ Feature Store initialized (without FeatureView)\")\n\n        print(f\"‚è≥ Loading features from table: {FEATURES_TABLE} ...\")\n        features_df = session.table(FEATURES_TABLE)\n\n        print(f\"‚è≥ Loading target variable and {STATS_NTILE_GROUP_COL} from training table...\")\n        target_df = session.table(TRAIN_TABLE_CLEANED).select(\n            \"CUSTOMER_ID\", \"BRAND_PRES_RET\", \"PROD_KEY\", \"WEEK\", TARGET_COLUMN, STATS_NTILE_GROUP_COL\n        )\n\n        print(\"‚è≥ Joining features with target...\")\n        train_df = features_df.join(\n            target_df, on=[\"CUSTOMER_ID\", \"BRAND_PRES_RET\", \"PROD_KEY\", \"WEEK\"], how=\"inner\"\n        )\n\n        total_rows = train_df.count()\n        print(f\"\\n‚úÖ Training data loaded from features table + target\")\n        print(f\"   Total rows: {total_rows:,}\")\n    except Exception as e:\n        print(\n            f\"‚ö†Ô∏è  Could not load/join features table ({FEATURES_TABLE}): {str(e)[:200]}\"\n        )\n        print(\n            \"   Falling back to TRAIN_DATASET_CLEANED (USE_CLEANED_TABLES=True behavior)\"\n        )\n        train_df = session.table(TRAIN_TABLE_CLEANED)\n        total_rows = train_df.count()\n        print(f\"\\n‚úÖ Training data loaded from cleaned table (fallback)\")\n        print(f\"   Total rows: {total_rows:,}\")\n",
      "id": "d5f9911f-f73f-4a80-864c-d4e6765ab7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Define Hyperparameter Search Space\n",
      "id": "5e061ee0-37c0-48d3-8e78-da688f1158a3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üéØ DEFINING HYPERPARAMETER SEARCH SPACE\")\nprint(\"=\" * 80)\n\n# Search spaces per model type (LGBM, XGBoost)\nSEARCH_SPACES = {\n    \"XGBRegressor\": {\n        \"n_estimators\": randint(50, 300),\n        \"max_depth\": randint(3, 10),\n        \"learning_rate\": uniform(0.01, 0.3),\n        \"subsample\": uniform(0.6, 1.0),\n        \"colsample_bytree\": uniform(0.6, 1.0),\n        \"min_child_weight\": randint(1, 7),\n        \"gamma\": uniform(0, 0.5),\n        \"reg_alpha\": uniform(0, 1),\n        \"reg_lambda\": uniform(0, 1),\n    },\n    \"LGBMRegressor\": {\n        \"n_estimators\": randint(50, 300),\n        \"max_depth\": randint(3, 10),\n        \"learning_rate\": uniform(0.01, 0.3),\n        \"num_leaves\": randint(20, 150),\n        \"subsample\": uniform(0.6, 1.0),\n        \"colsample_bytree\": uniform(0.6, 1.0),\n        \"reg_alpha\": uniform(0, 1),\n        \"reg_lambda\": uniform(0, 1),\n        \"min_child_samples\": randint(5, 50),\n    },\n}\n\nprint(\"\\nüìã Hyperparameter Search Spaces (per model type):\")\nfor model_type, search_space in SEARCH_SPACES.items():\n    print(f\"   {model_type}: {list(search_space.keys())}\")\nprint(\"\\nüìã XGBRegressor search space (example):\")\nfor param, dist in SEARCH_SPACES[\"XGBRegressor\"].items():\n    if hasattr(dist, \"low\") and hasattr(dist, \"high\"):\n        dist_name = dist.__class__.__name__.lower()\n        if \"rand\" in dist_name and \"int\" in dist_name:\n            print(f\"   {param}: randint({dist.low}, {dist.high})\")\n        else:\n            print(f\"   {param}: uniform({dist.low:.2f}, {dist.high:.2f})\")\n\nprint(f\"\\nüî¢ Random Search trials per group: {NUM_TRIALS}\")\nprint(f\"   Max concurrent trials per group: {MAX_CONCURRENT_TRIALS}\")\nprint(f\"üìä Sample rate per group: {SAMPLE_RATE_PER_GROUP*100:.0f}%\")\nif SAMPLE_RATE_PER_GROUP < 1.0:\n    print(\n        f\"   ‚ö†Ô∏è  Using {SAMPLE_RATE_PER_GROUP*100:.0f}% of data - consider using 1.0 (full group) for better results\"\n    )\nelse:\n    print(f\"   ‚úÖ Using full group data for optimal hyperparameter search\")\n",
      "id": "08d9534f-b71d-41b0-ae84-41ecfac2f51c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 3c. (Optional) View Ray Dashboard\n#\nUse this cell in Snowflake Notebooks to get the Ray Dashboard URL\nassociated with the current runtime. Copy and paste the URL in your browser.\n",
      "id": "42822851-444a-43f7-b407-ca19bddfad8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "try:\n    from snowflake.ml.runtime_cluster import get_ray_dashboard_url\n\n    dashboard_url = get_ray_dashboard_url()\n    print(f\"‚úÖ Access the Ray Dashboard here: {dashboard_url}\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è Could not get Ray Dashboard URL.\")\n    print(f\"   Detail: {str(e)[:200]}\")\n",
      "id": "58369e1b-4c45-48b7-a501-f6202e66f6ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Scale Cluster for HPO\n",
      "id": "f5b368df-0fdc-47d2-a58c-0528c1435143"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà SCALING CLUSTER FOR HPO\")\nprint(\"=\" * 80)\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n\n    print(f\"‚è≥ Scaling cluster to {CLUSTER_SIZE_HPO} containers...\")\n    scale_cluster(\n        expected_cluster_size=CLUSTER_SIZE_HPO,\n        options={\n            \"block_until_min_cluster_size\": CLUSTER_SIZE_MIN_HPO\n        }\n    )\n    print(f\"‚úÖ Cluster scaled to {CLUSTER_SIZE_HPO} containers\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Error scaling cluster: {str(e)[:200]}\")\n    print(\"   Continuing with current cluster...\")\n",
      "id": "b339662f-a155-4557-954a-5b854e7b99ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Perform Hyperparameter Search Per Group\n",
      "id": "13a56a39-0350-4ddb-9b89-4c8a373538fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5a. Setup: ML Experiments, table (if applicable), features and all_results\n",
      "id": "c49ebb74-b2da-41e5-9a69-8ccf6bae83f7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç PERFORMING HYPERPARAMETER SEARCH PER GROUP\")\nprint(\"=\" * 80)\n\n# Initialize ML Experiments for hyperparameter tracking FIRST\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üî¨ INITIALIZING ML EXPERIMENTS\")\nprint(\"=\" * 80)\n\n# Initialize experiment_name in global scope (multi-model: LGBMRegressor, XGBRegressor, SGDRegressor)\nexperiment_name = (\n    f\"hyperparameter_search_regression_{datetime.now().strftime('%Y%m%d')}\"\n)\n\ntry:\n    exp_tracking = ExperimentTracking(session)\n    exp_tracking.set_experiment(experiment_name)\n    print(f\"‚úÖ Experiment created: {experiment_name}\")\n    experiments_available = True\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will continue with table-based storage only\")\n    exp_tracking = None\n    experiments_available = False\n\n# Create results table ONLY if ML Experiments is not available\n# If Experiments is available, we don't need this table\nif not experiments_available:\n    print(\"\\nüìã Creating HYPERPARAMETER_RESULTS table (Experiments not available)\")\n    session.sql(\n        f\"\"\"\n        CREATE TABLE IF NOT EXISTS {HYPERPARAMETER_RESULTS_TABLE} (\n            search_id VARCHAR,\n            group_name VARCHAR,\n            algorithm VARCHAR,\n            best_params VARIANT,\n            best_cv_rmse FLOAT,\n            best_cv_mae FLOAT,\n            val_rmse FLOAT,\n            val_mae FLOAT,\n            n_iter INTEGER,\n            sample_size INTEGER,\n            feature_version_id VARCHAR,\n            feature_snapshot_at TIMESTAMP_NTZ,\n            created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\n    \"\"\"\n    ).collect()\n    # Ensure new columns exist if table was created without them in the past (one ALTER per column)\n    for col_def in [\"feature_version_id VARCHAR\", \"feature_snapshot_at TIMESTAMP_NTZ\"]:\n        try:\n            session.sql(\n                f\"ALTER TABLE {HYPERPARAMETER_RESULTS_TABLE} ADD COLUMN IF NOT EXISTS {col_def}\"\n            ).collect()\n        except Exception:\n            pass\n    print(\"   ‚úÖ Table created/updated (will be used as primary storage)\")\nelse:\n    print(\"\\nüìã Skipping table creation (using ML Experiments as primary storage)\")\n\n",
      "id": "c4cd81a6-a7db-464f-85cb-44621912e3cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4b. Helper Functions: Target and Numeric Features\n",
      "id": "0979278a-526d-4230-a976-6f47ad5a9f38"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def _get_target_column(df):\n    \"\"\"Return the target column name in df (case-insensitive match).\"\"\"\n    for c in df.columns:\n        if str(c).upper() == TARGET_COLUMN:\n            return c\n    return TARGET_COLUMN.lower()\n\n\ndef _get_feature_cols_numeric(df, excluded_cols, target_col):\n    \"\"\"Numeric feature columns: excludes metadata/target (case-insensitive) and only int/float/bool (Snowflake ML).\"\"\"\n    # excluded_cols is already in UPPER CASE, target_col may vary\n    excluded_upper = {col.upper() if isinstance(col, str) else str(col).upper() for col in excluded_cols}\n    excluded_upper.add(str(target_col).upper())\n    return [\n        col\n        for col in df.columns\n        if str(col).upper() not in excluded_upper\n        and getattr(df[col].dtype, \"kind\", \"O\") in \"iufb\"\n    ]\n\n\n# Get feature columns from first group (all groups should have same features)\n# Use limit(1).to_pandas() only for schema inspection - minimal memory impact\nstats_ntile_col = next((c for c in train_df.columns if c.upper() == STATS_NTILE_GROUP_COL), STATS_NTILE_GROUP_COL)\nsample_group_df = train_df.filter(train_df[stats_ntile_col] == groups_list[0])\n# Only convert 1 row to pandas for schema inspection (minimal memory)\nsample_pandas = sample_group_df.limit(1).to_pandas()\ntarget_col_sample = _get_target_column(sample_pandas)\nfeature_cols = _get_feature_cols_numeric(sample_pandas, EXCLUDED_COLS, target_col_sample)\n\nprint(f\"\\nüìã Features ({len(feature_cols)}):\")\nfor col in sorted(feature_cols):\n    print(f\"   - {col}\")\n\n# Dictionary to store all results (filled in loop 5c)\nall_results = {}\n",
      "id": "a96dde21-7152-455a-9578-999cda17efb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4c. Training Function for the Tuner\n",
      "id": "50717546-4e6e-40b0-b811-9945f0852db2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def create_train_func_for_tuner(feature_cols, model_type, target_col):\n    \"\"\"\n    Create a training function for the Tuner (XGBRegressor or LGBMRegressor).\n    Called for each trial with different hyperparameters; model_type selects the regressor class.\n\n    Args:\n        feature_cols: List of feature column names\n        model_type: \"XGBRegressor\" or \"LGBMRegressor\"\n        target_col: Actual target column name in dataset (e.g. uni_box_week or UNI_BOX_WEEK)\n\n    Returns:\n        train_func: Function that can be used by Tuner\n    \"\"\"\n\n    def train_func():\n        from sklearn.metrics import mean_squared_error, mean_absolute_error\n        import numpy as np\n\n        tuner_context = get_tuner_context()\n        params = tuner_context.get_hyper_params()\n        dm = tuner_context.get_dataset_map()\n\n        train_pd = dm[\"train\"].to_pandas()\n        test_pd = dm[\"test\"].to_pandas()\n        # Snowflake ML uses fit(dataset) with a single DataFrame; input_cols and label_cols on the model\n        train_dataset = train_pd[feature_cols + [target_col]].fillna(0)\n        test_features = test_pd[feature_cols].fillna(0)\n        y_val = test_pd[target_col].fillna(0).values\n\n        model_params = params.copy()\n        model_params[\"random_state\"] = 42\n\n        if model_type == \"XGBRegressor\":\n            from snowflake.ml.modeling.xgboost import XGBRegressor\n\n            model_params[\"n_jobs\"] = -1\n            model_params[\"objective\"] = \"reg:squarederror\"\n            model_params[\"eval_metric\"] = \"rmse\"\n            model = XGBRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n        elif model_type == \"LGBMRegressor\":\n            from snowflake.ml.modeling.lightgbm import LGBMRegressor\n\n            model_params[\"n_jobs\"] = -1\n            model_params[\"verbosity\"] = -1\n            model = LGBMRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n        elif model_type == \"SGDRegressor\":\n            from snowflake.ml.modeling.linear_model import SGDRegressor\n\n            model_params.setdefault(\"penalty\", \"l2\")\n            model_params.setdefault(\"learning_rate\", \"invscaling\")\n            model = SGDRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n        else:\n            from snowflake.ml.modeling.xgboost import XGBRegressor\n\n            model_params[\"n_jobs\"] = -1\n            model_params[\"objective\"] = \"reg:squarederror\"\n            model_params[\"eval_metric\"] = \"rmse\"\n            model = XGBRegressor(\n                input_cols=feature_cols, label_cols=[target_col], **model_params\n            )\n\n        model.fit(train_dataset)\n        pred_result = model.predict(test_features)\n        pred_df = pred_result.to_pandas() if hasattr(pred_result, \"to_pandas\") else pred_result\n        out_col = model.get_output_cols()[0]\n        y_val_pred = np.asarray(pred_df[out_col])\n\n        # Regression metrics\n        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        val_mae = mean_absolute_error(y_val, y_val_pred)\n\n        # WAPE: sum(|y - y_hat|) / sum(|y|)\n        abs_errors = np.abs(y_val - y_val_pred)\n        denom_wape = np.sum(np.abs(y_val))\n        val_wape = float(abs_errors.sum() / denom_wape) if denom_wape > 0 else 0.0\n\n        # MAPE: mean(|y - y_hat| / |y|) * 100, ignoring targets 0\n        non_zero_mask = np.abs(y_val) > 1e-8\n        if non_zero_mask.any():\n            val_mape = float(\n                (np.abs(y_val[non_zero_mask] - y_val_pred[non_zero_mask]) / np.abs(y_val[non_zero_mask])).mean()\n                * 100.0\n            )\n        else:\n            val_mape = 0.0\n\n        tuner_context.report(\n            metrics={\n                \"rmse\": val_rmse,\n                \"mae\": val_mae,\n                \"wape\": val_wape,\n                \"mape\": val_mape,\n            },\n            model=model,\n        )\n\n    return train_func\n\n",
      "id": "43760422-b3b2-4e13-87af-4d7fdb1c26c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4d. Hyperparameter Search for One Group\n",
      "id": "bde8a653-aa07-44ef-962d-44bec579885a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def run_hyperparameter_search_for_one_group(group_name, group_snowpark_df):\n    \"\"\"\n    Run hyperparameter search for one group (HPO only, no MMT).\n    Called from a loop in the main script to avoid Ray serialization issues.\n    \n    OPTIMIZED: Works with Snowpark DataFrame to avoid OOM, only converts to pandas\n    when necessary for train_test_split and model training.\n\n    Args:\n        group_name: stats_ntile_group name (e.g. \"GROUP_01\").\n        group_snowpark_df: Snowpark DataFrame with data for this group only.\n\n    Returns:\n        HyperparameterResult (best params, metrics) or DummyResult (skipped/failed).\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    from snowflake.snowpark import functions as F\n    import numpy as np\n\n    print(f\"\\n{'='*80}\")\n    print(f\"üîç Hyperparameter Search for Group: {group_name}\")\n    print(f\"{'='*80}\")\n\n    class DummyResult:\n        def __init__(self):\n            self.group_name = group_name\n            self.skipped = True\n\n    # Count records using Snowpark (avoids loading to memory)\n    group_count = group_snowpark_df.count()\n    print(f\"üìä Group data: {group_count:,} records\")\n\n    if group_count < 50:\n        print(\n            f\"‚ö†Ô∏è  WARNING: Group has less than 50 records. Skipping hyperparameter search.\"\n        )\n        print(f\"   Will use default hyperparameters for this group.\")\n        return DummyResult()\n\n    # Sample data for this group using Snowpark (avoids loading full group to memory)\n    if SAMPLE_RATE_PER_GROUP < 1.0:\n        sampled_snowpark_df = group_snowpark_df.sample(frac=SAMPLE_RATE_PER_GROUP)\n        sampled_count = sampled_snowpark_df.count()\n        print(\n            f\"   Sampled: {sampled_count:,} records ({SAMPLE_RATE_PER_GROUP*100:.0f}% of {group_count:,} total)\"\n        )\n    else:\n        # Use full group for better hyperparameter search\n        sampled_snowpark_df = group_snowpark_df\n        sampled_count = group_count\n        print(f\"   Using full group: {sampled_count:,} records (100% of group)\")\n\n    # Get column names and target from Snowpark DataFrame (no pandas conversion yet)\n    # Get a small sample to identify column names and types\n    sample_row = sampled_snowpark_df.limit(1).to_pandas()\n    target_col = _get_target_column(sample_row)\n    feature_cols_list = _get_feature_cols_numeric(sample_row, EXCLUDED_COLS, target_col)\n\n    # Check if we have enough data for train/val split\n    if sampled_count < 20:\n        print(f\"‚ö†Ô∏è  WARNING: Not enough data for train/val split. Skipping.\")\n        return DummyResult()\n\n    # Prepare data in Snowpark: select features and target, fill nulls\n    # This keeps data in Snowflake until we need it for train_test_split\n    feature_cols_filled = [F.coalesce(F.col(c), F.lit(0)).alias(c) for c in feature_cols_list]\n    target_col_filled = F.coalesce(F.col(target_col), F.lit(0)).alias(target_col)\n    \n    prepared_snowpark_df = sampled_snowpark_df.select(\n        *feature_cols_filled,\n        target_col_filled\n    )\n\n    # NOW convert to pandas only for train_test_split (necessary for sklearn)\n    # This is the minimal conversion point - we've done all filtering/sampling in Snowpark\n    prepared_pandas_df = prepared_snowpark_df.to_pandas()\n    \n    X = prepared_pandas_df[feature_cols_list]\n    y = prepared_pandas_df[target_col]\n\n    print(f\"   Data shape: X={X.shape}, y={y.shape}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}], mean: {y.mean():.2f}\")\n\n    # Split into train and validation sets (requires pandas)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    print(f\"   Train: {X_train.shape[0]:,} samples, Val: {X_val.shape[0]:,} samples\")\n\n    # Prepare data for Tuner using DataConnector\n    # DataConnector.from_dataframe() expects Snowpark DataFrames, not Pandas\n    # Use explicit copy and reset index to avoid Pandas warnings\n    train_data = X_train.reset_index(drop=True).copy()\n    train_data[target_col] = np.asarray(y_train)\n\n    val_data = X_val.reset_index(drop=True).copy()\n    val_data[target_col] = np.asarray(y_val)\n\n    train_snowpark = session.create_dataframe(train_data)\n    val_snowpark = session.create_dataframe(val_data)\n\n    train_dc = DataConnector.from_dataframe(train_snowpark)\n    val_dc = DataConnector.from_dataframe(val_snowpark)\n\n    # dataset_map must include both \"train\" and \"test\" keys (following HPO documentation)\n    dataset_map = {\"train\": train_dc, \"test\": val_dc}\n\n    # Model for this group (Snowflake ML class name)\n    model_type = GROUP_MODEL.get(group_name, _DEFAULT_MODEL)\n    search_space = SEARCH_SPACES.get(model_type, SEARCH_SPACES[\"XGBRegressor\"])\n    print(f\"   Model: {model_type}\")\n\n    # Create training function for Tuner (model_type selects XGB/LGBM/SGD)\n    train_func = create_train_func_for_tuner(feature_cols_list, model_type, target_col)\n\n    tuner_config = TunerConfig(\n        metric=\"rmse\",\n        mode=\"min\",\n        search_alg=RandomSearch(),\n        num_trials=NUM_TRIALS,\n        max_concurrent_trials=MAX_CONCURRENT_TRIALS,\n    )\n\n    # Create and run Tuner\n    print(f\"   ‚è≥ Starting Random Search ({model_type}, {NUM_TRIALS} trials)...\")\n    start_time = time.time()\n\n    try:\n        tuner = Tuner(train_func, search_space, tuner_config)\n        results = tuner.run(dataset_map=dataset_map)\n\n        elapsed_time = time.time() - start_time\n\n        # Get best results (best_result is a single-row pd.DataFrame: config/* = hyperparams, other cols = metrics)\n        best_result = results.best_result\n        config_cols = [c for c in best_result.columns if str(c).startswith(\"config/\")]\n\n        def _to_native(v):\n            if hasattr(v, \"item\"):\n                return v.item()\n            return v\n\n        best_params = {\n            str(c).replace(\"config/\", \"\"): _to_native(best_result[c].iloc[0])\n            for c in config_cols\n        }\n        best_rmse = float(best_result[\"rmse\"].iloc[0]) if \"rmse\" in best_result.columns else None\n        best_mae = float(best_result[\"mae\"].iloc[0]) if \"mae\" in best_result.columns else None\n        if best_rmse is None:\n            raise ValueError(\"TunerResults best_result has no 'rmse' metric column\")\n        best_model = results.best_model\n\n        # Evaluate best model on validation set again for consistency\n        pred_result = best_model.predict(X_val)\n        pred_df = pred_result.to_pandas() if hasattr(pred_result, \"to_pandas\") else pred_result\n        out_col = best_model.get_output_cols()[0]\n        y_val_pred = np.asarray(pred_df[out_col])\n\n        # Validation metrics\n        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        val_mae = mean_absolute_error(y_val, y_val_pred)\n\n        abs_errors = np.abs(y_val - y_val_pred)\n        denom_wape = np.sum(np.abs(y_val))\n        val_wape = float(abs_errors.sum() / denom_wape) if denom_wape > 0 else 0.0\n\n        non_zero_mask = np.abs(y_val) > 1e-8\n        if non_zero_mask.any():\n            val_mape = float(\n                (np.abs(y_val[non_zero_mask] - y_val_pred[non_zero_mask]) / np.abs(y_val[non_zero_mask])).mean()\n                * 100.0\n            )\n        else:\n            val_mape = 0.0\n\n        print(f\"   ‚úÖ Completed in {elapsed_time:.1f}s\")\n        print(f\"      Best RMSE: {best_rmse:.4f}\")\n        print(\n            f\"      Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}, \"\n            f\"WAPE: {val_wape:.4f}, MAPE: {val_mape:.2f}%\"\n        )\n\n        # Store results in global dictionary (for summary later)\n        all_results[group_name] = {\n            \"best_params\": best_params,\n            \"best_cv_rmse\": best_rmse,\n            \"val_rmse\": val_rmse,\n            \"val_mae\": val_mae,\n            \"val_wape\": val_wape,\n            \"val_mape\": val_mape,\n            \"sample_size\": sampled_count,\n            \"algorithm\": model_type,\n        }\n\n        # Save to ML Experiments\n        search_id = f\"tune_{group_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        experiments_success = False\n\n        if experiments_available:\n            try:\n                # Create a run for this group's best hyperparameters\n                run_name = f\"best_{group_name}_{datetime.now().strftime('%H%M%S')}\"\n                with exp_tracking.start_run(run_name):\n                    # Log all hyperparameters\n                    exp_tracking.log_params(best_params)\n\n                    # Log metrics\n                    exp_tracking.log_metrics(\n                        {\n                            \"best_rmse\": float(best_rmse),\n                            \"val_rmse\": float(val_rmse),\n                            \"val_mae\": float(val_mae),\n                            \"val_wape\": float(val_wape),\n                            \"val_mape\": float(val_mape),\n                            \"sample_size\": int(sampled_count),\n                            \"num_trials\": int(NUM_TRIALS),\n                        }\n                    )\n\n                    # Log group identifier as a tag/parameter\n                    exp_tracking.log_param(\"group_name\", group_name)\n                    exp_tracking.log_param(\"search_id\", search_id)\n                    exp_tracking.log_param(\"algorithm\", model_type)\n                    if feature_version_id:\n                        exp_tracking.log_param(\"feature_version_id\", feature_version_id)\n                    if feature_snapshot_at is not None:\n                        exp_tracking.log_param(\n                            \"feature_snapshot_at\",\n                            str(feature_snapshot_at),\n                        )\n\n                print(f\"   ‚úÖ Results logged to ML Experiments (run: {run_name})\")\n                experiments_success = True\n            except Exception as e:\n                print(f\"   ‚ö†Ô∏è  Error logging to Experiments: {str(e)[:200]}\")\n                experiments_success = False\n\n        # Save to database ONLY if ML Experiments is not available or failed\n        # If Experiments works, we don't need the table\n        if not experiments_available or not experiments_success:\n            print(\n                f\"   üìã Saving to table (Experiments {'not available' if not experiments_available else 'failed'})\"\n            )\n\n            # Ensure table exists\n            session.sql(\n                f\"\"\"\n                CREATE TABLE IF NOT EXISTS {HYPERPARAMETER_RESULTS_TABLE} (\n                    search_id VARCHAR,\n                    group_name VARCHAR,\n                    algorithm VARCHAR,\n                    best_params VARIANT,\n                    best_cv_rmse FLOAT,\n                    best_cv_mae FLOAT,\n                    val_rmse FLOAT,\n                    val_mae FLOAT,\n                    n_iter INTEGER,\n                    sample_size INTEGER,\n                    feature_version_id VARCHAR,\n                    feature_snapshot_at TIMESTAMP_NTZ,\n                    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n                )\n            \"\"\"\n            ).collect()\n            # Ensure new columns exist if table already existed without them (one ALTER per column)\n            for col_def in [\"feature_version_id VARCHAR\", \"feature_snapshot_at TIMESTAMP_NTZ\"]:\n                try:\n                    session.sql(\n                        f\"ALTER TABLE {HYPERPARAMETER_RESULTS_TABLE} ADD COLUMN IF NOT EXISTS {col_def}\"\n                    ).collect()\n                except Exception:\n                    pass\n\n            best_params_json = json.dumps(\n                {\n                    k: float(v) if isinstance(v, (np.integer, np.floating)) else v\n                    for k, v in best_params.items()\n                }\n            )\n\n            best_mae_value = best_mae if best_mae is not None else None\n            best_mae_sql = (\n                f\"{best_mae_value:.6f}\" if best_mae_value is not None else \"NULL\"\n            )\n            best_params_escaped = best_params_json.replace(\"'\", \"''\")\n            search_id_escaped = search_id.replace(\"'\", \"''\")\n            group_name_escaped = group_name.replace(\"'\", \"''\")\n            algorithm_escaped = model_type.replace(\"'\", \"''\")\n            if feature_version_id:\n                feature_version_id_escaped = feature_version_id.replace(\"'\", \"''\")\n                feature_version_id_sql = f\"'{feature_version_id_escaped}'\"\n            else:\n                feature_version_id_sql = \"NULL\"\n            if feature_snapshot_at is not None:\n                feature_snapshot_at_sql = f\"TO_TIMESTAMP_NTZ('{str(feature_snapshot_at)}')\"\n            else:\n                feature_snapshot_at_sql = \"NULL\"\n\n            insert_sql = f\"\"\"\n                INSERT INTO {HYPERPARAMETER_RESULTS_TABLE}\n                (search_id, group_name, algorithm, best_params, best_cv_rmse, best_cv_mae, val_rmse, val_mae, n_iter, sample_size, feature_version_id, feature_snapshot_at)\n                VALUES (\n                    '{search_id_escaped}',\n                    '{group_name_escaped}',\n                    '{algorithm_escaped}',\n                    PARSE_JSON('{best_params_escaped}'),\n                    {best_rmse:.6f},\n                    {best_mae_sql},\n                    {val_rmse:.6f},\n                    {val_mae:.6f},\n                    {NUM_TRIALS},\n                    {sampled_count},\n                    {feature_version_id_sql},\n                    {feature_snapshot_at_sql}\n                )\n            \"\"\"\n            session.sql(insert_sql).collect()\n            print(f\"   ‚úÖ Results saved to table\")\n        else:\n            print(f\"   ‚úÖ Results stored in ML Experiments only (table not needed)\")\n\n        # Create result object to return\n        class HyperparameterResult:\n            def __init__(self):\n                self.group_name = group_name\n                self.best_params = best_params\n                self.best_rmse = best_rmse\n                self.val_rmse = val_rmse\n                self.val_mae = val_mae\n                self.skipped = False\n\n        print(f\"{'='*80}\\n\")\n        return HyperparameterResult()\n\n    except Exception as e:\n        print(f\"   ‚ùå Error during hyperparameter search: {str(e)[:200]}\")\n        import traceback\n\n        print(f\"   Traceback: {traceback.format_exc()[:300]}\")\n        print(f\"   Will use default hyperparameters for this group.\")\n        print(f\"{'='*80}\\n\")\n\n        # Return a dummy object indicating failure\n        return DummyResult()\n\n",
      "id": "13d53a16-ba9a-4cfb-84cc-9c7ccf7b9667",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5c. Execute hyperparameter search (loop per group)\n",
      "id": "77848a76-0511-4208-99a7-9701ff01ee82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# Run hyperparameter search per group (loop; no MMT to avoid Ray serialization)\n# Results are saved to BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS and/or ML Experiments.\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üöÄ HYPERPARAMETER SEARCH PER GROUP (sequential loop)\")\nprint(\"=\" * 80)\nprint(\"\\nRunning Random Search with Tuner for each group (no MMT).\\n\")\n\nstart_time = time.time()\ngroup_results = {}\n\nstats_ntile_col = next((c for c in train_df.columns if c.upper() == STATS_NTILE_GROUP_COL), STATS_NTILE_GROUP_COL)\nfor idx, group_name in enumerate(groups_list, 1):\n    print(f\"\\n[{idx}/{len(groups_list)}] Processing group: {group_name}\")\n    # Keep as Snowpark DataFrame - avoid to_pandas() here to prevent OOM\n    group_snowpark = train_df.filter(train_df[stats_ntile_col] == group_name)\n    # Pass Snowpark DataFrame directly - conversion to pandas happens inside function only when needed\n    result = run_hyperparameter_search_for_one_group(group_name, group_snowpark)\n    group_results[group_name] = result\n\nend_time = time.time()\nelapsed_minutes = (end_time - start_time) / 60\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE\")\nprint(\"=\" * 80)\nprint(f\"\\n‚è±Ô∏è  Total search time: {elapsed_minutes:.2f} minutes\")\n\n# Review results by group\nprint(\"\\nüìä Hyperparameter Search Results by Group:\\n\")\nsuccessful_searches = 0\nfor group_name in groups_list:\n    result = group_results.get(group_name)\n    if result is None:\n        print(f\"‚ö†Ô∏è  {group_name}: No result\")\n        continue\n    if getattr(result, \"skipped\", True):\n        print(f\"‚ö†Ô∏è  {group_name}: Skipped (insufficient data or error)\")\n    else:\n        print(f\"‚úÖ {group_name}:\")\n        print(f\"   Best RMSE: {result.best_rmse:.4f}\")\n        print(f\"   Val RMSE: {result.val_rmse:.4f}, Val MAE: {result.val_mae:.4f}\")\n        successful_searches += 1\n\nprint(\n    f\"\\n‚úÖ Completed hyperparameter search for {successful_searches}/{len(groups_list)} groups\"\n)\n",
      "id": "6ef714e2-6ce1-4f41-944b-f786500bf4c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Summary of All Results\n",
      "id": "a1a4776c-ac5c-4c8b-bf2e-c8e2f145ddab"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY OF ALL HYPERPARAMETER SEARCHES\")\nprint(\"=\" * 80)\n\n# Summary: Show results from Experiments or table\nif experiments_available:\n    print(\"\\nüìä Results Summary:\")\n    print(f\"   ‚úÖ All results stored in ML Experiments\")\n    print(f\"   ‚úÖ Experiment: {experiment_name}\")\n    print(f\"   ‚úÖ Groups processed: {len(all_results)}\")\n    print(f\"\\nüí° View results in Snowsight: AI & ML ‚Üí Experiments ‚Üí {experiment_name}\")\n\n    # Try to show summary from Experiments if possible\n    try:\n        # This is a conceptual query - actual API may vary\n        print(\"\\nüìä Sample results from Experiments:\")\n        for group_name, result in list(all_results.items())[:5]:\n            print(\n                f\"   {group_name}: RMSE={result['val_rmse']:.4f}, MAE={result['val_mae']:.4f}\"\n            )\n        if len(all_results) > 5:\n            print(f\"   ... and {len(all_results) - 5} more groups\")\n    except:\n        pass\nelse:\n    # Fallback to table summary if Experiments not available\n    print(\"\\nüìä Results from Table:\")\n    summary_df = session.sql(\n        f\"\"\"\n        SELECT \n            group_name,\n            best_cv_rmse,\n            val_rmse,\n            val_mae,\n            sample_size,\n            created_at\n        FROM {HYPERPARAMETER_RESULTS_TABLE}\n        WHERE created_at >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n        ORDER BY group_name\n    \"\"\"\n    )\n    summary_df.show()\n\n    # Overall statistics\n    overall_stats = session.sql(\n        f\"\"\"\n        SELECT \n            COUNT(*) AS TOTAL_SEARCHES,\n            AVG(best_cv_rmse) AS AVG_CV_RMSE,\n            AVG(val_rmse) AS AVG_VAL_RMSE,\n            MIN(val_rmse) AS MIN_VAL_RMSE,\n            MAX(val_rmse) AS MAX_VAL_RMSE\n        FROM {HYPERPARAMETER_RESULTS_TABLE}\n        WHERE created_at >= DATEADD(HOUR, -1, CURRENT_TIMESTAMP())\n    \"\"\"\n    )\n    print(\"\\nüìä Overall Statistics:\")\n    overall_stats.show()\n",
      "id": "9af3de78-6ae4-4b35-9ca3-8917b6ee8193",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Scale Cluster Down\n",
      "id": "9b4ca033-23b3-4316-87dd-1cba2b2f0d2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìâ SCALING CLUSTER DOWN\")\nprint(\"=\" * 80)\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n\n    print(\"‚è≥ Reduciendo cluster a 1 contenedor...\")\n    scale_cluster(\n        expected_cluster_size=1\n    )\n    print(\"‚úÖ Cluster reducido a 1 contenedor\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Error al reducir cluster: {str(e)[:200]}\")\n    print(\"   Cluster may still be scaling...\")\n",
      "id": "dbfa5940-007b-478f-99fd-368be844b055",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Summary\n",
      "id": "b90ddf89-60b6-4432-93b3-c16dbc7dc095"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úÖ Models: LGBMRegressor, XGBRegressor, SGDRegressor (per group)\")\nprint(\n    f\"   ‚úÖ Search method: Snowflake ML tune.search RandomSearch (per-group loop, no MMT)\"\n)\nprint(f\"   ‚úÖ Execution: Sequential loop over groups (avoids Ray serialization)\")\nprint(f\"   ‚úÖ Groups processed: {successful_searches}/{len(groups_list)}\")\nprint(f\"   ‚úÖ Trials per group: {NUM_TRIALS}\")\nprint(f\"   ‚úÖ Sample rate per group: {SAMPLE_RATE_PER_GROUP*100:.0f}%\")\nprint(f\"   ‚è±Ô∏è  Total time: {elapsed_minutes:.2f} minutes\")\n\n# Calculate statistics from all_results if available\nif all_results:\n    avg_val_rmse = np.mean([r[\"val_rmse\"] for r in all_results.values()])\n    min_val_rmse = min([r[\"val_rmse\"] for r in all_results.values()])\n    max_val_rmse = max([r[\"val_rmse\"] for r in all_results.values()])\n\n    print(f\"   ‚úÖ Average Validation RMSE: {avg_val_rmse:.4f}\")\n    print(f\"   ‚úÖ Best Group RMSE: {min_val_rmse:.4f}\")\n    print(f\"   ‚úÖ Worst Group RMSE: {max_val_rmse:.4f}\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review hyperparameter results by group\")\nprint(\"   2. Run 04_many_model_training.py to train 16 models (one per group)\")\nprint(\"   3. Each model will use its group-specific hyperparameters\")\n\nprint(\"\\n\" + \"=\" * 80)\n",
      "id": "c1bc99d9-efa9-4f64-bf2c-2f31001b56ab",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}