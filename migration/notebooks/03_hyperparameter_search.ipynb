{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Hyperparameter Search (XGBoost + Random Search)\n\n## Overview\nThis script performs hyperparameter optimization using Random Search for XGBoost regression.\n\n## What We'll Do:\n1. Load cleaned training data (sampled for efficiency)\n2. Prepare features and target\n3. Perform Random Search with cross-validation\n4. Save best hyperparameters\n",
      "id": "88eeaa4b-6b34-459c-aae2-f13287b04752"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import randint, uniform\nimport pickle\nfrom datetime import datetime\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE WAREHOUSE ARCA_DEMO_WH\").collect()\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "3009af91-ac81-468e-bbea-f090fb30b70d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Load and Sample Training Data\n",
      "id": "fbe29ce7-52d8-4157-a165-352ecd1f0d29"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä LOADING TRAINING DATA\")\nprint(\"=\"*80)\n\n# Load cleaned training data\ntrain_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n\ntotal_rows = train_df.count()\nprint(f\"\\n‚úÖ Training data loaded\")\nprint(f\"   Total rows: {total_rows:,}\")\n\n# Sample data for hyperparameter search (5-10% for efficiency)\nsample_rate = 0.05  # 5% sample\nprint(f\"\\nüìä Sampling {sample_rate*100:.0f}% of data for hyperparameter search...\")\n\nsampled_df = train_df.sample(fraction=sample_rate, seed=42)\nsampled_count = sampled_df.count()\nprint(f\"   Sampled rows: {sampled_count:,}\")\n\n# Convert to pandas for sklearn\nprint(\"\\n‚è≥ Converting to pandas (this may take a moment)...\")\ndf = sampled_df.to_pandas()\nprint(f\"‚úÖ Converted to pandas: {df.shape}\")\n",
      "id": "6898e111-ea59-4ba1-b4f2-127e4db664fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Prepare Features and Target\n",
      "id": "86590ece-0786-4e09-aa3f-417fc346bf4a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîß PREPARING FEATURES AND TARGET\")\nprint(\"=\"*80)\n\n# Define excluded columns\nexcluded_cols = [\n    'customer_id', 'brand_pres_ret', 'week', \n    'group', 'stats_group', 'percentile_group', 'stats_ntile_group'\n]\n\n# Get feature columns (all except excluded and target)\nfeature_cols = [col for col in df.columns \n                if col not in excluded_cols + ['uni_box_week']]\n\nprint(f\"\\nüìã Features ({len(feature_cols)}):\")\nfor col in sorted(feature_cols):\n    print(f\"   - {col}\")\n\n# Prepare X and y\nX = df[feature_cols].fillna(0)  # Fill NaN with 0\ny = df['uni_box_week'].fillna(0)\n\nprint(f\"\\n‚úÖ Features prepared:\")\nprint(f\"   X shape: {X.shape}\")\nprint(f\"   y shape: {y.shape}\")\nprint(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\nprint(f\"   Target mean: {y.mean():.2f}\")\n\n# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nüìä Train/Validation split:\")\nprint(f\"   Training: {X_train.shape[0]:,} samples\")\nprint(f\"   Validation: {X_val.shape[0]:,} samples\")\n",
      "id": "9a156309-b29e-41ec-993c-c56c3a9746e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Define Hyperparameter Search Space\n",
      "id": "406c5668-7f3f-44ba-8f16-6e429b49e2ec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üéØ DEFINING HYPERPARAMETER SEARCH SPACE\")\nprint(\"=\"*80)\n\n# Define parameter distributions for Random Search\nparam_distributions = {\n    'n_estimators': randint(50, 300),\n    'max_depth': randint(3, 10),\n    'learning_rate': uniform(0.01, 0.3),\n    'subsample': uniform(0.6, 1.0),\n    'colsample_bytree': uniform(0.6, 1.0),\n    'min_child_weight': randint(1, 7),\n    'gamma': uniform(0, 0.5),\n    'reg_alpha': uniform(0, 1),\n    'reg_lambda': uniform(0, 1)\n}\n\nprint(\"\\nüìã Hyperparameter Search Space:\")\nfor param, dist in param_distributions.items():\n    if hasattr(dist, 'a') and hasattr(dist, 'b'):\n        print(f\"   {param}: uniform({dist.a:.2f}, {dist.b:.2f})\")\n    elif hasattr(dist, 'low') and hasattr(dist, 'high'):\n        print(f\"   {param}: randint({dist.low}, {dist.high})\")\n\n# Number of iterations for Random Search\nn_iter = 50\nprint(f\"\\nüî¢ Random Search iterations: {n_iter}\")\n",
      "id": "8b7976a0-fc1f-4df8-b3db-77d07ff7dd57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Perform Random Search\n",
      "id": "d8ae043e-0748-4fb8-aba7-893fccc5ccc7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîç PERFORMING RANDOM SEARCH\")\nprint(\"=\"*80)\n\n# Create base XGBoost model\nbase_model = XGBRegressor(\n    random_state=42,\n    n_jobs=-1,\n    objective='reg:squarederror',\n    eval_metric='rmse'\n)\n\n# Create Random Search\nrandom_search = RandomizedSearchCV(\n    estimator=base_model,\n    param_distributions=param_distributions,\n    n_iter=n_iter,\n    cv=5,  # 5-fold cross-validation\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    random_state=42,\n    verbose=1\n)\n\nprint(\"\\n‚è≥ Starting Random Search (this may take several minutes)...\")\nprint(\"   Using 5-fold cross-validation\")\nprint(\"   This will test 50 different hyperparameter combinations\\n\")\n\n# Fit Random Search\nimport time\nstart_time = time.time()\n\nrandom_search.fit(X_train, y_train)\n\nelapsed_time = time.time() - start_time\nprint(f\"\\n‚úÖ Random Search completed in {elapsed_time/60:.2f} minutes\")\n",
      "id": "a5c1fbb7-856e-42c9-b754-8b8b4afce430",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Evaluate Best Model\n",
      "id": "e0202aa3-c417-4ba9-a1bd-93ff5185c6c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üèÜ BEST HYPERPARAMETERS\")\nprint(\"=\"*80)\n\nbest_params = random_search.best_params_\nbest_score = random_search.best_score_\nbest_model = random_search.best_estimator_\n\nprint(\"\\nüìä Best Hyperparameters:\")\nfor param, value in sorted(best_params.items()):\n    print(f\"   {param}: {value}\")\n\nprint(f\"\\nüìà Best CV Score (neg MSE): {best_score:.4f}\")\nprint(f\"   Best CV RMSE: {np.sqrt(-best_score):.4f}\")\n\n# Evaluate on validation set\ny_val_pred = best_model.predict(X_val)\nval_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\nval_mae = mean_absolute_error(y_val, y_val_pred)\n\nprint(f\"\\nüìä Validation Set Performance:\")\nprint(f\"   RMSE: {val_rmse:.4f}\")\nprint(f\"   MAE: {val_mae:.4f}\")\n",
      "id": "43175bdd-3c35-4de5-95b0-8372a0c61244",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Save Hyperparameters\n",
      "id": "e34321eb-56bd-4f12-a354-de5553748316"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üíæ SAVING HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create results table\nsession.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS (\n        search_id VARCHAR,\n        algorithm VARCHAR,\n        best_params VARIANT,\n        best_cv_rmse FLOAT,\n        best_cv_mae FLOAT,\n        val_rmse FLOAT,\n        val_mae FLOAT,\n        n_iter INTEGER,\n        sample_size INTEGER,\n        created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n    )\n\"\"\").collect()\n\n# Save results\nsearch_id = f\"xgb_random_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n# Convert best_params to JSON string for VARIANT type\nimport json\nbest_params_json = json.dumps({k: float(v) if isinstance(v, (np.integer, np.floating)) else v \n                               for k, v in best_params.items()})\n\ninsert_sql = f\"\"\"\n    INSERT INTO BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n    (search_id, algorithm, best_params, best_cv_rmse, best_cv_mae, val_rmse, val_mae, n_iter, sample_size)\n    VALUES (\n        '{search_id}',\n        'XGBoost',\n        PARSE_JSON('{best_params_json}'),\n        {np.sqrt(-best_score):.6f},\n        NULL,\n        {val_rmse:.6f},\n        {val_mae:.6f},\n        {n_iter},\n        {sampled_count}\n    )\n\"\"\"\n\nsession.sql(insert_sql).collect()\n\nprint(f\"‚úÖ Hyperparameters saved to HYPERPARAMETER_RESULTS\")\nprint(f\"   Search ID: {search_id}\")\n\n# Verify save\nsaved_results = session.sql(f\"\"\"\n    SELECT \n        search_id,\n        algorithm,\n        best_cv_rmse,\n        val_rmse,\n        val_mae,\n        created_at\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n    WHERE search_id = '{search_id}'\n\"\"\")\n\nprint(\"\\nüìä Saved Results:\")\nsaved_results.show()\n",
      "id": "76c16f79-7e00-4167-a876-517c2473a8a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Summary\n",
      "id": "b968c541-682d-49a8-8b71-2c6fec893fe5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE!\")\nprint(\"=\"*80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úÖ Algorithm: XGBoost\")\nprint(f\"   ‚úÖ Search method: Random Search\")\nprint(f\"   ‚úÖ Iterations: {n_iter}\")\nprint(f\"   ‚úÖ Sample size: {sampled_count:,} ({sample_rate*100:.0f}% of total)\")\nprint(f\"   ‚úÖ Best CV RMSE: {np.sqrt(-best_score):.4f}\")\nprint(f\"   ‚úÖ Validation RMSE: {val_rmse:.4f}\")\nprint(f\"   ‚úÖ Search ID: {search_id}\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review best hyperparameters\")\nprint(\"   2. Run 04_many_model_training.py to train model with best hyperparameters\")\nprint(\"   3. Use full dataset for final training\")\n\nprint(\"\\n\" + \"=\"*80)\n",
      "id": "0d976ec1-6b7b-439c-b6d8-1b7dad6e935b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}