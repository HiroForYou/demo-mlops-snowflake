{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Hyperparameter Search (XGBoost + Random Search)\n\n## Overview\nThis script performs hyperparameter optimization using Random Search for XGBoost regression.\n\n## What We'll Do:\n1. Load cleaned training data (sampled for efficiency)\n2. Prepare features and target\n3. Perform Random Search with cross-validation\n4. Save best hyperparameters\n",
      "id": "26aa066d-1c6e-44f5-b45f-ee00622061a7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import randint, uniform\nimport pickle\nfrom datetime import datetime\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE WAREHOUSE ARCA_DEMO_WH\").collect()\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n",
      "id": "d26e057b-00d7-41cf-be4e-146321e661ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Load and Sample Training Data\n",
      "id": "b1589269-dcb7-4bf0-9b02-2ed02ec27c52"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä LOADING TRAINING DATA\")\nprint(\"=\"*80)\n\n# Load cleaned training data\ntrain_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n\ntotal_rows = train_df.count()\nprint(f\"\\n‚úÖ Training data loaded\")\nprint(f\"   Total rows: {total_rows:,}\")\n\n# Sample data for hyperparameter search (5-10% for efficiency)\nsample_rate = 0.05  # 5% sample\nprint(f\"\\nüìä Sampling {sample_rate*100:.0f}% of data for hyperparameter search...\")\n\nsampled_df = train_df.sample(fraction=sample_rate, seed=42)\nsampled_count = sampled_df.count()\nprint(f\"   Sampled rows: {sampled_count:,}\")\n\n# Convert to pandas for sklearn\nprint(\"\\n‚è≥ Converting to pandas (this may take a moment)...\")\ndf = sampled_df.to_pandas()\nprint(f\"‚úÖ Converted to pandas: {df.shape}\")\n",
      "id": "937f6bbd-83ae-4fa6-865b-e747d3dd7ae1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Prepare Features and Target\n",
      "id": "2dfce702-b59d-46ab-ab5b-3b5253cc2ce6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîß PREPARING FEATURES AND TARGET\")\nprint(\"=\"*80)\n\n# Define excluded columns\nexcluded_cols = [\n    'customer_id', 'brand_pres_ret', 'week', \n    'group', 'stats_group', 'percentile_group', 'stats_ntile_group'\n]\n\n# Get feature columns (all except excluded and target)\nfeature_cols = [col for col in df.columns \n                if col not in excluded_cols + ['uni_box_week']]\n\nprint(f\"\\nüìã Features ({len(feature_cols)}):\")\nfor col in sorted(feature_cols):\n    print(f\"   - {col}\")\n\n# Prepare X and y\nX = df[feature_cols].fillna(0)  # Fill NaN with 0\ny = df['uni_box_week'].fillna(0)\n\nprint(f\"\\n‚úÖ Features prepared:\")\nprint(f\"   X shape: {X.shape}\")\nprint(f\"   y shape: {y.shape}\")\nprint(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\nprint(f\"   Target mean: {y.mean():.2f}\")\n\n# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nüìä Train/Validation split:\")\nprint(f\"   Training: {X_train.shape[0]:,} samples\")\nprint(f\"   Validation: {X_val.shape[0]:,} samples\")\n",
      "id": "4b975e18-7716-468e-b7ad-8d0ba961c23a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Define Hyperparameter Search Space\n",
      "id": "b5bdf0a2-4880-49c0-af93-16baf2007e5a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üéØ DEFINING HYPERPARAMETER SEARCH SPACE\")\nprint(\"=\"*80)\n\n# Define parameter distributions for Random Search\nparam_distributions = {\n    'n_estimators': randint(50, 300),\n    'max_depth': randint(3, 10),\n    'learning_rate': uniform(0.01, 0.3),\n    'subsample': uniform(0.6, 1.0),\n    'colsample_bytree': uniform(0.6, 1.0),\n    'min_child_weight': randint(1, 7),\n    'gamma': uniform(0, 0.5),\n    'reg_alpha': uniform(0, 1),\n    'reg_lambda': uniform(0, 1)\n}\n\nprint(\"\\nüìã Hyperparameter Search Space:\")\nfor param, dist in param_distributions.items():\n    if hasattr(dist, 'a') and hasattr(dist, 'b'):\n        print(f\"   {param}: uniform({dist.a:.2f}, {dist.b:.2f})\")\n    elif hasattr(dist, 'low') and hasattr(dist, 'high'):\n        print(f\"   {param}: randint({dist.low}, {dist.high})\")\n\n# Number of iterations for Random Search\nn_iter = 50\nprint(f\"\\nüî¢ Random Search iterations: {n_iter}\")\n",
      "id": "e70efa8f-576b-4f2e-9d05-de29da1f4594",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Perform Random Search\n",
      "id": "17c8faa8-b796-4714-9f8e-c20570e318a9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üîç PERFORMING RANDOM SEARCH\")\nprint(\"=\"*80)\n\n# Create base XGBoost model\nbase_model = XGBRegressor(\n    random_state=42,\n    n_jobs=-1,\n    objective='reg:squarederror',\n    eval_metric='rmse'\n)\n\n# Create Random Search\nrandom_search = RandomizedSearchCV(\n    estimator=base_model,\n    param_distributions=param_distributions,\n    n_iter=n_iter,\n    cv=5,  # 5-fold cross-validation\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    random_state=42,\n    verbose=1\n)\n\nprint(\"\\n‚è≥ Starting Random Search (this may take several minutes)...\")\nprint(\"   Using 5-fold cross-validation\")\nprint(\"   This will test 50 different hyperparameter combinations\\n\")\n\n# Fit Random Search\nimport time\nstart_time = time.time()\n\nrandom_search.fit(X_train, y_train)\n\nelapsed_time = time.time() - start_time\nprint(f\"\\n‚úÖ Random Search completed in {elapsed_time/60:.2f} minutes\")\n",
      "id": "32619a6c-275a-45b7-b90f-5ad2166bbe68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Evaluate Best Model\n",
      "id": "81a24d81-7bde-43cf-b731-6ee33a7af50e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üèÜ BEST HYPERPARAMETERS\")\nprint(\"=\"*80)\n\nbest_params = random_search.best_params_\nbest_score = random_search.best_score_\nbest_model = random_search.best_estimator_\n\nprint(\"\\nüìä Best Hyperparameters:\")\nfor param, value in sorted(best_params.items()):\n    print(f\"   {param}: {value}\")\n\nprint(f\"\\nüìà Best CV Score (neg MSE): {best_score:.4f}\")\nprint(f\"   Best CV RMSE: {np.sqrt(-best_score):.4f}\")\n\n# Evaluate on validation set\ny_val_pred = best_model.predict(X_val)\nval_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\nval_mae = mean_absolute_error(y_val, y_val_pred)\n\nprint(f\"\\nüìä Validation Set Performance:\")\nprint(f\"   RMSE: {val_rmse:.4f}\")\nprint(f\"   MAE: {val_mae:.4f}\")\n",
      "id": "7b68bc3f-7bd1-4be8-9f18-2e739c218598",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Save Hyperparameters\n",
      "id": "1283fc59-36d2-4bad-8460-89486a6183c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üíæ SAVING HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Create results table\nsession.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS (\n        search_id VARCHAR,\n        algorithm VARCHAR,\n        best_params VARIANT,\n        best_cv_rmse FLOAT,\n        best_cv_mae FLOAT,\n        val_rmse FLOAT,\n        val_mae FLOAT,\n        n_iter INTEGER,\n        sample_size INTEGER,\n        created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n    )\n\"\"\").collect()\n\n# Save results\nsearch_id = f\"xgb_random_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n# Convert best_params to JSON string for VARIANT type\nimport json\nbest_params_json = json.dumps({k: float(v) if isinstance(v, (np.integer, np.floating)) else v \n                               for k, v in best_params.items()})\n\ninsert_sql = f\"\"\"\n    INSERT INTO BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n    (search_id, algorithm, best_params, best_cv_rmse, best_cv_mae, val_rmse, val_mae, n_iter, sample_size)\n    VALUES (\n        '{search_id}',\n        'XGBoost',\n        PARSE_JSON('{best_params_json}'),\n        {np.sqrt(-best_score):.6f},\n        NULL,\n        {val_rmse:.6f},\n        {val_mae:.6f},\n        {n_iter},\n        {sampled_count}\n    )\n\"\"\"\n\nsession.sql(insert_sql).collect()\n\nprint(f\"‚úÖ Hyperparameters saved to HYPERPARAMETER_RESULTS\")\nprint(f\"   Search ID: {search_id}\")\n\n# Verify save\nsaved_results = session.sql(f\"\"\"\n    SELECT \n        search_id,\n        algorithm,\n        best_cv_rmse,\n        val_rmse,\n        val_mae,\n        created_at\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n    WHERE search_id = '{search_id}'\n\"\"\")\n\nprint(\"\\nüìä Saved Results:\")\nsaved_results.show()\n",
      "id": "57dd6b2f-9739-4d37-ad49-a84bb4b7d33e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Summary\n",
      "id": "73774d4f-bbc9-4630-82b2-c09b34b5ee12"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ HYPERPARAMETER SEARCH COMPLETE!\")\nprint(\"=\"*80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úÖ Algorithm: XGBoost\")\nprint(f\"   ‚úÖ Search method: Random Search\")\nprint(f\"   ‚úÖ Iterations: {n_iter}\")\nprint(f\"   ‚úÖ Sample size: {sampled_count:,} ({sample_rate*100:.0f}% of total)\")\nprint(f\"   ‚úÖ Best CV RMSE: {np.sqrt(-best_score):.4f}\")\nprint(f\"   ‚úÖ Validation RMSE: {val_rmse:.4f}\")\nprint(f\"   ‚úÖ Search ID: {search_id}\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review best hyperparameters\")\nprint(\"   2. Run 04_many_model_training.py to train model with best hyperparameters\")\nprint(\"   3. Use full dataset for final training\")\n\nprint(\"\\n\" + \"=\"*80)\n",
      "id": "b4ed871f-9733-4415-8098-5fc80c992029",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}