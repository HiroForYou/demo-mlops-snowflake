{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Many Model Training (MMT) - XGBoost (16 Models)\n#\n## Overview\nThis script trains **16 XGBoost models** (one per stats_ntile_group) using Many Model Training (MMT) framework.\nEach model is trained with group-specific hyperparameters and data.\n#\n## What We'll Do:\n1. Load best hyperparameters per group from hyperparameter search\n2. Define training function for MMT (with group-specific hyperparameters)\n3. Execute MMT training with partition_by=\"stats_ntile_group\"\n4. Register 16 models in Model Registry (one per group)\n5. Create group-to-model mapping\n",
      "id": "1253809d-1a98-47c2-96cb-28d19d558723"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.modeling.distributors.many_model import ManyModelTraining\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.model import task\nimport time\nfrom datetime import datetime\nimport json\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n\n# Configuraci√≥n:\n# - Si no tienes permisos para FeatureView/Dynamic Tables, usa tablas limpias o tabla de features materializada.\nUSE_CLEANED_TABLES = False  # True = TRAIN_DATASET_CLEANED, False = intentar tabla de features materializada\nFEATURES_TABLE = \"BD_AA_DEV.SC_FEATURES_BMX.UNI_BOX_FEATURES\"  # creada por 02_feature_store_setup.py\n",
      "id": "83b95e9f-eaa7-4af9-83e4-199d4cfd3d8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Setup Model Registry & Staging\n",
      "id": "9d071d7f-d81b-429c-a064-0c4fa0e54e1d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# CREATE SCHEMA comentado (puede requerir permisos)\n# session.sql(\"CREATE SCHEMA IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX\").collect()\nsession.sql(\"CREATE STAGE IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\").collect()\n\nregistry = Registry(\n    session=session, database_name=\"BD_AA_DEV\", schema_name=\"SC_MODELS_BMX\"\n)\n\nprint(\"‚úÖ Model Registry initialized\")\nprint(\"‚úÖ Stage for MMT models created\")\n",
      "id": "03154b13-91c5-4a4d-8ccd-ade1cb9f6cd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Load Best Hyperparameters Per Group\n",
      "id": "f3453dff-7f25-4f5f-82ca-bf58716ee480"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä LOADING BEST HYPERPARAMETERS PER GROUP\")\nprint(\"=\" * 80)\n\n# Try to load from ML Experiments first, fallback to table\nhyperparams_by_group = {}\nexperiments_loaded = False\n\n# Get all groups that need hyperparameters\nall_groups_from_data = session.sql(\n    \"\"\"\n    SELECT DISTINCT stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE stats_ntile_group IS NOT NULL\n    ORDER BY stats_ntile_group\n\"\"\"\n).collect()\n\nexpected_groups = [row[\"STATS_NTILE_GROUP\"] for row in all_groups_from_data]\n\n# Try loading from ML Experiments\nprint(\"\\nüî¨ Attempting to load from ML Experiments...\")\ntry:\n    exp_tracking = ExperimentTracking(session)\n    \n    # Try to find the most recent experiment\n    # Note: This is a simplified approach - in production you might want to specify experiment name\n    from datetime import datetime, timedelta\n    today = datetime.now().strftime('%Y%m%d')\n    experiment_name = f\"hyperparameter_search_xgboost_{today}\"\n    \n    try:\n        exp_tracking.set_experiment(experiment_name)\n        print(f\"‚úÖ Found experiment: {experiment_name}\")\n        \n        # Get all runs from this experiment\n        # Note: The exact API may vary - this is a conceptual approach\n        # You may need to query the experiments table directly\n        experiments_loaded = True\n        print(\"   ‚úÖ ML Experiments available - loading from experiments\")\n    except:\n        # Try yesterday's experiment as fallback\n        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')\n        experiment_name = f\"hyperparameter_search_xgboost_{yesterday}\"\n        try:\n            exp_tracking.set_experiment(experiment_name)\n            print(f\"‚úÖ Found experiment: {experiment_name}\")\n            experiments_loaded = True\n        except:\n            print(\"   ‚ö†Ô∏è  No recent experiment found, will use table fallback\")\n            experiments_loaded = False\n    \n    if experiments_loaded:\n        # Use ExperimentTracking API methods via SQL SHOW commands\n        try:\n            print(f\"   üìã Listing runs from experiment: {experiment_name}\")\n            \n            # Step 1: List all runs in the experiment using SHOW RUNS\n            runs_query = f\"SHOW RUNS IN EXPERIMENT {experiment_name}\"\n            runs_df = session.sql(runs_query)\n            runs_list = runs_df.collect()\n            \n            if len(runs_list) == 0:\n                print(\"   ‚ö†Ô∏è  No runs found in experiment, using table fallback\")\n                experiments_loaded = False\n            else:\n                print(f\"   ‚úÖ Found {len(runs_list)} runs in experiment\")\n                \n                # Step 2: For each run, get parameters and metrics\n                runs_by_group = {}  # group_name -> best run info\n                \n                for run in runs_list:\n                    run_name = run[\"name\"]\n                    \n                    try:\n                        # Get parameters for this run\n                        params_query = f\"SHOW RUN PARAMETERS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        params_df = session.sql(params_query)\n                        params_list = params_df.collect()\n                        \n                        # Get metrics for this run\n                        metrics_query = f\"SHOW RUN METRICS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        metrics_df = session.sql(metrics_query)\n                        metrics_list = metrics_df.collect()\n                        \n                        # Extract group_name from parameters\n                        group_name = None\n                        search_id = None\n                        best_params = {}\n                        \n                        for param in params_list:\n                            param_name = param[\"name\"]\n                            param_value = param[\"value\"]\n                            \n                            if param_name == \"group_name\":\n                                group_name = param_value\n                            elif param_name == \"search_id\":\n                                search_id = param_value\n                            elif param_name not in [\"algorithm\"]:\n                                # This is a hyperparameter\n                                best_params[param_name] = param_value\n                        \n                        # Extract metrics\n                        val_rmse = None\n                        val_mae = None\n                        \n                        for metric in metrics_list:\n                            metric_name = metric[\"name\"]\n                            metric_value = metric[\"value\"]\n                            \n                            if metric_name == \"val_rmse\":\n                                val_rmse = float(metric_value)\n                            elif metric_name == \"val_mae\":\n                                val_mae = float(metric_value)\n                        \n                        # Only process runs that have a group_name\n                        if group_name and val_rmse is not None:\n                            # Keep the best run per group (lowest val_rmse)\n                            if group_name not in runs_by_group:\n                                runs_by_group[group_name] = {\n                                    \"run_name\": run_name,\n                                    \"params\": best_params,\n                                    \"val_rmse\": val_rmse,\n                                    \"val_mae\": val_mae,\n                                    \"search_id\": search_id,\n                                }\n                            else:\n                                # Replace if this run has better (lower) RMSE\n                                if val_rmse < runs_by_group[group_name][\"val_rmse\"]:\n                                    runs_by_group[group_name] = {\n                                        \"run_name\": run_name,\n                                        \"params\": best_params,\n                                        \"val_rmse\": val_rmse,\n                                        \"val_mae\": val_mae,\n                                        \"search_id\": search_id,\n                                    }\n                    \n                    except Exception as run_error:\n                        print(f\"   ‚ö†Ô∏è  Error processing run {run_name}: {str(run_error)[:100]}\")\n                        continue\n                \n                # Step 3: Store results in hyperparams_by_group\n                if len(runs_by_group) > 0:\n                    print(f\"   ‚úÖ Loaded {len(runs_by_group)} groups from Experiments\")\n                    \n                    for group_name, run_info in runs_by_group.items():\n                        hyperparams_by_group[group_name] = {\n                            \"params\": run_info[\"params\"],\n                            \"val_rmse\": run_info[\"val_rmse\"],\n                            \"search_id\": run_info[\"search_id\"] or f\"exp_{group_name}\",\n                        }\n                        \n                        print(f\"\\n   {group_name}:\")\n                        print(f\"      Val RMSE: {run_info['val_rmse']:.4f}\")\n                        if run_info[\"val_mae\"]:\n                            print(f\"      Val MAE: {run_info['val_mae']:.4f}\")\n                        print(f\"      Search ID: {run_info['search_id'] or 'N/A'}\")\n                        print(f\"      Source: ML Experiments (run: {run_info['run_name']})\")\n                    \n                    experiments_loaded = True\n                else:\n                    print(\"   ‚ö†Ô∏è  No valid runs with group_name found, using table fallback\")\n                    experiments_loaded = False\n                    \n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Error using ExperimentTracking API: {str(e)[:200]}\")\n            print(\"   Will use table fallback\")\n            experiments_loaded = False\n\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will use table fallback\")\n    experiments_loaded = False\n\n# Fallback to table ONLY if Experiments didn't work or didn't have all groups\n# Table is now a fallback mechanism, not the primary storage\nif not experiments_loaded or len(hyperparams_by_group) < len(expected_groups):\n    print(\"\\nüìã Loading from table (HYPERPARAMETER_RESULTS) - Fallback mode...\")\n    print(\"   Note: Table is only used when ML Experiments is not available\")\n    \n    # Check if table exists\n    table_exists = False\n    try:\n        check_table = session.sql(\n            \"\"\"\n            SELECT COUNT(*) as CNT \n            FROM INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = 'SC_STORAGE_BMX_PS' \n            AND TABLE_NAME = 'HYPERPARAMETER_RESULTS'\n            AND TABLE_CATALOG = 'BD_AA_DEV'\n            \"\"\"\n        ).collect()\n        table_exists = check_table[0][\"CNT\"] > 0\n    except:\n        table_exists = False\n    \n    if table_exists:\n        # Get most recent hyperparameter search results per group from table\n        hyperparams_df = session.sql(\n            \"\"\"\n            WITH latest_searches AS (\n                SELECT \n                    group_name,\n                    search_id,\n                    algorithm,\n                    best_params,\n                    best_cv_rmse,\n                    val_rmse,\n                    val_mae,\n                    created_at,\n                    ROW_NUMBER() OVER (PARTITION BY group_name ORDER BY created_at DESC) AS rn\n                FROM BD_AA_DEV.SC_STORAGE_BMX_PS.HYPERPARAMETER_RESULTS\n                WHERE algorithm = 'XGBoost'\n                    AND group_name IS NOT NULL\n            )\n            SELECT \n                group_name,\n                search_id,\n                best_params,\n                best_cv_rmse,\n                val_rmse,\n                val_mae\n            FROM latest_searches\n            WHERE rn = 1\n            ORDER BY group_name\n        \"\"\"\n        )\n        \n        hyperparams_results = hyperparams_df.collect()\n        \n        if len(hyperparams_results) > 0:\n            print(f\"   ‚úÖ Loaded {len(hyperparams_results)} groups from table\")\n            \n            # Update or add to hyperparams_by_group\n            for result in hyperparams_results:\n                group_name = result[\"GROUP_NAME\"]\n                best_params_json = result[\"BEST_PARAMS\"]\n                \n                # Parse hyperparameters\n                if isinstance(best_params_json, str):\n                    best_params = json.loads(best_params_json)\n                else:\n                    best_params = best_params_json\n                \n                # Only add if not already loaded from Experiments\n                if group_name not in hyperparams_by_group:\n                    hyperparams_by_group[group_name] = {\n                        \"params\": best_params,\n                        \"val_rmse\": result[\"VAL_RMSE\"],\n                        \"search_id\": result[\"SEARCH_ID\"],\n                    }\n                    \n                    print(f\"\\n   {group_name}:\")\n                    print(f\"      Val RMSE: {result['VAL_RMSE']:.4f}\")\n                    print(f\"      Search ID: {result['SEARCH_ID']}\")\n                    print(f\"      Source: Table (fallback)\")\n        else:\n            print(\"   ‚ö†Ô∏è  Table exists but has no results\")\n    else:\n        print(\"   ‚ö†Ô∏è  Table does not exist (this is OK if using ML Experiments)\")\n\nif len(hyperparams_by_group) == 0:\n    raise ValueError(\n        \"No hyperparameter results found in Experiments or table! Please run 03_hyperparameter_search.py first\"\n    )\n\nprint(f\"\\n‚úÖ Total loaded hyperparameters: {len(hyperparams_by_group)}/{len(expected_groups)} groups\")\n\n# Get default hyperparameters (for groups without search results)\ndefault_params = {\n    \"n_estimators\": 100,\n    \"max_depth\": 6,\n    \"learning_rate\": 0.1,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"min_child_weight\": 1,\n    \"gamma\": 0,\n    \"reg_alpha\": 0,\n    \"reg_lambda\": 1,\n}\n\nprint(f\"\\nüìã Default hyperparameters (for groups without search results):\")\nfor param, value in sorted(default_params.items()):\n    print(f\"   {param}: {value}\")\n\n# Validate that we have hyperparameters for all expected groups\nprint(f\"\\nüîç Validating hyperparameter coverage...\")\ngroups_with_hyperparams = set(hyperparams_by_group.keys())\ngroups_without_hyperparams = set(expected_groups) - groups_with_hyperparams\n\nif groups_without_hyperparams:\n    print(f\"‚ö†Ô∏è  WARNING: {len(groups_without_hyperparams)} groups will use default hyperparameters:\")\n    for group in sorted(groups_without_hyperparams):\n        print(f\"      - {group}\")\nelse:\n    print(f\"‚úÖ All {len(expected_groups)} groups have optimized hyperparameters!\")\n",
      "id": "cc6f79fc-9f2e-44c2-a087-d7859714f9d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Preparar datos de entrenamiento (sin FeatureView)\n",
      "id": "a9a9b223-21d9-46ac-937d-b9578c07093b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üè™ LOADING TRAINING DATA (SIN FEATURE VIEW)\")\nprint(\"=\" * 80)\n\nif USE_CLEANED_TABLES:\n    print(\"üìä Loading from cleaned table: TRAIN_DATASET_CLEANED\")\n    training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n    print(f\"\\n‚úÖ Training data loaded from cleaned table\")\n    print(f\"   Total records: {training_df.count():,}\")\n    print(f\"   Columns: {len(training_df.columns)}\")\nelse:\n    # Preferimos la tabla materializada de features (sin Dynamic Tables).\n    # Si falla por permisos/no existencia, hacemos fallback a la tabla limpia.\n    try:\n        # Mantener inicializaci√≥n del Feature Store (aunque no usemos FeatureView)\n        _fs = FeatureStore(session=session, database=\"BD_AA_DEV\", name=\"SC_FEATURES_BMX\")\n        print(\"‚úÖ Feature Store inicializado (sin FeatureView)\")\n\n        print(f\"üìä Loading features from table: {FEATURES_TABLE}\")\n        features_df = session.table(FEATURES_TABLE)\n\n        print(\"‚è≥ Loading target variable and stats_ntile_group from training table...\")\n        target_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\").select(\n            \"customer_id\", \"brand_pres_ret\", \"week\", \"uni_box_week\", \"stats_ntile_group\"\n        )\n\n        print(\"‚è≥ Joining features with target...\")\n        training_df = features_df.join(\n            target_df, on=[\"customer_id\", \"brand_pres_ret\", \"week\"], how=\"inner\"\n        )\n\n        print(f\"\\n‚úÖ Training data loaded from features table + target\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Could not load/join features table ({FEATURES_TABLE}): {str(e)[:200]}\")\n        print(\"   Falling back to TRAIN_DATASET_CLEANED\")\n        training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n        print(f\"\\n‚úÖ Training data loaded from cleaned table (fallback)\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n\n# Show distribution by group\nprint(\"\\nüìä Records per Group:\")\ngroup_counts = training_df.group_by(\"stats_ntile_group\").count().sort(\"stats_ntile_group\")\ngroup_counts.show()\n",
      "id": "0e082a3b-6d3a-4cce-81a1-9424646cc572",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Define Training Function for MMT\n",
      "id": "70210b6e-5384-410d-bfe5-f3e5b7948ea3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîß DEFINING TRAINING FUNCTION\")\nprint(\"=\" * 80)\n\n\ndef train_segment_model(data_connector, context):\n    \"\"\"\n    Train XGBoost model for uni_box_week regression for a specific group.\n\n    This function:\n    1. Receives data for ONE group (via MMT partitioning)\n    2. Loads group-specific hyperparameters\n    3. Trains XGBoost model\n    4. Evaluates on test set\n    5. Returns trained model\n\n    Args:\n        data_connector: Snowflake data connector (provided by MMT)\n        context: Contains partition_id (stats_ntile_group name)\n\n    Returns:\n        Trained XGBoost model object\n    \"\"\"\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    import numpy as np\n\n    segment_name = context.partition_id\n    print(f\"\\n{'='*80}\")\n    print(f\"üöÄ Training model for {segment_name}\")\n    print(f\"{'='*80}\")\n\n    # Load data\n    df = data_connector.to_pandas()\n    print(f\"üìä Data shape: {df.shape}\")\n\n    # Define excluded columns (metadata / non-features)\n    excluded_cols = [\n        \"customer_id\",\n        \"brand_pres_ret\",\n        \"week\",\n        \"FEATURE_TIMESTAMP\",  # puede existir si usas tabla de features\n        \"stats_ntile_group\",  # Group column - not a feature\n    ]\n\n    # Get feature columns\n    feature_cols = [\n        col for col in df.columns if col not in excluded_cols + [\"uni_box_week\"]\n    ]\n\n    target_col = \"uni_box_week\"\n\n    X = df[feature_cols].fillna(0)\n    y = df[target_col].fillna(0)\n\n    print(f\"   Features: {len(feature_cols)}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\n    print(f\"   Target mean: {y.mean():.2f}\")\n\n    # Split train/test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    print(f\"   Training set: {X_train.shape[0]:,} samples\")\n    print(f\"   Test set: {X_test.shape[0]:,} samples\")\n\n    # Get group-specific hyperparameters\n    # IMPORTANT: This uses hyperparams_by_group loaded from script 03\n    if segment_name in hyperparams_by_group:\n        group_params = hyperparams_by_group[segment_name][\"params\"]\n        search_id = hyperparams_by_group[segment_name][\"search_id\"]\n        val_rmse = hyperparams_by_group[segment_name][\"val_rmse\"]\n        print(f\"\\n   ‚úÖ Using OPTIMIZED hyperparameters from script 03\")\n        print(f\"      Search ID: {search_id}\")\n        print(f\"      Validation RMSE (from search): {val_rmse:.4f}\")\n        print(f\"      Hyperparameters: {', '.join([f'{k}={v:.3f}' if isinstance(v, float) else f'{k}={v}' for k, v in sorted(group_params.items())[:5]])}...\")\n    else:\n        group_params = default_params\n        print(f\"\\n   ‚ö†Ô∏è  Using DEFAULT hyperparameters (no search results found for {segment_name})\")\n        print(f\"      This group was not processed in script 03 or had insufficient data\")\n\n    # Convert hyperparameters to proper types\n    xgb_params = {}\n    for k, v in group_params.items():\n        if isinstance(v, (int, float)):\n            xgb_params[k] = v\n        elif isinstance(v, (np.integer, np.floating)):\n            xgb_params[k] = float(v) if isinstance(v, np.floating) else int(v)\n        else:\n            xgb_params[k] = v\n\n    # Ensure required parameters\n    xgb_params[\"random_state\"] = 42\n    xgb_params[\"n_jobs\"] = -1\n    xgb_params[\"objective\"] = \"reg:squarederror\"\n    xgb_params[\"eval_metric\"] = \"rmse\"\n\n    print(f\"\\n   Training XGBoost with {len(xgb_params)} hyperparameters...\")\n\n    # Train model\n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test, y_pred)\n\n    print(f\"\\n   ‚úÖ Model trained\")\n    print(f\"      RMSE: {rmse:.2f}\")\n    print(f\"      MAE: {mae:.2f}\")\n    print(f\"{'='*80}\\n\")\n\n    # Attach metadata to model\n    model.rmse = rmse\n    model.mae = mae\n    model.training_samples = X_train.shape[0]\n    model.test_samples = X_test.shape[0]\n    model.feature_cols = feature_cols\n    model.hyperparameters = xgb_params\n    model.segment = segment_name\n    model.group_name = segment_name\n\n    return model\n\n\nprint(\"‚úÖ Training function defined\")\n",
      "id": "ffdb5f6c-75d0-4e44-8d4c-14f5f6e3f591",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Execute Many Model Training (MMT) - 16 Models\n",
      "id": "54ce5eef-25f5-433e-a626-5aafc811550c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üöÄ STARTING MANY MODEL TRAINING (MMT) - 16 MODELS\")\nprint(\"=\" * 80)\nprint(\"\\nTraining 16 XGBoost models in PARALLEL (one per stats_ntile_group)\")\nprint(\"Each model uses group-specific hyperparameters\\n\")\n\nstart_time = time.time()\n\n# Create MMT trainer\ntrainer = ManyModelTraining(\n    train_segment_model, \"BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\"\n)\n\n# Execute training with partition_by stats_ntile_group\ntraining_run = trainer.run(\n    partition_by=\"stats_ntile_group\",  # ‚Üê KEY: Partition by group\n    snowpark_dataframe=training_df,\n    run_id=f\"uni_box_regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n)\n\nprint(\"\\n‚è≥ Training in progress... Monitoring completion...\\n\")\n\n# Monitor with timeout\nimport time as time_module\n\nmax_wait = 1800  # 30 minutes max\ncheck_interval = 10  # Check every 10 seconds\nelapsed = 0\ncompleted = False\n\nwhile elapsed < max_wait:\n    time_module.sleep(check_interval)\n    elapsed += check_interval\n\n    try:\n        done_count = 0\n        total_count = 0\n        for partition_id in training_run.partition_details:\n            total_count += 1\n            status = training_run.partition_details[partition_id].status\n            if status.name == \"DONE\" or status.name == \"FAILED\":\n                done_count += 1\n\n        print(\n            f\"‚è±Ô∏è  {elapsed}s elapsed - Progress: {done_count}/{total_count} models completed\",\n            end=\"\\r\",\n        )\n\n        if done_count == total_count:\n            print(\"\\n‚úÖ All models completed!\" + \" \" * 50)\n            completed = True\n            break\n    except:\n        print(f\"‚è±Ô∏è  {elapsed}s elapsed - Waiting for status update...\", end=\"\\r\")\n\nif not completed:\n    print(\"\\n‚è±Ô∏è  Timeout reached - Verifying completion via stage...\" + \" \" * 30)\n    stage_files = session.sql(\n        f\"LIST @BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS PATTERN='.*{training_run.run_id}.*'\"\n    ).collect()\n    if len(stage_files) > 0:\n        print(\n            f\"‚úÖ Found {len(stage_files)} model files in stage - Training completed successfully!\"\n        )\n        completed = True\n\nend_time = time.time()\nelapsed_minutes = (end_time - start_time) / 60\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"‚úÖ TRAINING COMPLETE! Status: {'COMPLETED' if completed else 'UNKNOWN'}\")\nprint(\"=\" * 80)\nprint(f\"\\n‚è±Ô∏è  Total training time: {elapsed_minutes:.2f} minutes\")\n",
      "id": "b22acd06-bc62-47f4-b247-b90f68657b39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Review Training Results\n",
      "id": "cdd027ac-6750-4ca3-a2e1-60f627db835e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüìä Training Results:\\n\")\n\nfor partition_id in training_run.partition_details:\n    details = training_run.partition_details[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            print(f\"\\n‚úÖ {partition_id if partition_id else 'DEFAULT'}:\")\n            print(f\"   RMSE: {model.rmse:.2f}\")\n            print(f\"   MAE: {model.mae:.2f}\")\n            print(f\"   Training samples: {model.training_samples:,}\")\n            print(f\"   Test samples: {model.test_samples:,}\")\n        except Exception as e:\n            print(f\"\\n‚ö†Ô∏è  {partition_id}: Could not load model - {str(e)[:100]}\")\n    else:\n        print(f\"\\n‚ùå {partition_id}: Training failed\")\n        print(f\"   Status: {details.status}\")\n",
      "id": "b3298114-ebe5-4ffc-8dba-957e2ef5544c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Register Model in Model Registry\n",
      "id": "f9f05887-ba98-4687-bacd-9198dd5389a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìù REGISTERING MODEL IN MODEL REGISTRY\")\nprint(\"=\" * 80)\n\nversion_date = datetime.now().strftime(\"%Y%m%d_%H%M\")\nregistered_models = {}\n\nfor partition_id in training_run.partition_details:\n    details = training_run.partition_details[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            # Model name includes group identifier\n            model_name = f\"uni_box_regression_{partition_id.lower()}\"\n\n            # Get group-specific search ID and hyperparameters if available\n            group_search_id = None\n            group_hyperparams = None\n            if partition_id in hyperparams_by_group:\n                group_search_id = hyperparams_by_group[partition_id][\"search_id\"]\n                group_hyperparams = hyperparams_by_group[partition_id][\"params\"]\n\n            # Prepare sample input from this group\n            sample_input = (\n                training_df.filter(training_df[\"stats_ntile_group\"] == partition_id)\n                .select(model.feature_cols)\n                .limit(5)\n            )\n\n            print(f\"\\nRegistering {partition_id}...\")\n\n            # Prepare metrics including hyperparameters\n            model_metrics = {\n                \"rmse\": float(model.rmse),\n                \"mae\": float(model.mae),\n                \"training_samples\": int(model.training_samples),\n                \"test_samples\": int(model.test_samples),\n                \"algorithm\": \"XGBoost\",\n                \"group\": partition_id,\n                \"hyperparameter_search_id\": group_search_id or \"default\",\n            }\n            \n            # Add hyperparameters to metrics (as nested dict)\n            if group_hyperparams:\n                # Convert hyperparameters to a format suitable for metrics\n                for key, value in group_hyperparams.items():\n                    if isinstance(value, (int, float)):\n                        model_metrics[f\"hyperparameter_{key}\"] = float(value) if isinstance(value, float) else int(value)\n                model_metrics[\"hyperparameters\"] = json.dumps({\n                    k: float(v) if isinstance(v, (int, float)) else v\n                    for k, v in group_hyperparams.items()\n                })\n\n            mv = registry.log_model(\n                model,\n                model_name=model_name,\n                version_name=f\"v_{version_date}\",\n                comment=f\"XGBoost regression model for uni_box_week - Group: {partition_id}\",\n                metrics=model_metrics,\n                sample_input_data=sample_input,\n                task=task.Task.TABULAR_REGRESSION,\n            )\n\n            registered_models[partition_id] = {\n                \"model_name\": model_name,\n                \"version\": f\"v_{version_date}\",\n                \"model_version\": mv,\n            }\n\n            print(f\"‚úÖ {partition_id}: {model_name} v_{version_date}\")\n            print(f\"   RMSE: {model.rmse:.2f}, MAE: {model.mae:.2f}\")\n\n        except Exception as e:\n            print(f\"‚ùå Error registering model: {str(e)[:200]}\")\n\nprint(f\"\\n‚úÖ {len(registered_models)} model(s) registered successfully!\")\n",
      "id": "c73ece53-1947-47fc-8e75-6a00f2a08563",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Set Production Alias\n",
      "id": "9c8ebd2d-0abe-4cf7-aa73-a3b04bef6c21"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüè∑Ô∏è  Setting PRODUCTION aliases...\\n\")\n\nfor partition_id, model_info in registered_models.items():\n    model_name = model_info[\"model_name\"]\n    version = model_info[\"version\"]\n    model_version = model_info[\"model_version\"]\n\n    try:\n        # Remove existing PRODUCTION alias\n        try:\n            model_ref = registry.get_model(model_name)\n            model_ref.default.unset_alias(\"PRODUCTION\")\n        except:\n            pass\n\n        # Set alias on new version\n        model_version.set_alias(\"PRODUCTION\")\n        print(f\"‚úÖ {model_name}: PRODUCTION ‚Üí {version}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  {model_name}: Error setting alias - {str(e)[:100]}\")\n\nprint(\"\\n‚úÖ All production aliases configured!\")\n",
      "id": "47ec4225-7c3f-49db-b921-53842ebc835e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Summary\n",
      "id": "0b003ec9-4d91-4f2d-a402-fa0e28dd7d98"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üéâ MANY MODEL TRAINING (MMT) COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìä Summary:\")\nprint(f\"   ‚úÖ Models trained: {len(registered_models)}/16\")\nprint(f\"   ‚è±Ô∏è  Training time: {elapsed_minutes:.2f} minutes\")\nprint(f\"   üîß Algorithm: XGBoost\")\nprint(f\"   üìà Hyperparameters: Group-specific (from hyperparameter search)\")\n\nif registered_models:\n    print(f\"\\nüèÜ Model Performance by Group:\")\n    for partition_id in sorted(registered_models.keys()):\n        model = training_run.get_model(partition_id)\n        print(f\"   {partition_id}: RMSE={model.rmse:.2f}, MAE={model.mae:.2f}, Samples={model.training_samples:,}\")\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review model performance by group\")\nprint(\"   2. Run 05_create_partitioned_model.py to create partitioned model (combines all 16)\")\nprint(\"   3. Run 06_partitioned_inference_batch.py for batch inference with automatic routing\")\n\nprint(\"\\n\" + \"=\" * 80)\n",
      "id": "5e7f6450-abb6-4306-8631-aeda7de4b6ae",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}