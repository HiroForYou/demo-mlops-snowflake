{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Many Model Training (MMT) - 16 Models (LGBMRegressor, XGBRegressor, SGDRegressor)\n#\n## Overview\nThis script trains **16 regression models** (one per stats_ntile_group) using Many Model Training (MMT).\nModel type per group: LGBMRegressor, XGBRegressor, or SGDRegressor (see GROUP_MODEL).\nEach model is trained with group-specific hyperparameters from script 03.\n#\n## What We'll Do:\n1. Load best hyperparameters per group from hyperparameter search\n2. Define training function for MMT (with group-specific hyperparameters)\n3. Execute MMT training with partition_by=\"stats_ntile_group\"\n4. Register 16 models in Model Registry (one per group)\n5. Create group-to-model mapping\n",
      "id": "f65e1b16-6da7-48fb-8e50-e462668ed05f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.modeling.distributors.many_model import ManyModelTraining\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.model import task\nimport time\nfrom datetime import datetime\nimport json\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n\n# Configuraci√≥n:\n# - Si no tienes permisos para FeatureView/Dynamic Tables, usa tablas limpias o tabla de features materializada.\nUSE_CLEANED_TABLES = False  # True = TRAIN_DATASET_CLEANED, False = intentar tabla de features materializada\nFEATURES_TABLE = (\n    \"BD_AA_DEV.SC_FEATURES_BMX.UNI_BOX_FEATURES\"  # creada por 02_feature_store_setup.py\n)\n\n# Un solo objeto: grupo -> nombre de clase Snowflake ML (snowflake.ml.modeling.*)\nGROUP_MODEL = {\n    \"group_stat_0_1\": \"LGBMRegressor\",\n    \"group_stat_0_2\": \"LGBMRegressor\",\n    \"group_stat_0_3\": \"LGBMRegressor\",\n    \"group_stat_0_4\": \"LGBMRegressor\",\n    \"group_stat_1_1\": \"LGBMRegressor\",\n    \"group_stat_1_2\": \"LGBMRegressor\",\n    \"group_stat_1_3\": \"XGBRegressor\",\n    \"group_stat_1_4\": \"SGDRegressor\",\n    \"group_stat_2_1\": \"LGBMRegressor\",\n    \"group_stat_2_2\": \"LGBMRegressor\",\n    \"group_stat_2_3\": \"XGBRegressor\",\n    \"group_stat_2_4\": \"XGBRegressor\",\n    \"group_stat_3_1\": \"LGBMRegressor\",\n    \"group_stat_3_2\": \"LGBMRegressor\",\n    \"group_stat_3_3\": \"LGBMRegressor\",\n    \"group_stat_3_4\": \"SGDRegressor\",\n}\n_DEFAULT_MODEL = \"XGBRegressor\"\n",
      "id": "b165b7f1-da4d-44ea-bb70-07d3c89d2b83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Setup Model Registry & Staging\n",
      "id": "dcceb020-428a-4c2f-a5c5-bc207e763336"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# CREATE SCHEMA comentado (puede requerir permisos)\n# session.sql(\"CREATE SCHEMA IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX\").collect()\nsession.sql(\"CREATE STAGE IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\").collect()\n\nregistry = Registry(\n    session=session, database_name=\"BD_AA_DEV\", schema_name=\"SC_MODELS_BMX\"\n)\n\nprint(\"‚úÖ Model Registry initialized\")\nprint(\"‚úÖ Stage for MMT models created\")\n",
      "id": "c6014a6b-f573-438c-86bf-5007325e2a3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Load Best Hyperparameters Per Group\n",
      "id": "fd12f7df-ef4d-43c0-8fa1-f6751ac0ae53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä LOADING BEST HYPERPARAMETERS PER GROUP\")\nprint(\"=\" * 80)\n\n# Try to load from ML Experiments first, fallback to table\nhyperparams_by_group = {}\nexperiments_loaded = False\n\n# Get all groups that need hyperparameters\nall_groups_from_data = session.sql(\n    \"\"\"\n    SELECT DISTINCT stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE stats_ntile_group IS NOT NULL\n    ORDER BY stats_ntile_group\n\"\"\"\n).collect()\n\nexpected_groups = [row[\"STATS_NTILE_GROUP\"] for row in all_groups_from_data]\n\n# Try loading from ML Experiments\nprint(\"\\nüî¨ Attempting to load from ML Experiments...\")\ntry:\n    exp_tracking = ExperimentTracking(session)\n\n    # Try to find the most recent experiment\n    # Note: This is a simplified approach - in production you might want to specify experiment name\n    from datetime import datetime, timedelta\n\n    today = datetime.now().strftime(\"%Y%m%d\")\n    experiment_name = f\"hyperparameter_search_regression_{today}\"\n\n    try:\n        exp_tracking.set_experiment(experiment_name)\n        print(f\"‚úÖ Found experiment: {experiment_name}\")\n\n        # Get all runs from this experiment\n        # Note: The exact API may vary - this is a conceptual approach\n        # You may need to query the experiments table directly\n        experiments_loaded = True\n        print(\"   ‚úÖ ML Experiments available - loading from experiments\")\n    except:\n        # Try yesterday's experiment as fallback\n        yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\n        experiment_name = f\"hyperparameter_search_regression_{yesterday}\"\n        try:\n            exp_tracking.set_experiment(experiment_name)\n            print(f\"‚úÖ Found experiment: {experiment_name}\")\n            experiments_loaded = True\n        except:\n            print(\"   ‚ö†Ô∏è  No recent experiment found, will use table fallback\")\n            experiments_loaded = False\n\n    if experiments_loaded:\n        # Use ExperimentTracking API methods via SQL SHOW commands\n        try:\n            print(f\"   üìã Listing runs from experiment: {experiment_name}\")\n\n            # Step 1: List all runs in the experiment using SHOW RUNS\n            runs_query = f\"SHOW RUNS IN EXPERIMENT {experiment_name}\"\n            runs_df = session.sql(runs_query)\n            runs_list = runs_df.collect()\n\n            if len(runs_list) == 0:\n                print(\"   ‚ö†Ô∏è  No runs found in experiment, using table fallback\")\n                experiments_loaded = False\n            else:\n                print(f\"   ‚úÖ Found {len(runs_list)} runs in experiment\")\n\n                # Step 2: For each run, get parameters and metrics\n                runs_by_group = {}  # group_name -> best run info\n\n                for run in runs_list:\n                    run_name = run[\"name\"]\n\n                    try:\n                        # Get parameters for this run\n                        params_query = f\"SHOW RUN PARAMETERS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        params_df = session.sql(params_query)\n                        params_list = params_df.collect()\n\n                        # Get metrics for this run\n                        metrics_query = f\"SHOW RUN METRICS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        metrics_df = session.sql(metrics_query)\n                        metrics_list = metrics_df.collect()\n\n                        # Extract group_name and algorithm from parameters\n                        group_name = None\n                        search_id = None\n                        algorithm = None\n                        best_params = {}\n\n                        for param in params_list:\n                            param_name = param[\"name\"]\n                            param_value = param[\"value\"]\n\n                            if param_name == \"group_name\":\n                                group_name = param_value\n                            elif param_name == \"search_id\":\n                                search_id = param_value\n                            elif param_name == \"algorithm\":\n                                algorithm = param_value\n                            else:\n                                best_params[param_name] = param_value\n\n                        # Extract metrics\n                        val_rmse = None\n                        val_mae = None\n\n                        for metric in metrics_list:\n                            metric_name = metric[\"name\"]\n                            metric_value = metric[\"value\"]\n\n                            if metric_name == \"val_rmse\":\n                                val_rmse = float(metric_value)\n                            elif metric_name == \"val_mae\":\n                                val_mae = float(metric_value)\n\n                        # Only process runs that have a group_name\n                        if group_name and val_rmse is not None:\n                            alg = algorithm or GROUP_MODEL.get(\n                                group_name, _DEFAULT_MODEL\n                            )\n                            if group_name not in runs_by_group:\n                                runs_by_group[group_name] = {\n                                    \"run_name\": run_name,\n                                    \"params\": best_params,\n                                    \"val_rmse\": val_rmse,\n                                    \"val_mae\": val_mae,\n                                    \"search_id\": search_id,\n                                    \"algorithm\": alg,\n                                }\n                            else:\n                                if val_rmse < runs_by_group[group_name][\"val_rmse\"]:\n                                    runs_by_group[group_name] = {\n                                        \"run_name\": run_name,\n                                        \"params\": best_params,\n                                        \"val_rmse\": val_rmse,\n                                        \"val_mae\": val_mae,\n                                        \"search_id\": search_id,\n                                        \"algorithm\": alg,\n                                    }\n\n                    except Exception as run_error:\n                        print(\n                            f\"   ‚ö†Ô∏è  Error processing run {run_name}: {str(run_error)[:100]}\"\n                        )\n                        continue\n\n                # Step 3: Store results in hyperparams_by_group\n                if len(runs_by_group) > 0:\n                    print(f\"   ‚úÖ Loaded {len(runs_by_group)} groups from Experiments\")\n\n                    for group_name, run_info in runs_by_group.items():\n                        hyperparams_by_group[group_name] = {\n                            \"params\": run_info[\"params\"],\n                            \"val_rmse\": run_info[\"val_rmse\"],\n                            \"search_id\": run_info[\"search_id\"] or f\"exp_{group_name}\",\n                            \"algorithm\": run_info.get(\"algorithm\", _DEFAULT_MODEL),\n                        }\n\n                        print(f\"\\n   {group_name}:\")\n                        print(\n                            f\"      Algorithm: {run_info.get('algorithm', _DEFAULT_MODEL)}\"\n                        )\n                        print(f\"      Val RMSE: {run_info['val_rmse']:.4f}\")\n                        if run_info[\"val_mae\"]:\n                            print(f\"      Val MAE: {run_info['val_mae']:.4f}\")\n                        print(f\"      Search ID: {run_info['search_id'] or 'N/A'}\")\n                        print(\n                            f\"      Source: ML Experiments (run: {run_info['run_name']})\"\n                        )\n\n                    experiments_loaded = True\n                else:\n                    print(\n                        \"   ‚ö†Ô∏è  No valid runs with group_name found, using table fallback\"\n                    )\n                    experiments_loaded = False\n\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Error using ExperimentTracking API: {str(e)[:200]}\")\n            print(\"   Will use table fallback\")\n            experiments_loaded = False\n\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will use table fallback\")\n    experiments_loaded = False\n\n# Fallback to table ONLY if Experiments didn't work or didn't have all groups\n# Table is now a fallback mechanism, not the primary storage\nif not experiments_loaded or len(hyperparams_by_group) < len(expected_groups):\n    print(\"\\nüìã Loading from table (HYPERPARAMETER_RESULTS) - Fallback mode...\")\n    print(\"   Note: Table is only used when ML Experiments is not available\")\n\n    # Check if table exists\n    table_exists = False\n    try:\n        check_table = session.sql(\n            \"\"\"\n            SELECT COUNT(*) as CNT \n            FROM INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = 'SC_MODELS_BMX' \n            AND TABLE_NAME = 'HYPERPARAMETER_RESULTS'\n            AND TABLE_CATALOG = 'BD_AA_DEV'\n            \"\"\"\n        ).collect()\n        table_exists = check_table[0][\"CNT\"] > 0\n    except:\n        table_exists = False\n\n    if table_exists:\n        # Get most recent hyperparameter search results per group from table\n        hyperparams_df = session.sql(\n            \"\"\"\n            WITH latest_searches AS (\n                SELECT \n                    group_name,\n                    search_id,\n                    algorithm,\n                    best_params,\n                    best_cv_rmse,\n                    val_rmse,\n                    val_mae,\n                    created_at,\n                    ROW_NUMBER() OVER (PARTITION BY group_name ORDER BY created_at DESC) AS rn\n                FROM BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n                WHERE group_name IS NOT NULL\n            )\n            SELECT \n                group_name,\n                search_id,\n                best_params,\n                best_cv_rmse,\n                val_rmse,\n                val_mae\n            FROM latest_searches\n            WHERE rn = 1\n            ORDER BY group_name\n        \"\"\"\n        )\n\n        hyperparams_results = hyperparams_df.collect()\n\n        if len(hyperparams_results) > 0:\n            print(f\"   ‚úÖ Loaded {len(hyperparams_results)} groups from table\")\n\n            # Update or add to hyperparams_by_group\n            for result in hyperparams_results:\n                group_name = result[\"GROUP_NAME\"]\n                best_params_json = result[\"BEST_PARAMS\"]\n\n                # Parse hyperparameters\n                if isinstance(best_params_json, str):\n                    best_params = json.loads(best_params_json)\n                else:\n                    best_params = best_params_json\n\n                # Only add if not already loaded from Experiments\n                if group_name not in hyperparams_by_group:\n                    alg = result.get(\"ALGORITHM\") or GROUP_MODEL.get(\n                        group_name, _DEFAULT_MODEL\n                    )\n                    hyperparams_by_group[group_name] = {\n                        \"params\": best_params,\n                        \"val_rmse\": result[\"VAL_RMSE\"],\n                        \"search_id\": result[\"SEARCH_ID\"],\n                        \"algorithm\": alg,\n                    }\n\n                    print(f\"\\n   {group_name}:\")\n                    print(f\"      Algorithm: {alg}\")\n                    print(f\"      Val RMSE: {result['VAL_RMSE']:.4f}\")\n                    print(f\"      Search ID: {result['SEARCH_ID']}\")\n                    print(f\"      Source: Table (fallback)\")\n        else:\n            print(\"   ‚ö†Ô∏è  Table exists but has no results\")\n    else:\n        print(\"   ‚ö†Ô∏è  Table does not exist (this is OK if using ML Experiments)\")\n\nif len(hyperparams_by_group) == 0:\n    raise ValueError(\n        \"No hyperparameter results found in Experiments or table! Please run 03_hyperparameter_search.py first\"\n    )\n\nprint(\n    f\"\\n‚úÖ Total loaded hyperparameters: {len(hyperparams_by_group)}/{len(expected_groups)} groups\"\n)\n\n# Default hyperparameters por clase Snowflake ML (grupos sin resultados de b√∫squeda)\nDEFAULT_PARAMS_BY_MODEL = {\n    \"XGBRegressor\": {\n        \"n_estimators\": 100,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"min_child_weight\": 1,\n        \"gamma\": 0,\n        \"reg_alpha\": 0,\n        \"reg_lambda\": 1,\n    },\n    \"LGBMRegressor\": {\n        \"n_estimators\": 100,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"reg_alpha\": 0,\n        \"reg_lambda\": 1,\n        \"min_child_samples\": 20,\n    },\n    \"SGDRegressor\": {\n        \"alpha\": 0.0001,\n        \"max_iter\": 2000,\n        \"tol\": 1e-3,\n        \"eta0\": 0.01,\n    },\n}\n\nprint(\n    f\"\\nüìã Default hyperparameters (per Snowflake ML model, for groups without search results):\"\n)\nfor model_name, params in DEFAULT_PARAMS_BY_MODEL.items():\n    print(f\"   {model_name}: {list(params.keys())}\")\n\n# Validate that we have hyperparameters for all expected groups\nprint(f\"\\nüîç Validating hyperparameter coverage...\")\ngroups_with_hyperparams = set(hyperparams_by_group.keys())\ngroups_without_hyperparams = set(expected_groups) - groups_with_hyperparams\n\nif groups_without_hyperparams:\n    print(\n        f\"‚ö†Ô∏è  WARNING: {len(groups_without_hyperparams)} groups will use default hyperparameters:\"\n    )\n    for group in sorted(groups_without_hyperparams):\n        print(f\"      - {group}\")\nelse:\n    print(f\"‚úÖ All {len(expected_groups)} groups have optimized hyperparameters!\")\n",
      "id": "d4d7b01a-ca3c-449b-a3e6-58e4b397571a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Preparar datos de entrenamiento (sin FeatureView)\n",
      "id": "c073ed64-85d5-45d0-870a-9c11cb378a52"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üè™ LOADING TRAINING DATA (SIN FEATURE VIEW)\")\nprint(\"=\" * 80)\n\nif USE_CLEANED_TABLES:\n    print(\"üìä Loading from cleaned table: TRAIN_DATASET_CLEANED\")\n    training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n    print(f\"\\n‚úÖ Training data loaded from cleaned table\")\n    print(f\"   Total records: {training_df.count():,}\")\n    print(f\"   Columns: {len(training_df.columns)}\")\nelse:\n    # Preferimos la tabla materializada de features (sin Dynamic Tables).\n    # Si falla por permisos/no existencia, hacemos fallback a la tabla limpia.\n    try:\n        # Mantener inicializaci√≥n del Feature Store (aunque no usemos FeatureView)\n        _fs = FeatureStore(\n            session=session,\n            database=\"BD_AA_DEV\",\n            name=\"SC_FEATURES_BMX\",\n            default_warehouse=\"WH_AA_DEV_DS_SQL\",\n        )\n        print(\"‚úÖ Feature Store inicializado (sin FeatureView)\")\n\n        print(f\"üìä Loading features from table: {FEATURES_TABLE}\")\n        features_df = session.table(FEATURES_TABLE)\n\n        print(\"‚è≥ Loading target variable and stats_ntile_group from training table...\")\n        target_df = session.table(\n            \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n        ).select(\n            \"customer_id\", \"brand_pres_ret\", \"week\", \"uni_box_week\", \"stats_ntile_group\"\n        )\n\n        print(\"‚è≥ Joining features with target...\")\n        training_df = features_df.join(\n            target_df, on=[\"customer_id\", \"brand_pres_ret\", \"week\"], how=\"inner\"\n        )\n\n        print(f\"\\n‚úÖ Training data loaded from features table + target\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n    except Exception as e:\n        print(\n            f\"‚ö†Ô∏è  Could not load/join features table ({FEATURES_TABLE}): {str(e)[:200]}\"\n        )\n        print(\"   Falling back to TRAIN_DATASET_CLEANED\")\n        training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n        print(f\"\\n‚úÖ Training data loaded from cleaned table (fallback)\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n\n# Show distribution by group\nprint(\"\\nüìä Records per Group:\")\ngroup_counts = (\n    training_df.group_by(\"stats_ntile_group\").count().sort(\"stats_ntile_group\")\n)\ngroup_counts.show()\n",
      "id": "66398601-7001-41ae-8fde-30ff3f34d31a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Define Training Function for MMT\n",
      "id": "31aa2818-605d-4eee-8403-a032bb266e98"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîß DEFINING TRAINING FUNCTION\")\nprint(\"=\" * 80)\n\n\ndef train_segment_model(data_connector, context):\n    \"\"\"\n    Train regression model for uni_box_week for a specific group.\n    Model type per group: LGBMRegressor, XGBRegressor, or SGDRegressor (from script 03 / GROUP_MODEL).\n\n    Args:\n        data_connector: Snowflake data connector (provided by MMT)\n        context: Contains partition_id (stats_ntile_group name)\n\n    Returns:\n        Trained model (XGBRegressor, LGBMRegressor, or SGDRegressor)\n    \"\"\"\n    from snowflake.ml.modeling.xgboost import XGBRegressor\n    from snowflake.ml.modeling.lightgbm import LGBMRegressor\n    from snowflake.ml.modeling.linear_model import SGDRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    import numpy as np\n\n    segment_name = context.partition_id\n    print(f\"\\n{'='*80}\")\n    print(f\"üöÄ Training model for {segment_name}\")\n    print(f\"{'='*80}\")\n\n    df = data_connector.to_pandas()\n    print(f\"üìä Data shape: {df.shape}\")\n\n    excluded_cols = [\n        \"customer_id\",\n        \"brand_pres_ret\",\n        \"week\",\n        \"FEATURE_TIMESTAMP\",\n        \"stats_ntile_group\",\n    ]\n    target_col = _get_target_column(df)\n    feature_cols = [\n        col for col in df.columns if col not in excluded_cols + [target_col]\n    ]\n    X = df[feature_cols].fillna(0)\n    y = df[target_col].fillna(0)\n\n    print(f\"   Features: {len(feature_cols)}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\n    print(f\"   Target mean: {y.mean():.2f}\")\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    print(f\"   Training set: {X_train.shape[0]:,} samples\")\n    print(f\"   Test set: {X_test.shape[0]:,} samples\")\n\n    # Modelo para este grupo: nombre clase Snowflake ML (desde hyperparams o GROUP_MODEL)\n    model_type = GROUP_MODEL.get(segment_name, _DEFAULT_MODEL)\n    if segment_name in hyperparams_by_group:\n        algorithm = hyperparams_by_group[segment_name].get(\"algorithm\")\n        if algorithm:\n            model_type = algorithm\n        group_params = hyperparams_by_group[segment_name][\"params\"]\n        search_id = hyperparams_by_group[segment_name][\"search_id\"]\n        val_rmse = hyperparams_by_group[segment_name][\"val_rmse\"]\n        print(f\"\\n   ‚úÖ Using OPTIMIZED hyperparameters from script 03\")\n        print(f\"      Model: {model_type}\")\n        print(f\"      Search ID: {search_id}\")\n        print(f\"      Validation RMSE (from search): {val_rmse:.4f}\")\n    else:\n        group_params = DEFAULT_PARAMS_BY_MODEL.get(\n            model_type, DEFAULT_PARAMS_BY_MODEL[\"XGBRegressor\"]\n        )\n        print(\n            f\"\\n   ‚ö†Ô∏è  Using DEFAULT hyperparameters for {model_type} (no search results for {segment_name})\"\n        )\n\n    # Convert params to proper types\n    model_params = {}\n    for k, v in group_params.items():\n        if isinstance(v, (int, float)):\n            model_params[k] = v\n        elif isinstance(v, (np.integer, np.floating)):\n            model_params[k] = float(v) if isinstance(v, np.floating) else int(v)\n        else:\n            model_params[k] = v\n    model_params[\"random_state\"] = 42\n\n    MODEL_CLASSES = {\n        \"XGBRegressor\": XGBRegressor,\n        \"LGBMRegressor\": LGBMRegressor,\n        \"SGDRegressor\": SGDRegressor,\n    }\n    ModelClass = MODEL_CLASSES.get(model_type, XGBRegressor)\n    if model_type == \"XGBRegressor\":\n        model_params[\"n_jobs\"] = -1\n        model_params[\"objective\"] = \"reg:squarederror\"\n        model_params[\"eval_metric\"] = \"rmse\"\n    elif model_type == \"LGBMRegressor\":\n        model_params[\"n_jobs\"] = -1\n        model_params[\"verbosity\"] = -1\n    elif model_type == \"SGDRegressor\":\n        model_params.setdefault(\"penalty\", \"l2\")\n        model_params.setdefault(\"learning_rate\", \"invscaling\")\n\n    print(f\"\\n   Training {model_type} with {len(model_params)} hyperparameters...\")\n    model = ModelClass(**model_params)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test, y_pred)\n\n    print(f\"\\n   ‚úÖ Model trained\")\n    print(f\"      RMSE: {rmse:.2f}\")\n    print(f\"      MAE: {mae:.2f}\")\n    print(f\"{'='*80}\\n\")\n\n    model.rmse = rmse\n    model.mae = mae\n    model.training_samples = X_train.shape[0]\n    model.test_samples = X_test.shape[0]\n    model.feature_cols = feature_cols\n    model.hyperparameters = model_params\n    model.segment = segment_name\n    model.group_name = segment_name\n\n    return model\n\n\nprint(\"‚úÖ Training function defined\")\n",
      "id": "68de028e-f08e-43bb-a4c8-3d7875e231be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Execute Many Model Training (MMT) - 16 Models\n",
      "id": "9e9381da-376f-4b18-ac54-0a0caa4ed969"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üöÄ STARTING MANY MODEL TRAINING (MMT) - 16 MODELS\")\nprint(\"=\" * 80)\nprint(\n    \"\\nTraining 16 models in PARALLEL (LGBMRegressor, XGBRegressor, SGDRegressor per group)\"\n)\nprint(\"Each model uses group-specific hyperparameters\\n\")\n\nstart_time = time.time()\n\n# Create MMT trainer\ntrainer = ManyModelTraining(train_segment_model, \"BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\")\n\n# Execute training with partition_by stats_ntile_group\ntraining_run = trainer.run(\n    partition_by=\"stats_ntile_group\",  # ‚Üê KEY: Partition by group\n    snowpark_dataframe=training_df,\n    run_id=f\"uni_box_regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n)\n\nprint(\"\\n‚è≥ Training in progress... Monitoring completion...\\n\")\n\n# Monitor with timeout\nimport time as time_module\n\nmax_wait = 1800  # 30 minutes max\ncheck_interval = 10  # Check every 10 seconds\nelapsed = 0\ncompleted = False\n\nwhile elapsed < max_wait:\n    time_module.sleep(check_interval)\n    elapsed += check_interval\n\n    try:\n        done_count = 0\n        total_count = 0\n        for partition_id in training_run.partition_details:\n            total_count += 1\n            status = training_run.partition_details[partition_id].status\n            if status.name == \"DONE\" or status.name == \"FAILED\":\n                done_count += 1\n\n        print(\n            f\"‚è±Ô∏è  {elapsed}s elapsed - Progress: {done_count}/{total_count} models completed\",\n            end=\"\\r\",\n        )\n\n        if done_count == total_count:\n            print(\"\\n‚úÖ All models completed!\" + \" \" * 50)\n            completed = True\n            break\n    except:\n        print(f\"‚è±Ô∏è  {elapsed}s elapsed - Waiting for status update...\", end=\"\\r\")\n\nif not completed:\n    print(\"\\n‚è±Ô∏è  Timeout reached - Verifying completion via stage...\" + \" \" * 30)\n    stage_files = session.sql(\n        f\"LIST @BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS PATTERN='.*{training_run.run_id}.*'\"\n    ).collect()\n    if len(stage_files) > 0:\n        print(\n            f\"‚úÖ Found {len(stage_files)} model files in stage - Training completed successfully!\"\n        )\n        completed = True\n\nend_time = time.time()\nelapsed_minutes = (end_time - start_time) / 60\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"‚úÖ TRAINING COMPLETE! Status: {'COMPLETED' if completed else 'UNKNOWN'}\")\nprint(\"=\" * 80)\nprint(f\"\\n‚è±Ô∏è  Total training time: {elapsed_minutes:.2f} minutes\")\n",
      "id": "a887de4e-6990-4577-969c-8022d918876f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Review Training Results\n",
      "id": "548dbb94-e8bf-4024-a297-36f84996977a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüìä Training Results:\\n\")\n\nfor partition_id in training_run.partition_details:\n    details = training_run.partition_details[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            print(f\"\\n‚úÖ {partition_id if partition_id else 'DEFAULT'}:\")\n            print(f\"   RMSE: {model.rmse:.2f}\")\n            print(f\"   MAE: {model.mae:.2f}\")\n            print(f\"   Training samples: {model.training_samples:,}\")\n            print(f\"   Test samples: {model.test_samples:,}\")\n        except Exception as e:\n            print(f\"\\n‚ö†Ô∏è  {partition_id}: Could not load model - {str(e)[:100]}\")\n    else:\n        print(f\"\\n‚ùå {partition_id}: Training failed\")\n        print(f\"   Status: {details.status}\")\n",
      "id": "5106b4a3-0016-4ce6-a1b7-ca48d97ced1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Register Model in Model Registry\n",
      "id": "0b6052a4-261e-4d81-a8f6-d181c6c9a2f4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìù REGISTERING MODEL IN MODEL REGISTRY\")\nprint(\"=\" * 80)\n\nversion_date = datetime.now().strftime(\"%Y%m%d_%H%M\")\nregistered_models = {}\n\nfor partition_id in training_run.partition_details:\n    details = training_run.partition_details[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            # Model name includes group identifier\n            model_name = f\"uni_box_regression_{partition_id.lower()}\"\n\n            # Get group-specific search ID, algorithm, and hyperparameters if available\n            group_search_id = None\n            group_hyperparams = None\n            group_algorithm = GROUP_MODEL.get(partition_id, _DEFAULT_MODEL)\n            if partition_id in hyperparams_by_group:\n                group_search_id = hyperparams_by_group[partition_id][\"search_id\"]\n                group_hyperparams = hyperparams_by_group[partition_id][\"params\"]\n                alg = hyperparams_by_group[partition_id].get(\"algorithm\")\n                if alg:\n                    group_algorithm = alg\n\n            # Prepare sample input from this group\n            sample_input = (\n                training_df.filter(training_df[\"stats_ntile_group\"] == partition_id)\n                .select(model.feature_cols)\n                .limit(5)\n            )\n\n            print(f\"\\nRegistering {partition_id}...\")\n\n            # Prepare metrics including hyperparameters\n            model_metrics = {\n                \"rmse\": float(model.rmse),\n                \"mae\": float(model.mae),\n                \"training_samples\": int(model.training_samples),\n                \"test_samples\": int(model.test_samples),\n                \"algorithm\": group_algorithm,\n                \"group\": partition_id,\n                \"hyperparameter_search_id\": group_search_id or \"default\",\n            }\n\n            # Add hyperparameters to metrics (as nested dict)\n            if group_hyperparams:\n                # Convert hyperparameters to a format suitable for metrics\n                for key, value in group_hyperparams.items():\n                    if isinstance(value, (int, float)):\n                        model_metrics[f\"hyperparameter_{key}\"] = (\n                            float(value) if isinstance(value, float) else int(value)\n                        )\n                model_metrics[\"hyperparameters\"] = json.dumps(\n                    {\n                        k: float(v) if isinstance(v, (int, float)) else v\n                        for k, v in group_hyperparams.items()\n                    }\n                )\n\n            mv = registry.log_model(\n                model,\n                model_name=model_name,\n                version_name=f\"v_{version_date}\",\n                comment=f\"{group_algorithm} regression model for uni_box_week - Group: {partition_id}\",\n                metrics=model_metrics,\n                sample_input_data=sample_input,\n                task=task.Task.TABULAR_REGRESSION,\n            )\n\n            registered_models[partition_id] = {\n                \"model_name\": model_name,\n                \"version\": f\"v_{version_date}\",\n                \"model_version\": mv,\n            }\n\n            print(f\"‚úÖ {partition_id}: {model_name} v_{version_date}\")\n            print(f\"   RMSE: {model.rmse:.2f}, MAE: {model.mae:.2f}\")\n\n        except Exception as e:\n            print(f\"‚ùå Error registering model: {str(e)[:200]}\")\n\nprint(f\"\\n‚úÖ {len(registered_models)} model(s) registered successfully!\")\n",
      "id": "717e142b-6fd7-4198-b65a-90f382ace8a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Set Production Alias\n",
      "id": "82326281-24e4-4c65-bd3e-40890d8c9f55"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüè∑Ô∏è  Setting PRODUCTION aliases...\\n\")\n\nfor partition_id, model_info in registered_models.items():\n    model_name = model_info[\"model_name\"]\n    version = model_info[\"version\"]\n    model_version = model_info[\"model_version\"]\n\n    try:\n        # Remove existing PRODUCTION alias\n        try:\n            model_ref = registry.get_model(model_name)\n            model_ref.default.unset_alias(\"PRODUCTION\")\n        except:\n            pass\n\n        # Set alias on new version\n        model_version.set_alias(\"PRODUCTION\")\n        print(f\"‚úÖ {model_name}: PRODUCTION ‚Üí {version}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  {model_name}: Error setting alias - {str(e)[:100]}\")\n\nprint(\"\\n‚úÖ All production aliases configured!\")\n",
      "id": "ddc14bc4-c1b4-4886-ab56-10ba0b41231f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Summary\n",
      "id": "041c9828-2217-4cef-92ac-8dc7291158c7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üéâ MANY MODEL TRAINING (MMT) COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìä Summary:\")\nprint(f\"   ‚úÖ Models trained: {len(registered_models)}/16\")\nprint(f\"   ‚è±Ô∏è  Training time: {elapsed_minutes:.2f} minutes\")\nprint(f\"   üîß Algorithms: LGBM / XGB / SGD (per group)\")\nprint(f\"   üìà Hyperparameters: Group-specific (from hyperparameter search)\")\n\nif registered_models:\n    print(f\"\\nüèÜ Model Performance by Group:\")\n    for partition_id in sorted(registered_models.keys()):\n        model = training_run.get_model(partition_id)\n        print(\n            f\"   {partition_id}: RMSE={model.rmse:.2f}, MAE={model.mae:.2f}, Samples={model.training_samples:,}\"\n        )\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review model performance by group\")\nprint(\n    \"   2. Run 05_create_partitioned_model.py to create partitioned model (combines all 16)\"\n)\nprint(\n    \"   3. Run 06_partitioned_inference_batch.py for batch inference with automatic routing\"\n)\n\nprint(\"\\n\" + \"=\" * 80)\n",
      "id": "5ea49344-04d6-43a3-8194-6ae48a950b1f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}