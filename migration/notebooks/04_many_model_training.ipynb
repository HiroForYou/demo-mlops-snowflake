{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# MMT: 16 modelos (LGBM/XGB/SGD por stats_ntile_group)\nHiperpar√°metros por grupo desde script 03 ‚Üí entrenar ‚Üí registrar en Model Registry.\n",
      "id": "380fbd96-814b-413a-993f-704a8eef79a0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.modeling.distributors.many_model import ManyModelTraining\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.model import task\nimport time\nfrom datetime import datetime\nimport json\n\nsession = get_active_session()\n\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\nprint(f\"‚úÖ {session.get_current_database()}.{session.get_current_schema()}\")\n\nUSE_CLEANED_TABLES = False\nFEATURES_TABLE = \"BD_AA_DEV.SC_FEATURES_BMX.UNI_BOX_FEATURES\"\nMMT_SAMPLE_FRACTION = 0.01  # None = 100%\n\nGROUP_MODEL = {\n    \"group_stat_0_1\": \"LGBMRegressor\",\n    \"group_stat_0_2\": \"LGBMRegressor\",\n    \"group_stat_0_3\": \"LGBMRegressor\",\n    \"group_stat_0_4\": \"LGBMRegressor\",\n    \"group_stat_1_1\": \"LGBMRegressor\",\n    \"group_stat_1_2\": \"LGBMRegressor\",\n    \"group_stat_1_3\": \"XGBRegressor\",\n    \"group_stat_1_4\": \"SGDRegressor\",\n    \"group_stat_2_1\": \"LGBMRegressor\",\n    \"group_stat_2_2\": \"LGBMRegressor\",\n    \"group_stat_2_3\": \"XGBRegressor\",\n    \"group_stat_2_4\": \"XGBRegressor\",\n    \"group_stat_3_1\": \"LGBMRegressor\",\n    \"group_stat_3_2\": \"LGBMRegressor\",\n    \"group_stat_3_3\": \"LGBMRegressor\",\n    \"group_stat_3_4\": \"SGDRegressor\",\n}\n_DEFAULT_MODEL = \"XGBRegressor\"\n",
      "id": "3f196012-f56a-41c7-a6cf-ab02f5e1dfa4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Registry y stage\n",
      "id": "7bef0374-d022-440b-aedc-017c64a85f8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "session.sql(\"CREATE STAGE IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\").collect()\nregistry = Registry(session=session, database_name=\"BD_AA_DEV\", schema_name=\"SC_MODELS_BMX\")\nprint(\"‚úÖ Registry + stage listos\")\n",
      "id": "e5e3e4fd-47d5-408e-b19e-de5cba102ecd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Hiperpar√°metros por grupo (Experiments o tabla)\n",
      "id": "41d118b1-4de5-417f-8fb3-868886674bea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "hyperparams_by_group = {}\nexperiments_loaded = False\nall_groups_from_data = session.sql(\n    \"\"\"\n    SELECT DISTINCT stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE stats_ntile_group IS NOT NULL\n    ORDER BY stats_ntile_group\n\"\"\"\n).collect()\n\nexpected_groups = [row[\"STATS_NTILE_GROUP\"] for row in all_groups_from_data]\n\nprint(\"\\nüî¨ Cargando desde ML Experiments...\")\ntry:\n    exp_tracking = ExperimentTracking(session)\n    from datetime import datetime, timedelta\n\n    today = datetime.now().strftime(\"%Y%m%d\")\n    experiment_name = f\"hyperparameter_search_regression_{today}\"\n\n    try:\n        exp_tracking.set_experiment(experiment_name)\n        print(f\"‚úÖ Found experiment: {experiment_name}\")\n\n        # Get all runs from this experiment\n        # Note: The exact API may vary - this is a conceptual approach\n        # You may need to query the experiments table directly\n        experiments_loaded = True\n        print(\"   ‚úÖ ML Experiments available - loading from experiments\")\n    except:\n        # Try yesterday's experiment as fallback\n        yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\n        experiment_name = f\"hyperparameter_search_regression_{yesterday}\"\n        try:\n            exp_tracking.set_experiment(experiment_name)\n            print(f\"‚úÖ Found experiment: {experiment_name}\")\n            experiments_loaded = True\n        except:\n            print(\"   ‚ö†Ô∏è  No recent experiment found, will use table fallback\")\n            experiments_loaded = False\n\n    if experiments_loaded:\n        try:\n            print(f\"   üìã Runs en experiment: {experiment_name}\")\n\n            runs_query = f\"SHOW RUNS IN EXPERIMENT {experiment_name}\"\n            runs_df = session.sql(runs_query)\n            runs_list = runs_df.collect()\n\n            if len(runs_list) == 0:\n                print(\"   ‚ö†Ô∏è  No runs found in experiment, using table fallback\")\n                experiments_loaded = False\n            else:\n                print(f\"   ‚úÖ Found {len(runs_list)} runs in experiment\")\n\n                runs_by_group = {}\n\n                for run in runs_list:\n                    run_name = run[\"name\"]\n\n                    try:\n                        # Get parameters for this run\n                        params_query = f\"SHOW RUN PARAMETERS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        params_df = session.sql(params_query)\n                        params_list = params_df.collect()\n\n                        # Get metrics for this run\n                        metrics_query = f\"SHOW RUN METRICS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        metrics_df = session.sql(metrics_query)\n                        metrics_list = metrics_df.collect()\n\n                        # Extract group_name and algorithm from parameters\n                        group_name = None\n                        search_id = None\n                        algorithm = None\n                        best_params = {}\n\n                        for param in params_list:\n                            param_name = param[\"name\"]\n                            param_value = param[\"value\"]\n\n                            if param_name == \"group_name\":\n                                group_name = param_value\n                            elif param_name == \"search_id\":\n                                search_id = param_value\n                            elif param_name == \"algorithm\":\n                                algorithm = param_value\n                            else:\n                                best_params[param_name] = param_value\n\n                        # Extract metrics\n                        val_rmse = None\n                        val_mae = None\n\n                        for metric in metrics_list:\n                            metric_name = metric[\"name\"]\n                            metric_value = metric[\"value\"]\n\n                            if metric_name == \"val_rmse\":\n                                val_rmse = float(metric_value)\n                            elif metric_name == \"val_mae\":\n                                val_mae = float(metric_value)\n\n                        # Only process runs that have a group_name\n                        if group_name and val_rmse is not None:\n                            alg = algorithm or GROUP_MODEL.get(\n                                group_name, _DEFAULT_MODEL\n                            )\n                            if group_name not in runs_by_group:\n                                runs_by_group[group_name] = {\n                                    \"run_name\": run_name,\n                                    \"params\": best_params,\n                                    \"val_rmse\": val_rmse,\n                                    \"val_mae\": val_mae,\n                                    \"search_id\": search_id,\n                                    \"algorithm\": alg,\n                                }\n                            else:\n                                if val_rmse < runs_by_group[group_name][\"val_rmse\"]:\n                                    runs_by_group[group_name] = {\n                                        \"run_name\": run_name,\n                                        \"params\": best_params,\n                                        \"val_rmse\": val_rmse,\n                                        \"val_mae\": val_mae,\n                                        \"search_id\": search_id,\n                                        \"algorithm\": alg,\n                                    }\n\n                    except Exception as run_error:\n                        print(\n                            f\"   ‚ö†Ô∏è  Error processing run {run_name}: {str(run_error)[:100]}\"\n                        )\n                        continue\n\n                # Step 3: Store results in hyperparams_by_group\n                if len(runs_by_group) > 0:\n                    print(f\"   ‚úÖ Loaded {len(runs_by_group)} groups from Experiments\")\n\n                    for group_name, run_info in runs_by_group.items():\n                        hyperparams_by_group[group_name] = {\n                            \"params\": run_info[\"params\"],\n                            \"val_rmse\": run_info[\"val_rmse\"],\n                            \"search_id\": run_info[\"search_id\"] or f\"exp_{group_name}\",\n                            \"algorithm\": run_info.get(\"algorithm\", _DEFAULT_MODEL),\n                        }\n\n                        print(f\"\\n   {group_name}:\")\n                        print(\n                            f\"      Algorithm: {run_info.get('algorithm', _DEFAULT_MODEL)}\"\n                        )\n                        print(f\"      Val RMSE: {run_info['val_rmse']:.4f}\")\n                        if run_info[\"val_mae\"]:\n                            print(f\"      Val MAE: {run_info['val_mae']:.4f}\")\n                        print(f\"      Search ID: {run_info['search_id'] or 'N/A'}\")\n                        print(\n                            f\"      Source: ML Experiments (run: {run_info['run_name']})\"\n                        )\n\n                    experiments_loaded = True\n                else:\n                    print(\n                        \"   ‚ö†Ô∏è  No valid runs with group_name found, using table fallback\"\n                    )\n                    experiments_loaded = False\n\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Error using ExperimentTracking API: {str(e)[:200]}\")\n            print(\"   Will use table fallback\")\n            experiments_loaded = False\n\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will use table fallback\")\n    experiments_loaded = False\n",
      "id": "91be0154-2f8f-42d7-946b-0c3e8101a95e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 2b. Fallback a tabla\n",
      "id": "0481de94-e031-48a7-bf44-21bf07564000"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "if not experiments_loaded or len(hyperparams_by_group) < len(expected_groups):\n    print(\"\\nüìã Fallback: HYPERPARAMETER_RESULTS\")\n\n    table_exists = False\n    try:\n        check_table = session.sql(\n            \"\"\"\n            SELECT COUNT(*) as CNT \n            FROM INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = 'SC_MODELS_BMX' \n            AND TABLE_NAME = 'HYPERPARAMETER_RESULTS'\n            AND TABLE_CATALOG = 'BD_AA_DEV'\n            \"\"\"\n        ).collect()\n        table_exists = check_table[0][\"CNT\"] > 0\n    except:\n        table_exists = False\n\n    if table_exists:\n        hyperparams_df = session.sql(\n            \"\"\"\n            WITH latest_searches AS (\n                SELECT \n                    group_name,\n                    search_id,\n                    algorithm,\n                    best_params,\n                    best_cv_rmse,\n                    val_rmse,\n                    val_mae,\n                    created_at,\n                    ROW_NUMBER() OVER (PARTITION BY group_name ORDER BY created_at DESC) AS rn\n                FROM BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n                WHERE group_name IS NOT NULL\n            )\n            SELECT \n                group_name,\n                search_id,\n                best_params,\n                best_cv_rmse,\n                val_rmse,\n                val_mae\n            FROM latest_searches\n            WHERE rn = 1\n            ORDER BY group_name\n        \"\"\"\n        )\n\n        hyperparams_results = hyperparams_df.collect()\n\n        if len(hyperparams_results) > 0:\n            print(f\"   ‚úÖ Loaded {len(hyperparams_results)} groups from table\")\n\n            for result in hyperparams_results:\n                group_name = result[\"GROUP_NAME\"]\n                best_params_json = result[\"BEST_PARAMS\"]\n\n                if isinstance(best_params_json, str):\n                    best_params = json.loads(best_params_json)\n                else:\n                    best_params = best_params_json\n\n                if group_name not in hyperparams_by_group:\n                    alg = result.get(\"ALGORITHM\") or GROUP_MODEL.get(\n                        group_name, _DEFAULT_MODEL\n                    )\n                    hyperparams_by_group[group_name] = {\n                        \"params\": best_params,\n                        \"val_rmse\": result[\"VAL_RMSE\"],\n                        \"search_id\": result[\"SEARCH_ID\"],\n                        \"algorithm\": alg,\n                    }\n\n                    print(f\"\\n   {group_name}:\")\n                    print(f\"      Algorithm: {alg}\")\n                    print(f\"      Val RMSE: {result['VAL_RMSE']:.4f}\")\n                    print(f\"      Search ID: {result['SEARCH_ID']}\")\n                    print(f\"      Source: Table (fallback)\")\n        else:\n            print(\"   ‚ö†Ô∏è  Table exists but has no results\")\n    else:\n        print(\"   ‚ö†Ô∏è  Table does not exist (this is OK if using ML Experiments)\")\n",
      "id": "84754a88-15cf-45b6-9033-902e82e0b4f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 2c. Defaults y validaci√≥n\n",
      "id": "f6979ad6-e133-432c-813e-602a533f0d72"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "if len(hyperparams_by_group) == 0:\n    raise ValueError(\n        \"No hyperparameter results found in Experiments or table! Please run 03_hyperparameter_search.py first\"\n    )\n\nprint(\n    f\"\\n‚úÖ Total loaded hyperparameters: {len(hyperparams_by_group)}/{len(expected_groups)} groups\"\n)\n\nDEFAULT_PARAMS_BY_MODEL = {\n    \"XGBRegressor\": {\n        \"n_estimators\": 100,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"min_child_weight\": 1,\n        \"gamma\": 0,\n        \"reg_alpha\": 0,\n        \"reg_lambda\": 1,\n    },\n    \"LGBMRegressor\": {\n        \"n_estimators\": 100,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"reg_alpha\": 0,\n        \"reg_lambda\": 1,\n        \"min_child_samples\": 20,\n    },\n    \"SGDRegressor\": {\n        \"alpha\": 0.0001,\n        \"max_iter\": 2000,\n        \"tol\": 1e-3,\n        \"eta0\": 0.01,\n    },\n}\n\nprint(f\"\\nüìã Defaults por modelo: {list(DEFAULT_PARAMS_BY_MODEL.keys())}\")\nprint(f\"üîç Validando cobertura...\")\ngroups_with_hyperparams = set(hyperparams_by_group.keys())\ngroups_without_hyperparams = set(expected_groups) - groups_with_hyperparams\n\nif groups_without_hyperparams:\n    print(\n        f\"‚ö†Ô∏è  WARNING: {len(groups_without_hyperparams)} groups will use default hyperparameters:\"\n    )\n    for group in sorted(groups_without_hyperparams):\n        print(f\"      - {group}\")\nelse:\n    print(f\"‚úÖ All {len(expected_groups)} groups have optimized hyperparameters!\")\n",
      "id": "ccda257b-ea9f-4cc7-af3b-c2efb7a9ae11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Datos de entrenamiento\n",
      "id": "a1254257-d277-42a2-8ce8-7f337e39a6a2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüè™ Cargando datos de entrenamiento...\")\n\nif USE_CLEANED_TABLES:\n    print(\"üìä Loading from cleaned table: TRAIN_DATASET_CLEANED\")\n    training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n    print(f\"\\n‚úÖ Training data loaded from cleaned table\")\n    print(f\"   Total records: {training_df.count():,}\")\n    print(f\"   Columns: {len(training_df.columns)}\")\nelse:\n    # Preferimos la tabla materializada de features (sin Dynamic Tables).\n    # Si falla por permisos/no existencia, hacemos fallback a la tabla limpia.\n    try:\n        # Mantener inicializaci√≥n del Feature Store (aunque no usemos FeatureView)\n        _fs = FeatureStore(\n            session=session,\n            database=\"BD_AA_DEV\",\n            name=\"SC_FEATURES_BMX\",\n            default_warehouse=\"WH_AA_DEV_DS_SQL\",\n        )\n        print(\"‚úÖ Feature Store inicializado (sin FeatureView)\")\n\n        print(f\"üìä Loading features from table: {FEATURES_TABLE}\")\n        features_df = session.table(FEATURES_TABLE)\n\n        print(\"‚è≥ Loading target variable and stats_ntile_group from training table...\")\n        target_df = session.table(\n            \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n        ).select(\n            \"customer_id\", \"brand_pres_ret\", \"week\", \"uni_box_week\", \"stats_ntile_group\"\n        )\n\n        print(\"‚è≥ Joining features with target...\")\n        training_df = features_df.join(\n            target_df, on=[\"customer_id\", \"brand_pres_ret\", \"week\"], how=\"inner\"\n        )\n\n        print(f\"\\n‚úÖ Training data loaded from features table + target\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n    except Exception as e:\n        print(\n            f\"‚ö†Ô∏è  Could not load/join features table ({FEATURES_TABLE}): {str(e)[:200]}\"\n        )\n        print(\"   Falling back to TRAIN_DATASET_CLEANED\")\n        training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n        print(f\"\\n‚úÖ Training data loaded from cleaned table (fallback)\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n\nPARTITION_COL = next(\n    (c for c in training_df.columns if c.upper() == \"STATS_NTILE_GROUP\"),\n    \"STATS_NTILE_GROUP\",\n)\nprint(f\"\\nüìå Partici√≥n: '{PARTITION_COL}'\")\nprint(\"\\nüìä Filas por grupo:\")\ngroup_counts = (\n    training_df.group_by(PARTITION_COL).count().sort(PARTITION_COL)\n)\ngroup_counts.show()\n\nif MMT_SAMPLE_FRACTION is not None and 0 < MMT_SAMPLE_FRACTION < 1:\n    n_before = training_df.count()\n    training_df = training_df.sample(frac=MMT_SAMPLE_FRACTION)\n    n_after = training_df.count()\n    print(f\"\\n‚ö†Ô∏è  MMT en modo PRUEBA: usando {MMT_SAMPLE_FRACTION*100:.0f}% de la data ({n_after:,} de {n_before:,} filas)\")\n",
      "id": "27444695-19d0-47c6-bd71-d57fc2defe98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Funci√≥n de entrenamiento MMT\n",
      "id": "43578674-0429-4f67-901c-d448a3cb9a33"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def _get_target_column(df):\n    for c in df.columns:\n        if str(c).upper() == \"UNI_BOX_WEEK\":\n            return c\n    return \"uni_box_week\"\n\n\ndef _get_feature_cols_numeric(df, excluded_cols, target_col):\n    \"\"\"Solo columnas num√©ricas (igual que script 03): Snowflake ML exige int/float/bool.\"\"\"\n    excluded_upper = {str(x).upper() for x in list(excluded_cols) + [target_col]}\n    return [\n        col\n        for col in df.columns\n        if str(col).upper() not in excluded_upper\n        and getattr(df[col].dtype, \"kind\", \"O\") in \"iufb\"\n    ]\n\n\ndef train_segment_model(data_connector, context):\n    import pandas as pd\n    from snowflake.ml.modeling.xgboost import XGBRegressor\n    from snowflake.ml.modeling.lightgbm import LGBMRegressor\n    from snowflake.ml.modeling.linear_model import SGDRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    import numpy as np\n\n    segment_name = context.partition_id\n    print(f\"\\n{'='*80}\")\n    print(f\"üöÄ Training model for {segment_name}\")\n    print(f\"{'='*80}\")\n\n    df = data_connector.to_pandas()\n    print(f\"üìä Data shape: {df.shape}\")\n\n    partition_col_in_df = next(\n        (c for c in df.columns if c.upper() == \"STATS_NTILE_GROUP\"), \"STATS_NTILE_GROUP\"\n    )\n    excluded_cols = [\n        \"customer_id\",\n        \"brand_pres_ret\",\n        \"week\",\n        \"FEATURE_TIMESTAMP\",\n        partition_col_in_df,\n    ]\n    target_col = _get_target_column(df)\n    feature_cols = _get_feature_cols_numeric(df, excluded_cols, target_col)\n    if len(feature_cols) < 5:\n        feature_cols = [c for c in df.columns if c not in excluded_cols + [target_col]]\n    X = df[feature_cols].copy()\n    for c in feature_cols:\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0)\n    y = pd.to_numeric(df[target_col], errors=\"coerce\").fillna(0)\n\n    print(f\"   Features: {len(feature_cols)}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\n    print(f\"   Target mean: {y.mean():.2f}\")\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    print(f\"   Training set: {X_train.shape[0]:,} samples\")\n    print(f\"   Test set: {X_test.shape[0]:,} samples\")\n\n    # Snowflake ML exige int/float/bool (como script 03); asegurar float64\n    train_dataset = X_train.copy()\n    for c in feature_cols:\n        train_dataset[c] = np.asarray(train_dataset[c], dtype=np.float64)\n    train_dataset[target_col] = np.asarray(y_train, dtype=np.float64)\n    test_features = X_test.copy()\n    for c in feature_cols:\n        test_features[c] = np.asarray(test_features[c], dtype=np.float64)\n\n    model_type = GROUP_MODEL.get(segment_name, _DEFAULT_MODEL)\n    if segment_name in hyperparams_by_group:\n        algorithm = hyperparams_by_group[segment_name].get(\"algorithm\")\n        if algorithm:\n            model_type = algorithm\n        group_params = hyperparams_by_group[segment_name][\"params\"]\n        search_id = hyperparams_by_group[segment_name][\"search_id\"]\n        val_rmse = hyperparams_by_group[segment_name][\"val_rmse\"]\n        print(f\"\\n   ‚úÖ Using OPTIMIZED hyperparameters from script 03\")\n        print(f\"      Model: {model_type}\")\n        print(f\"      Search ID: {search_id}\")\n        print(f\"      Validation RMSE (from search): {val_rmse:.4f}\")\n    else:\n        group_params = DEFAULT_PARAMS_BY_MODEL.get(\n            model_type, DEFAULT_PARAMS_BY_MODEL[\"XGBRegressor\"]\n        )\n        print(\n            f\"\\n   ‚ö†Ô∏è  Using DEFAULT hyperparameters for {model_type} (no search results for {segment_name})\"\n        )\n\n    def _to_native(v):\n        \"\"\"Como script 03: numpy -> Python nativo; string num√©rico -> float/int.\"\"\"\n        if hasattr(v, \"item\"):\n            return v.item()\n        if isinstance(v, (int, float)):\n            return v\n        if isinstance(v, (np.integer, np.floating)):\n            return int(v) if isinstance(v, np.integer) else float(v)\n        if isinstance(v, str):\n            v = v.strip()\n            try:\n                f = float(v)\n                return int(f) if f == int(f) else f\n            except (ValueError, TypeError):\n                return v\n        return v\n\n    int_params = (\"n_estimators\", \"max_depth\", \"num_leaves\", \"min_child_weight\", \"min_child_samples\", \"max_iter\")\n    float_params = (\"alpha\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"gamma\", \"reg_alpha\", \"reg_lambda\", \"tol\", \"eta0\")\n    defaults = DEFAULT_PARAMS_BY_MODEL.get(model_type, DEFAULT_PARAMS_BY_MODEL[\"XGBRegressor\"])\n    model_params = {}\n    for k, v in group_params.items():\n        vn = _to_native(v)\n        try:\n            if k in int_params:\n                model_params[k] = int(vn) if isinstance(vn, (int, float, np.integer, np.floating)) else defaults.get(k, vn)\n            elif k in float_params:\n                model_params[k] = float(vn) if isinstance(vn, (int, float, np.integer, np.floating)) else defaults.get(k, vn)\n            else:\n                model_params[k] = vn\n        except (TypeError, ValueError):\n            model_params[k] = defaults.get(k, vn)\n    model_params[\"random_state\"] = 42\n\n    MODEL_CLASSES = {\n        \"XGBRegressor\": XGBRegressor,\n        \"LGBMRegressor\": LGBMRegressor,\n        \"SGDRegressor\": SGDRegressor,\n    }\n    ModelClass = MODEL_CLASSES.get(model_type, XGBRegressor)\n    if model_type == \"XGBRegressor\":\n        model_params[\"n_jobs\"] = -1\n        model_params[\"objective\"] = \"reg:squarederror\"\n        model_params[\"eval_metric\"] = \"rmse\"\n    elif model_type == \"LGBMRegressor\":\n        model_params[\"n_jobs\"] = -1\n        model_params[\"verbosity\"] = -1\n    elif model_type == \"SGDRegressor\":\n        model_params.setdefault(\"penalty\", \"l2\")\n        model_params.setdefault(\"learning_rate\", \"invscaling\")\n\n    print(f\"\\n   Training {model_type} with {len(model_params)} hyperparameters...\")\n    model = ModelClass(\n        input_cols=feature_cols, label_cols=[target_col], **model_params\n    )\n    model.fit(train_dataset)\n\n    pred_result = model.predict(test_features)\n    pred_df = pred_result.to_pandas() if hasattr(pred_result, \"to_pandas\") else pred_result\n    out_col = model.get_output_cols()[0]\n    y_pred = np.asarray(pred_df[out_col])\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test, y_pred)\n\n    print(f\"\\n   ‚úÖ Model trained\")\n    print(f\"      RMSE: {rmse:.2f}\")\n    print(f\"      MAE: {mae:.2f}\")\n    print(f\"{'='*80}\\n\")\n\n    model.rmse = rmse\n    model.mae = mae\n    model.training_samples = X_train.shape[0]\n    model.test_samples = X_test.shape[0]\n    model.feature_cols = feature_cols\n    model.hyperparameters = model_params\n    model.segment = segment_name\n    model.group_name = segment_name\n\n    return model\n\n",
      "id": "b2f79530-1488-45f9-bc05-598c700cab29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Escalar cluster, Ray Dashboard, MMT\n",
      "id": "8bf987cb-e07a-465f-9a6d-09e308953e2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "try:\n    from snowflake.ml.runtime_cluster import scale_cluster\n    scale_cluster(expected_cluster_size=4, options={\"block_until_min_cluster_size\": 2})\n    print(\"‚úÖ Cluster 4 nodos\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è scale_cluster: {str(e)[:150]}\")\n\ntry:\n    from snowflake.ml.runtime_cluster import get_ray_dashboard_url\n    print(f\"‚úÖ Ray Dashboard: {get_ray_dashboard_url()}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Ray Dashboard: {str(e)[:100]}\")\n",
      "id": "0d26ce91-fba4-4f21-8c57-b16828ba3126",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "start_time = time.time()\ntrainer = ManyModelTraining(train_segment_model, \"BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\")\ntraining_run = trainer.run(\n    partition_by=PARTITION_COL,\n    snowpark_dataframe=training_df,\n    run_id=f\"uni_box_regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n)\n\nprint(f\"\\n‚úÖ Run ID: {training_run.run_id}\\n\")\n",
      "id": "046d7778-1a58-4eb3-9d7c-e143ebbc11c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5d. Esperar MMT (opcional; si falla partition_details se sale del bucle)\n",
      "id": "f17e5cd6-f951-4a63-9d3b-97311ef33e3d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import time as time_module\nMMT_MAX_WAIT = 600\nMMT_CHECK_INTERVAL = 30\n\nelapsed = 0\ncompleted = False\nrun_start = start_time\n\nwhile elapsed < MMT_MAX_WAIT:\n    time_module.sleep(MMT_CHECK_INTERVAL)\n    elapsed += MMT_CHECK_INTERVAL\n\n    try:\n        details = training_run.partition_details\n    except Exception as e:\n        print(f\"\\n‚ö†Ô∏è  partition_details fall√≥: {str(e)[:180]}\")\n        print(\"   Deja de esperar. Revisa Ray Dashboard o ejecuta 6/7 m√°s tarde.\")\n        break\n\n    total_count = len(details)\n    done_count = sum(1 for pid in details if details[pid].status.name == \"DONE\")\n    failed_count = sum(1 for pid in details if details[pid].status.name == \"FAILED\")\n    pending_count = total_count - done_count - failed_count\n    print(\n        f\"‚è±Ô∏è  {elapsed}s - OK: {done_count} | FAILED: {failed_count} | pending: {pending_count}\",\n        end=\"\\r\",\n    )\n\n    if done_count + failed_count == total_count:\n        print(\"\\n‚úÖ All models completed!\" + \" \" * 30)\n        completed = True\n        break\n\nif not completed:\n    print(\"\\n‚è±Ô∏è  Timeout. Training puede seguir en background; revisa Ray Dashboard o ejecuta 6/7 m√°s tarde.\")\n    try:\n        stage_files = session.sql(\n            f\"LIST @BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS PATTERN='.*{training_run.run_id}.*'\"\n        ).collect()\n        if len(stage_files) >= 16:\n            print(f\"\\n‚úÖ Hay {len(stage_files)} archivos en stage - training probablemente completado.\")\n            completed = True\n    except Exception:\n        pass\nelse:\n    print(\"\\n‚úÖ TRAINING COMPLETE\")\nend_time = time.time()\nprint(f\"\\n‚è±Ô∏è  {((end_time - run_start) / 60):.2f} min\")\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n    scale_cluster(expected_cluster_size=1)\n    print(\"‚úÖ Cluster scale down a 1 nodo\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è scale down: {str(e)[:120]}\")\n",
      "id": "3571d09d-e60b-452e-8845-d89e45327191",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Resultados por partici√≥n\n",
      "id": "b3f19788-cacb-4cd8-8e3e-3c5573da66e8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "try:\n    partition_details = training_run.partition_details\nexcept Exception as e:\n    partition_details = {}\n    print(f\"‚ö†Ô∏è partition_details fall√≥: {str(e)[:200]}. Re-ejecuta desde ¬ß3, luego 5c‚Üí5d‚Üí6.\")\n\ndone_ids = []\nfailed_ids = []\npending_ids = []\nfor partition_id in partition_details:\n    details = partition_details[partition_id]\n    st = details.status.name\n    if st == \"DONE\":\n        done_ids.append(partition_id)\n        try:\n            model = training_run.get_model(partition_id)\n            print(f\"\\n‚úÖ {partition_id}: RMSE={model.rmse:.2f}, MAE={model.mae:.2f}, samples={model.training_samples:,}\")\n        except Exception as e:\n            print(f\"\\n‚ö†Ô∏è  {partition_id}: DONE pero no se pudo cargar - {str(e)[:100]}\")\n    elif st == \"FAILED\":\n        failed_ids.append(partition_id)\n        print(f\"\\n‚ùå {partition_id}: FAILED\")\n        try:\n            logs = getattr(details, \"logs\", None)\n            if logs and \"Error:\" in logs:\n                err_line = next((l for l in logs.split(\"\\n\") if \"Error:\" in l), None)\n                if err_line:\n                    print(f\"   {err_line.strip()[:200]}\")\n        except Exception:\n            pass\n    else:\n        pending_ids.append(partition_id)\n        print(f\"\\n‚è≥ {partition_id}: {st}\")\nprint(f\"\\n--- Resumen: {len(done_ids)} OK, {len(failed_ids)} FAILED, {len(pending_ids)} pendientes ---\")\n",
      "id": "ee78d596-f50b-4f39-82ee-93ed482d6b6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Registrar modelos en Registry\n",
      "id": "e0e65247-62ef-4ac6-a84d-0ca4051c4063"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "version_date = datetime.now().strftime(\"%Y%m%d_%H%M\")\nregistered_models = {}\ntry:\n    _reg_partitions = training_run.partition_details\nexcept Exception as e:\n    _reg_partitions = {}\n    print(f\"‚ö†Ô∏è partition_details: {str(e)[:180]}\")\n\nfor partition_id in _reg_partitions:\n    details = _reg_partitions[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            model_name = f\"uni_box_regression_{partition_id.lower()}\"\n            group_search_id = None\n            group_hyperparams = None\n            group_algorithm = GROUP_MODEL.get(partition_id, _DEFAULT_MODEL)\n            if partition_id in hyperparams_by_group:\n                group_search_id = hyperparams_by_group[partition_id][\"search_id\"]\n                group_hyperparams = hyperparams_by_group[partition_id][\"params\"]\n                alg = hyperparams_by_group[partition_id].get(\"algorithm\")\n                if alg:\n                    group_algorithm = alg\n\n            print(f\"\\nRegistrando {partition_id}...\")\n            model_metrics = {\n                \"rmse\": float(model.rmse),\n                \"mae\": float(model.mae),\n                \"training_samples\": int(model.training_samples),\n                \"test_samples\": int(model.test_samples),\n                \"algorithm\": group_algorithm,\n                \"group\": partition_id,\n                \"hyperparameter_search_id\": group_search_id or \"default\",\n            }\n\n            if group_hyperparams:\n                for key, value in group_hyperparams.items():\n                    if isinstance(value, (int, float)):\n                        model_metrics[f\"hyperparameter_{key}\"] = (\n                            float(value) if isinstance(value, float) else int(value)\n                        )\n                model_metrics[\"hyperparameters\"] = json.dumps(\n                    {\n                        k: float(v) if isinstance(v, (int, float)) else v\n                        for k, v in group_hyperparams.items()\n                    }\n                )\n\n            mv = registry.log_model(\n                model,\n                model_name=model_name,\n                version_name=f\"v_{version_date}\",\n                comment=f\"{group_algorithm} regression model for uni_box_week - Group: {partition_id}\",\n                metrics=model_metrics,\n                task=task.Task.TABULAR_REGRESSION,\n            )\n\n            registered_models[partition_id] = {\n                \"model_name\": model_name,\n                \"version\": f\"v_{version_date}\",\n                \"model_version\": mv,\n            }\n\n            print(f\"‚úÖ {partition_id}: {model_name} v_{version_date}\")\n            print(f\"   RMSE: {model.rmse:.2f}, MAE: {model.mae:.2f}\")\n\n        except Exception as e:\n            print(f\"‚ùå Error registering model: {str(e)[:200]}\")\n\nprint(f\"\\n‚úÖ {len(registered_models)} model(s) registered successfully!\")\n",
      "id": "e2ab1a16-7449-4a7a-8b8e-a702a378fd0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Alias PRODUCTION\n",
      "id": "7a749abb-ff22-465e-aad0-9c8ca30288da"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\nfor partition_id, model_info in registered_models.items():\n    model_name = model_info[\"model_name\"]\n    version = model_info[\"version\"]\n    model_version = model_info[\"model_version\"]\n\n    try:\n        try:\n            registry.get_model(model_name).default.unset_alias(\"PRODUCTION\")\n        except Exception:\n            pass\n        model_version.set_alias(\"PRODUCTION\")\n        print(f\"‚úÖ {model_name}: PRODUCTION ‚Üí {version}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  {model_name}: Error setting alias - {str(e)[:100]}\")\n",
      "id": "aa0187f1-f0d6-4333-8ec9-2c0f8e27bbb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Resumen\n",
      "id": "a7a59974-d232-4b12-8b2c-4a472a08c0be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "_elapsed = (time.time() - start_time) / 60\nprint(f\"\\n‚úÖ MMT: {len(registered_models)}/16 modelos | {_elapsed:.2f} min\")\nif registered_models:\n    for pid in sorted(registered_models.keys()):\n        try:\n            m = training_run.get_model(pid)\n            print(f\"   {pid}: RMSE={m.rmse:.2f}, MAE={m.mae:.2f}\")\n        except Exception:\n            pass\nprint(\"   Siguiente: 05_create_partitioned_model.py ‚Üí 06_partitioned_inference_batch.py\")\n\n",
      "id": "e18b3b1f-3d6c-495d-a2db-a9c8c094e5f7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}