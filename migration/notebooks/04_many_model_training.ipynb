{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Migration: Many Model Training (MMT) - 16 Models (LGBMRegressor, XGBRegressor, SGDRegressor)\n#\n## Overview\nThis script trains **16 regression models** (one per stats_ntile_group) using Many Model Training (MMT).\nModel type per group: LGBMRegressor, XGBRegressor, or SGDRegressor (see GROUP_MODEL).\nEach model is trained with group-specific hyperparameters from script 03.\n#\n## What We'll Do:\n1. Load best hyperparameters per group from hyperparameter search\n2. Define training function for MMT (with group-specific hyperparameters)\n3. Execute MMT training with partition_by=\"stats_ntile_group\"\n4. Register 16 models in Model Registry (one per group)\n5. Create group-to-model mapping\n",
      "id": "995c3fe6-b1a4-4c4a-9f44-de76b64125ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.modeling.distributors.many_model import ManyModelTraining\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore\nfrom snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.model import task\nimport time\nfrom datetime import datetime\nimport json\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE BD_AA_DEV\").collect()\nsession.sql(\"USE SCHEMA SC_STORAGE_BMX_PS\").collect()\n\nprint(f\"‚úÖ Connected to Snowflake\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")\n\n# Configuraci√≥n:\n# - Si no tienes permisos para FeatureView/Dynamic Tables, usa tablas limpias o tabla de features materializada.\nUSE_CLEANED_TABLES = False  # True = TRAIN_DATASET_CLEANED, False = intentar tabla de features materializada\nFEATURES_TABLE = (\n    \"BD_AA_DEV.SC_FEATURES_BMX.UNI_BOX_FEATURES\"  # creada por 02_feature_store_setup.py\n)\n\n# Un solo objeto: grupo -> nombre de clase Snowflake ML (snowflake.ml.modeling.*)\nGROUP_MODEL = {\n    \"group_stat_0_1\": \"LGBMRegressor\",\n    \"group_stat_0_2\": \"LGBMRegressor\",\n    \"group_stat_0_3\": \"LGBMRegressor\",\n    \"group_stat_0_4\": \"LGBMRegressor\",\n    \"group_stat_1_1\": \"LGBMRegressor\",\n    \"group_stat_1_2\": \"LGBMRegressor\",\n    \"group_stat_1_3\": \"XGBRegressor\",\n    \"group_stat_1_4\": \"SGDRegressor\",\n    \"group_stat_2_1\": \"LGBMRegressor\",\n    \"group_stat_2_2\": \"LGBMRegressor\",\n    \"group_stat_2_3\": \"XGBRegressor\",\n    \"group_stat_2_4\": \"XGBRegressor\",\n    \"group_stat_3_1\": \"LGBMRegressor\",\n    \"group_stat_3_2\": \"LGBMRegressor\",\n    \"group_stat_3_3\": \"LGBMRegressor\",\n    \"group_stat_3_4\": \"SGDRegressor\",\n}\n_DEFAULT_MODEL = \"XGBRegressor\"\n",
      "id": "efe713da-24b0-467c-bfae-1eea84091a8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Setup Model Registry & Staging\n",
      "id": "f74f58f8-bcbd-451a-a9ad-9705e77028d9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# CREATE SCHEMA comentado (puede requerir permisos)\n# session.sql(\"CREATE SCHEMA IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX\").collect()\nsession.sql(\"CREATE STAGE IF NOT EXISTS BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\").collect()\n\nregistry = Registry(\n    session=session, database_name=\"BD_AA_DEV\", schema_name=\"SC_MODELS_BMX\"\n)\n\nprint(\"‚úÖ Model Registry initialized\")\nprint(\"‚úÖ Stage for MMT models created\")\n",
      "id": "12d2a8a5-e824-44e8-a8b1-2973d079cf18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Load Best Hyperparameters Per Group\n",
      "id": "1162a0a0-0da8-424c-b4b7-a06c189a77f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìä LOADING BEST HYPERPARAMETERS PER GROUP\")\nprint(\"=\" * 80)\n\n# Try to load from ML Experiments first, fallback to table\nhyperparams_by_group = {}\nexperiments_loaded = False\n\n# Get all groups that need hyperparameters\nall_groups_from_data = session.sql(\n    \"\"\"\n    SELECT DISTINCT stats_ntile_group\n    FROM BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\n    WHERE stats_ntile_group IS NOT NULL\n    ORDER BY stats_ntile_group\n\"\"\"\n).collect()\n\nexpected_groups = [row[\"STATS_NTILE_GROUP\"] for row in all_groups_from_data]\n\n# Try loading from ML Experiments\nprint(\"\\nüî¨ Attempting to load from ML Experiments...\")\ntry:\n    exp_tracking = ExperimentTracking(session)\n\n    # Try to find the most recent experiment\n    # Note: This is a simplified approach - in production you might want to specify experiment name\n    from datetime import datetime, timedelta\n\n    today = datetime.now().strftime(\"%Y%m%d\")\n    experiment_name = f\"hyperparameter_search_regression_{today}\"\n\n    try:\n        exp_tracking.set_experiment(experiment_name)\n        print(f\"‚úÖ Found experiment: {experiment_name}\")\n\n        # Get all runs from this experiment\n        # Note: The exact API may vary - this is a conceptual approach\n        # You may need to query the experiments table directly\n        experiments_loaded = True\n        print(\"   ‚úÖ ML Experiments available - loading from experiments\")\n    except:\n        # Try yesterday's experiment as fallback\n        yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\n        experiment_name = f\"hyperparameter_search_regression_{yesterday}\"\n        try:\n            exp_tracking.set_experiment(experiment_name)\n            print(f\"‚úÖ Found experiment: {experiment_name}\")\n            experiments_loaded = True\n        except:\n            print(\"   ‚ö†Ô∏è  No recent experiment found, will use table fallback\")\n            experiments_loaded = False\n\n    if experiments_loaded:\n        # Use ExperimentTracking API methods via SQL SHOW commands\n        try:\n            print(f\"   üìã Listing runs from experiment: {experiment_name}\")\n\n            # Step 1: List all runs in the experiment using SHOW RUNS\n            runs_query = f\"SHOW RUNS IN EXPERIMENT {experiment_name}\"\n            runs_df = session.sql(runs_query)\n            runs_list = runs_df.collect()\n\n            if len(runs_list) == 0:\n                print(\"   ‚ö†Ô∏è  No runs found in experiment, using table fallback\")\n                experiments_loaded = False\n            else:\n                print(f\"   ‚úÖ Found {len(runs_list)} runs in experiment\")\n\n                # Step 2: For each run, get parameters and metrics\n                runs_by_group = {}  # group_name -> best run info\n\n                for run in runs_list:\n                    run_name = run[\"name\"]\n\n                    try:\n                        # Get parameters for this run\n                        params_query = f\"SHOW RUN PARAMETERS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        params_df = session.sql(params_query)\n                        params_list = params_df.collect()\n\n                        # Get metrics for this run\n                        metrics_query = f\"SHOW RUN METRICS IN EXPERIMENT {experiment_name} RUN {run_name}\"\n                        metrics_df = session.sql(metrics_query)\n                        metrics_list = metrics_df.collect()\n\n                        # Extract group_name and algorithm from parameters\n                        group_name = None\n                        search_id = None\n                        algorithm = None\n                        best_params = {}\n\n                        for param in params_list:\n                            param_name = param[\"name\"]\n                            param_value = param[\"value\"]\n\n                            if param_name == \"group_name\":\n                                group_name = param_value\n                            elif param_name == \"search_id\":\n                                search_id = param_value\n                            elif param_name == \"algorithm\":\n                                algorithm = param_value\n                            else:\n                                best_params[param_name] = param_value\n\n                        # Extract metrics\n                        val_rmse = None\n                        val_mae = None\n\n                        for metric in metrics_list:\n                            metric_name = metric[\"name\"]\n                            metric_value = metric[\"value\"]\n\n                            if metric_name == \"val_rmse\":\n                                val_rmse = float(metric_value)\n                            elif metric_name == \"val_mae\":\n                                val_mae = float(metric_value)\n\n                        # Only process runs that have a group_name\n                        if group_name and val_rmse is not None:\n                            alg = algorithm or GROUP_MODEL.get(\n                                group_name, _DEFAULT_MODEL\n                            )\n                            if group_name not in runs_by_group:\n                                runs_by_group[group_name] = {\n                                    \"run_name\": run_name,\n                                    \"params\": best_params,\n                                    \"val_rmse\": val_rmse,\n                                    \"val_mae\": val_mae,\n                                    \"search_id\": search_id,\n                                    \"algorithm\": alg,\n                                }\n                            else:\n                                if val_rmse < runs_by_group[group_name][\"val_rmse\"]:\n                                    runs_by_group[group_name] = {\n                                        \"run_name\": run_name,\n                                        \"params\": best_params,\n                                        \"val_rmse\": val_rmse,\n                                        \"val_mae\": val_mae,\n                                        \"search_id\": search_id,\n                                        \"algorithm\": alg,\n                                    }\n\n                    except Exception as run_error:\n                        print(\n                            f\"   ‚ö†Ô∏è  Error processing run {run_name}: {str(run_error)[:100]}\"\n                        )\n                        continue\n\n                # Step 3: Store results in hyperparams_by_group\n                if len(runs_by_group) > 0:\n                    print(f\"   ‚úÖ Loaded {len(runs_by_group)} groups from Experiments\")\n\n                    for group_name, run_info in runs_by_group.items():\n                        hyperparams_by_group[group_name] = {\n                            \"params\": run_info[\"params\"],\n                            \"val_rmse\": run_info[\"val_rmse\"],\n                            \"search_id\": run_info[\"search_id\"] or f\"exp_{group_name}\",\n                            \"algorithm\": run_info.get(\"algorithm\", _DEFAULT_MODEL),\n                        }\n\n                        print(f\"\\n   {group_name}:\")\n                        print(\n                            f\"      Algorithm: {run_info.get('algorithm', _DEFAULT_MODEL)}\"\n                        )\n                        print(f\"      Val RMSE: {run_info['val_rmse']:.4f}\")\n                        if run_info[\"val_mae\"]:\n                            print(f\"      Val MAE: {run_info['val_mae']:.4f}\")\n                        print(f\"      Search ID: {run_info['search_id'] or 'N/A'}\")\n                        print(\n                            f\"      Source: ML Experiments (run: {run_info['run_name']})\"\n                        )\n\n                    experiments_loaded = True\n                else:\n                    print(\n                        \"   ‚ö†Ô∏è  No valid runs with group_name found, using table fallback\"\n                    )\n                    experiments_loaded = False\n\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Error using ExperimentTracking API: {str(e)[:200]}\")\n            print(\"   Will use table fallback\")\n            experiments_loaded = False\n\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  ML Experiments not available: {str(e)[:200]}\")\n    print(\"   Will use table fallback\")\n    experiments_loaded = False\n",
      "id": "b7722ab2-9b8c-4860-89bc-ce89e341d3ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 2b. Fallback a tabla (si no hay Experiments o faltan grupos)\n",
      "id": "9de62859-297b-4000-89c3-9ac3954eeac0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# Fallback to table ONLY if Experiments didn't work or didn't have all groups\nif not experiments_loaded or len(hyperparams_by_group) < len(expected_groups):\n    print(\"\\nüìã Loading from table (HYPERPARAMETER_RESULTS) - Fallback mode...\")\n    print(\"   Note: Table is only used when ML Experiments is not available\")\n\n    # Check if table exists\n    table_exists = False\n    try:\n        check_table = session.sql(\n            \"\"\"\n            SELECT COUNT(*) as CNT \n            FROM INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = 'SC_MODELS_BMX' \n            AND TABLE_NAME = 'HYPERPARAMETER_RESULTS'\n            AND TABLE_CATALOG = 'BD_AA_DEV'\n            \"\"\"\n        ).collect()\n        table_exists = check_table[0][\"CNT\"] > 0\n    except:\n        table_exists = False\n\n    if table_exists:\n        # Get most recent hyperparameter search results per group from table\n        hyperparams_df = session.sql(\n            \"\"\"\n            WITH latest_searches AS (\n                SELECT \n                    group_name,\n                    search_id,\n                    algorithm,\n                    best_params,\n                    best_cv_rmse,\n                    val_rmse,\n                    val_mae,\n                    created_at,\n                    ROW_NUMBER() OVER (PARTITION BY group_name ORDER BY created_at DESC) AS rn\n                FROM BD_AA_DEV.SC_MODELS_BMX.HYPERPARAMETER_RESULTS\n                WHERE group_name IS NOT NULL\n            )\n            SELECT \n                group_name,\n                search_id,\n                best_params,\n                best_cv_rmse,\n                val_rmse,\n                val_mae\n            FROM latest_searches\n            WHERE rn = 1\n            ORDER BY group_name\n        \"\"\"\n        )\n\n        hyperparams_results = hyperparams_df.collect()\n\n        if len(hyperparams_results) > 0:\n            print(f\"   ‚úÖ Loaded {len(hyperparams_results)} groups from table\")\n\n            # Update or add to hyperparams_by_group\n            for result in hyperparams_results:\n                group_name = result[\"GROUP_NAME\"]\n                best_params_json = result[\"BEST_PARAMS\"]\n\n                # Parse hyperparameters\n                if isinstance(best_params_json, str):\n                    best_params = json.loads(best_params_json)\n                else:\n                    best_params = best_params_json\n\n                # Only add if not already loaded from Experiments\n                if group_name not in hyperparams_by_group:\n                    alg = result.get(\"ALGORITHM\") or GROUP_MODEL.get(\n                        group_name, _DEFAULT_MODEL\n                    )\n                    hyperparams_by_group[group_name] = {\n                        \"params\": best_params,\n                        \"val_rmse\": result[\"VAL_RMSE\"],\n                        \"search_id\": result[\"SEARCH_ID\"],\n                        \"algorithm\": alg,\n                    }\n\n                    print(f\"\\n   {group_name}:\")\n                    print(f\"      Algorithm: {alg}\")\n                    print(f\"      Val RMSE: {result['VAL_RMSE']:.4f}\")\n                    print(f\"      Search ID: {result['SEARCH_ID']}\")\n                    print(f\"      Source: Table (fallback)\")\n        else:\n            print(\"   ‚ö†Ô∏è  Table exists but has no results\")\n    else:\n        print(\"   ‚ö†Ô∏è  Table does not exist (this is OK if using ML Experiments)\")\n",
      "id": "b6677391-cfff-436f-8825-f28cfe1b632d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 2c. Defaults y validaci√≥n de hiperpar√°metros\n",
      "id": "70461dff-7dfd-455c-984f-ac3c43165125"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "if len(hyperparams_by_group) == 0:\n    raise ValueError(\n        \"No hyperparameter results found in Experiments or table! Please run 03_hyperparameter_search.py first\"\n    )\n\nprint(\n    f\"\\n‚úÖ Total loaded hyperparameters: {len(hyperparams_by_group)}/{len(expected_groups)} groups\"\n)\n\n# Default hyperparameters por clase Snowflake ML (grupos sin resultados de b√∫squeda)\nDEFAULT_PARAMS_BY_MODEL = {\n    \"XGBRegressor\": {\n        \"n_estimators\": 100,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"min_child_weight\": 1,\n        \"gamma\": 0,\n        \"reg_alpha\": 0,\n        \"reg_lambda\": 1,\n    },\n    \"LGBMRegressor\": {\n        \"n_estimators\": 100,\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"reg_alpha\": 0,\n        \"reg_lambda\": 1,\n        \"min_child_samples\": 20,\n    },\n    \"SGDRegressor\": {\n        \"alpha\": 0.0001,\n        \"max_iter\": 2000,\n        \"tol\": 1e-3,\n        \"eta0\": 0.01,\n    },\n}\n\nprint(\n    f\"\\nüìã Default hyperparameters (per Snowflake ML model, for groups without search results):\"\n)\nfor model_name, params in DEFAULT_PARAMS_BY_MODEL.items():\n    print(f\"   {model_name}: {list(params.keys())}\")\n\n# Validate that we have hyperparameters for all expected groups\nprint(f\"\\nüîç Validating hyperparameter coverage...\")\ngroups_with_hyperparams = set(hyperparams_by_group.keys())\ngroups_without_hyperparams = set(expected_groups) - groups_with_hyperparams\n\nif groups_without_hyperparams:\n    print(\n        f\"‚ö†Ô∏è  WARNING: {len(groups_without_hyperparams)} groups will use default hyperparameters:\"\n    )\n    for group in sorted(groups_without_hyperparams):\n        print(f\"      - {group}\")\nelse:\n    print(f\"‚úÖ All {len(expected_groups)} groups have optimized hyperparameters!\")\n",
      "id": "aaa395b6-5968-4d7c-95a7-e55d5345fbed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Preparar datos de entrenamiento (sin FeatureView)\n",
      "id": "b6956db6-0098-4521-9d3d-f1c6eb7c0f60"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üè™ LOADING TRAINING DATA (SIN FEATURE VIEW)\")\nprint(\"=\" * 80)\n\nif USE_CLEANED_TABLES:\n    print(\"üìä Loading from cleaned table: TRAIN_DATASET_CLEANED\")\n    training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n    print(f\"\\n‚úÖ Training data loaded from cleaned table\")\n    print(f\"   Total records: {training_df.count():,}\")\n    print(f\"   Columns: {len(training_df.columns)}\")\nelse:\n    # Preferimos la tabla materializada de features (sin Dynamic Tables).\n    # Si falla por permisos/no existencia, hacemos fallback a la tabla limpia.\n    try:\n        # Mantener inicializaci√≥n del Feature Store (aunque no usemos FeatureView)\n        _fs = FeatureStore(\n            session=session,\n            database=\"BD_AA_DEV\",\n            name=\"SC_FEATURES_BMX\",\n            default_warehouse=\"WH_AA_DEV_DS_SQL\",\n        )\n        print(\"‚úÖ Feature Store inicializado (sin FeatureView)\")\n\n        print(f\"üìä Loading features from table: {FEATURES_TABLE}\")\n        features_df = session.table(FEATURES_TABLE)\n\n        print(\"‚è≥ Loading target variable and stats_ntile_group from training table...\")\n        target_df = session.table(\n            \"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\"\n        ).select(\n            \"customer_id\", \"brand_pres_ret\", \"week\", \"uni_box_week\", \"stats_ntile_group\"\n        )\n\n        print(\"‚è≥ Joining features with target...\")\n        training_df = features_df.join(\n            target_df, on=[\"customer_id\", \"brand_pres_ret\", \"week\"], how=\"inner\"\n        )\n\n        print(f\"\\n‚úÖ Training data loaded from features table + target\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n    except Exception as e:\n        print(\n            f\"‚ö†Ô∏è  Could not load/join features table ({FEATURES_TABLE}): {str(e)[:200]}\"\n        )\n        print(\"   Falling back to TRAIN_DATASET_CLEANED\")\n        training_df = session.table(\"BD_AA_DEV.SC_STORAGE_BMX_PS.TRAIN_DATASET_CLEANED\")\n        print(f\"\\n‚úÖ Training data loaded from cleaned table (fallback)\")\n        print(f\"   Total records: {training_df.count():,}\")\n        print(f\"   Columns: {len(training_df.columns)}\")\n\n# Resolver nombre real de la columna de partici√≥n (Snowflake suele devolver MAY√öSCULAS)\n# DPFOrchestrator/MMT exige que el nombre de la columna coincida exactamente con partition_by.\nPARTITION_COL = next(\n    (c for c in training_df.columns if c.upper() == \"STATS_NTILE_GROUP\"),\n    \"stats_ntile_group\",\n)\n# Normalizar a \"stats_ntile_group\" para evitar KeyError en el orquestador (pandas busca el nombre literal)\nif PARTITION_COL != \"stats_ntile_group\":\n    training_df = training_df.with_column_renamed(PARTITION_COL, \"stats_ntile_group\")\n    PARTITION_COL = \"stats_ntile_group\"\n    print(f\"\\nüìå Columna de partici√≥n renombrada a: '{PARTITION_COL}'\")\nelse:\n    print(f\"\\nüìå Columna de partici√≥n usada: '{PARTITION_COL}'\")\n\n# Show distribution by group\nprint(\"\\nüìä Records per Group:\")\ngroup_counts = (\n    training_df.group_by(PARTITION_COL).count().sort(PARTITION_COL)\n)\ngroup_counts.show()\n",
      "id": "195fb9f1-ed94-46da-b71d-98db721cfd5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Define Training Function for MMT\n",
      "id": "5ae98cfe-8e2c-4909-8250-fb2fd2d4bfed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîß DEFINING TRAINING FUNCTION\")\nprint(\"=\" * 80)\n\n\ndef _get_target_column(df):\n    \"\"\"Return the target column name in df (case-insensitive match for uni_box_week).\"\"\"\n    for c in df.columns:\n        if str(c).upper() == \"UNI_BOX_WEEK\":\n            return c\n    return \"uni_box_week\"\n\n\ndef train_segment_model(data_connector, context):\n    \"\"\"\n    Train regression model for uni_box_week for a specific group.\n    Model type per group: LGBMRegressor, XGBRegressor, or SGDRegressor (from script 03 / GROUP_MODEL).\n\n    Args:\n        data_connector: Snowflake data connector (provided by MMT)\n        context: Contains partition_id (stats_ntile_group name)\n\n    Returns:\n        Trained model (XGBRegressor, LGBMRegressor, or SGDRegressor)\n    \"\"\"\n    from snowflake.ml.modeling.xgboost import XGBRegressor\n    from snowflake.ml.modeling.lightgbm import LGBMRegressor\n    from snowflake.ml.modeling.linear_model import SGDRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    import numpy as np\n\n    segment_name = context.partition_id\n    print(f\"\\n{'='*80}\")\n    print(f\"üöÄ Training model for {segment_name}\")\n    print(f\"{'='*80}\")\n\n    df = data_connector.to_pandas()\n    print(f\"üìä Data shape: {df.shape}\")\n\n    # Excluir columna de partici√≥n por nombre real (p. ej. STATS_NTILE_GROUP en Snowflake)\n    partition_col_in_df = next(\n        (c for c in df.columns if c.upper() == \"STATS_NTILE_GROUP\"), \"stats_ntile_group\"\n    )\n    excluded_cols = [\n        \"customer_id\",\n        \"brand_pres_ret\",\n        \"week\",\n        \"FEATURE_TIMESTAMP\",\n        partition_col_in_df,\n    ]\n    target_col = _get_target_column(df)\n    feature_cols = [\n        col for col in df.columns if col not in excluded_cols + [target_col]\n    ]\n    X = df[feature_cols].fillna(0)\n    y = df[target_col].fillna(0)\n\n    print(f\"   Features: {len(feature_cols)}\")\n    print(f\"   Target range: [{y.min():.2f}, {y.max():.2f}]\")\n    print(f\"   Target mean: {y.mean():.2f}\")\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    print(f\"   Training set: {X_train.shape[0]:,} samples\")\n    print(f\"   Test set: {X_test.shape[0]:,} samples\")\n\n    # Modelo para este grupo: nombre clase Snowflake ML (desde hyperparams o GROUP_MODEL)\n    model_type = GROUP_MODEL.get(segment_name, _DEFAULT_MODEL)\n    if segment_name in hyperparams_by_group:\n        algorithm = hyperparams_by_group[segment_name].get(\"algorithm\")\n        if algorithm:\n            model_type = algorithm\n        group_params = hyperparams_by_group[segment_name][\"params\"]\n        search_id = hyperparams_by_group[segment_name][\"search_id\"]\n        val_rmse = hyperparams_by_group[segment_name][\"val_rmse\"]\n        print(f\"\\n   ‚úÖ Using OPTIMIZED hyperparameters from script 03\")\n        print(f\"      Model: {model_type}\")\n        print(f\"      Search ID: {search_id}\")\n        print(f\"      Validation RMSE (from search): {val_rmse:.4f}\")\n    else:\n        group_params = DEFAULT_PARAMS_BY_MODEL.get(\n            model_type, DEFAULT_PARAMS_BY_MODEL[\"XGBRegressor\"]\n        )\n        print(\n            f\"\\n   ‚ö†Ô∏è  Using DEFAULT hyperparameters for {model_type} (no search results for {segment_name})\"\n        )\n\n    # Convert params to proper types\n    model_params = {}\n    for k, v in group_params.items():\n        if isinstance(v, (int, float)):\n            model_params[k] = v\n        elif isinstance(v, (np.integer, np.floating)):\n            model_params[k] = float(v) if isinstance(v, np.floating) else int(v)\n        else:\n            model_params[k] = v\n    model_params[\"random_state\"] = 42\n\n    MODEL_CLASSES = {\n        \"XGBRegressor\": XGBRegressor,\n        \"LGBMRegressor\": LGBMRegressor,\n        \"SGDRegressor\": SGDRegressor,\n    }\n    ModelClass = MODEL_CLASSES.get(model_type, XGBRegressor)\n    if model_type == \"XGBRegressor\":\n        model_params[\"n_jobs\"] = -1\n        model_params[\"objective\"] = \"reg:squarederror\"\n        model_params[\"eval_metric\"] = \"rmse\"\n    elif model_type == \"LGBMRegressor\":\n        model_params[\"n_jobs\"] = -1\n        model_params[\"verbosity\"] = -1\n    elif model_type == \"SGDRegressor\":\n        model_params.setdefault(\"penalty\", \"l2\")\n        model_params.setdefault(\"learning_rate\", \"invscaling\")\n\n    print(f\"\\n   Training {model_type} with {len(model_params)} hyperparameters...\")\n    model = ModelClass(**model_params)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test, y_pred)\n\n    print(f\"\\n   ‚úÖ Model trained\")\n    print(f\"      RMSE: {rmse:.2f}\")\n    print(f\"      MAE: {mae:.2f}\")\n    print(f\"{'='*80}\\n\")\n\n    model.rmse = rmse\n    model.mae = mae\n    model.training_samples = X_train.shape[0]\n    model.test_samples = X_test.shape[0]\n    model.feature_cols = feature_cols\n    model.hyperparameters = model_params\n    model.segment = segment_name\n    model.group_name = segment_name\n\n    return model\n\n\nprint(\"‚úÖ Training function defined\")\n",
      "id": "87a03a14-bc81-4762-83b8-91c5ca60799e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Execute Many Model Training (MMT) - 16 Models\n",
      "id": "4804f5a6-b078-470e-b70c-18f7a6cf2d16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìà SCALING CLUSTER UP FOR MMT\")\nprint(\"=\" * 80)\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n\n    print(\"‚è≥ Aumentando cluster a 4 contenedores...\")\n    scale_cluster(\n        expected_cluster_size=4,\n        options={\n            \"block_until_min_cluster_size\": 2  # Retorna cuando al menos 2 nodos est√©n listos\n        }\n    )\n    print(\"‚úÖ Cluster aumentado a 4 contenedores\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Error al escalar cluster: {str(e)[:200]}\")\n    print(\"   Continuando con el cluster actual...\")\n",
      "id": "15f749f3-2f67-410a-9dc6-325397584334",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5b. (Opcional) Enlace al Ray Dashboard\n#\nCopia la URL en tu navegador para monitorear el cluster durante el MMT.\n",
      "id": "ead337f1-d4ff-4d3b-bcc7-5d163a322a4f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "try:\n    from snowflake.ml.runtime_cluster import get_ray_dashboard_url\n\n    dashboard_url = get_ray_dashboard_url()\n    print(f\"‚úÖ Ray Dashboard: {dashboard_url}\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è No se pudo obtener la URL del Ray Dashboard.\")\n    print(f\"   Detalle: {str(e)[:200]}\")\n",
      "id": "61effa0f-9d0d-41ec-907c-a8c97a20ff51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5c. Ejecutar MMT y monitorear\n",
      "id": "4669caa8-cb6a-484b-b6e3-a7f53c1fe785"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üöÄ STARTING MANY MODEL TRAINING (MMT) - 16 MODELS\")\nprint(\"=\" * 80)\nprint(\n    \"\\nTraining 16 models in PARALLEL (LGBMRegressor, XGBRegressor, SGDRegressor per group)\"\n)\nprint(\"Each model uses group-specific hyperparameters\\n\")\n\nstart_time = time.time()\n\n# Create MMT trainer\ntrainer = ManyModelTraining(train_segment_model, \"BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS\")\n\n# Execute training with partition_by stats_ntile_group\ntraining_run = trainer.run(\n    partition_by=\"stats_ntile_group\",  # ‚Üê KEY: Partition by group\n    snowpark_dataframe=training_df,\n    run_id=f\"uni_box_regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n)\n\nprint(f\"\\n‚úÖ Training started. Run ID: {training_run.run_id}\")\nprint(\"   Monitorea en Ray Dashboard (celda 5b) o ejecuta la celda 5d cuando quieras esperar.\")\nprint(\"   Para no bloquear: ejecuta solo hasta aqu√≠ y m√°s tarde vuelve a ejecutar la celda 5d.\\n\")\n",
      "id": "bec2f75e-7118-450c-ac86-2dfeee4109a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5d. (Opcional) Esperar a que termine el MMT\n#\n**No hace falta esperar aqu√≠.** Puedes cerrar/continuar y volver m√°s tarde.\n- Monitorea en **Ray Dashboard** (celda 5b).\n- Cuando quieras bloquear hasta que termine: ejecuta esta celda.\n- Si pones `MMT_MAX_WAIT = 7200` (2 h) y ejecutas esta celda, esperar√° hasta 2 h o hasta que termine.\n- Si se cumple el timeout sin terminar, vuelve a ejecutar esta celda (con mismo o mayor `MMT_MAX_WAIT`).\n",
      "id": "b88deda0-0a58-4372-ad03-aab708e7043f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import time as time_module\n\n# Ajusta seg√∫n quieras: tiempo m√°ximo que esta celda esperar√° (segundos)\n# 600 = 10 min (solo para ver algo de progreso); 7200 = 2 h (esperar hasta el final)\nMMT_MAX_WAIT = 600\nMMT_CHECK_INTERVAL = 30  # Log cada 30 s (no cada 10 s)\n\nelapsed = 0\ncompleted = False\nrun_start = start_time\n\nwhile elapsed < MMT_MAX_WAIT:\n    time_module.sleep(MMT_CHECK_INTERVAL)\n    elapsed += MMT_CHECK_INTERVAL\n\n    try:\n        done_count = 0\n        total_count = 0\n        for partition_id in training_run.partition_details:\n            total_count += 1\n            status = training_run.partition_details[partition_id].status\n            if status.name == \"DONE\" or status.name == \"FAILED\":\n                done_count += 1\n\n        print(\n            f\"‚è±Ô∏è  {elapsed}s - Progress: {done_count}/{total_count} models completed\",\n            end=\"\\r\",\n        )\n\n        if done_count == total_count:\n            print(\"\\n‚úÖ All models completed!\" + \" \" * 50)\n            completed = True\n            break\n    except Exception:\n        print(f\"‚è±Ô∏è  {elapsed}s - Waiting for status update...\", end=\"\\r\")\n\nif not completed:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"‚è±Ô∏è  Timeout de espera alcanzado (training puede seguir en segundo plano).\")\n    print(\"   ‚Üí Revisa el Ray Dashboard (celda 5b) para ver progreso.\")\n    print(\"   ‚Üí Para esperar hasta el final: pon MMT_MAX_WAIT = 7200 (o m√°s) y vuelve a ejecutar esta celda.\")\n    print(\"   ‚Üí Cuando todo haya terminado, ejecuta las celdas 6 y 7 (resultados y registro).\")\n    print(\"=\" * 80)\n    # Verificaci√≥n r√°pida por stage\n    try:\n        stage_files = session.sql(\n            f\"LIST @BD_AA_DEV.SC_MODELS_BMX.MMT_MODELS PATTERN='.*{training_run.run_id}.*'\"\n        ).collect()\n        if len(stage_files) >= 16:\n            print(f\"\\n‚úÖ Hay {len(stage_files)} archivos en stage - training probablemente completado.\")\n            completed = True\n    except Exception:\n        pass\nelse:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"‚úÖ TRAINING COMPLETE!\")\n    print(\"=\" * 80)\n\nend_time = time.time()\nelapsed_minutes = (end_time - run_start) / 60\nprint(f\"\\n‚è±Ô∏è  Tiempo en esta celda: {elapsed_minutes:.2f} min\")\n",
      "id": "49829626-9930-4bed-81e9-8b4372647010",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Review Training Results\n",
      "id": "4cc19cc2-1a00-455e-86dc-37e805452087"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüìä Training Results:\\n\")\n\nfor partition_id in training_run.partition_details:\n    details = training_run.partition_details[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            print(f\"\\n‚úÖ {partition_id if partition_id else 'DEFAULT'}:\")\n            print(f\"   RMSE: {model.rmse:.2f}\")\n            print(f\"   MAE: {model.mae:.2f}\")\n            print(f\"   Training samples: {model.training_samples:,}\")\n            print(f\"   Test samples: {model.test_samples:,}\")\n        except Exception as e:\n            print(f\"\\n‚ö†Ô∏è  {partition_id}: Could not load model - {str(e)[:100]}\")\n    else:\n        print(f\"\\n‚ùå {partition_id}: Training failed\")\n        print(f\"   Status: {details.status}\")\n",
      "id": "6656c183-39f3-4388-9b82-75590e104388",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Register Model in Model Registry\n",
      "id": "870e7a86-e69f-45ce-8752-b438749fd741"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìù REGISTERING MODEL IN MODEL REGISTRY\")\nprint(\"=\" * 80)\n\nversion_date = datetime.now().strftime(\"%Y%m%d_%H%M\")\nregistered_models = {}\n\n# %% Registrar cada modelo por grupo\nfor partition_id in training_run.partition_details:\n    details = training_run.partition_details[partition_id]\n\n    if details.status.name == \"DONE\":\n        try:\n            model = training_run.get_model(partition_id)\n\n            # Model name includes group identifier\n            model_name = f\"uni_box_regression_{partition_id.lower()}\"\n\n            # Get group-specific search ID, algorithm, and hyperparameters if available\n            group_search_id = None\n            group_hyperparams = None\n            group_algorithm = GROUP_MODEL.get(partition_id, _DEFAULT_MODEL)\n            if partition_id in hyperparams_by_group:\n                group_search_id = hyperparams_by_group[partition_id][\"search_id\"]\n                group_hyperparams = hyperparams_by_group[partition_id][\"params\"]\n                alg = hyperparams_by_group[partition_id].get(\"algorithm\")\n                if alg:\n                    group_algorithm = alg\n\n            # Prepare sample input from this group\n            sample_input = (\n                training_df.filter(training_df[PARTITION_COL] == partition_id)\n                .select(model.feature_cols)\n                .limit(5)\n            )\n\n            print(f\"\\nRegistering {partition_id}...\")\n\n            # Prepare metrics including hyperparameters\n            model_metrics = {\n                \"rmse\": float(model.rmse),\n                \"mae\": float(model.mae),\n                \"training_samples\": int(model.training_samples),\n                \"test_samples\": int(model.test_samples),\n                \"algorithm\": group_algorithm,\n                \"group\": partition_id,\n                \"hyperparameter_search_id\": group_search_id or \"default\",\n            }\n\n            # Add hyperparameters to metrics (as nested dict)\n            if group_hyperparams:\n                # Convert hyperparameters to a format suitable for metrics\n                for key, value in group_hyperparams.items():\n                    if isinstance(value, (int, float)):\n                        model_metrics[f\"hyperparameter_{key}\"] = (\n                            float(value) if isinstance(value, float) else int(value)\n                        )\n                model_metrics[\"hyperparameters\"] = json.dumps(\n                    {\n                        k: float(v) if isinstance(v, (int, float)) else v\n                        for k, v in group_hyperparams.items()\n                    }\n                )\n\n            mv = registry.log_model(\n                model,\n                model_name=model_name,\n                version_name=f\"v_{version_date}\",\n                comment=f\"{group_algorithm} regression model for uni_box_week - Group: {partition_id}\",\n                metrics=model_metrics,\n                sample_input_data=sample_input,\n                task=task.Task.TABULAR_REGRESSION,\n            )\n\n            registered_models[partition_id] = {\n                \"model_name\": model_name,\n                \"version\": f\"v_{version_date}\",\n                \"model_version\": mv,\n            }\n\n            print(f\"‚úÖ {partition_id}: {model_name} v_{version_date}\")\n            print(f\"   RMSE: {model.rmse:.2f}, MAE: {model.mae:.2f}\")\n\n        except Exception as e:\n            print(f\"‚ùå Error registering model: {str(e)[:200]}\")\n\nprint(f\"\\n‚úÖ {len(registered_models)} model(s) registered successfully!\")\n",
      "id": "fdaed237-8355-468b-bd89-9364a1827894",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Set Production Alias\n",
      "id": "6653d934-387d-4284-b8db-1105843e612f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nüè∑Ô∏è  Setting PRODUCTION aliases...\\n\")\n\nfor partition_id, model_info in registered_models.items():\n    model_name = model_info[\"model_name\"]\n    version = model_info[\"version\"]\n    model_version = model_info[\"model_version\"]\n\n    try:\n        # Remove existing PRODUCTION alias\n        try:\n            model_ref = registry.get_model(model_name)\n            model_ref.default.unset_alias(\"PRODUCTION\")\n        except:\n            pass\n\n        # Set alias on new version\n        model_version.set_alias(\"PRODUCTION\")\n        print(f\"‚úÖ {model_name}: PRODUCTION ‚Üí {version}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  {model_name}: Error setting alias - {str(e)[:100]}\")\n\nprint(\"\\n‚úÖ All production aliases configured!\")\n",
      "id": "2e7589e6-2ec4-49b0-a69b-f8adddaba3ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8b. Scale cluster down\n",
      "id": "61f95e74-4ae9-4af5-b507-a8ff150bc3ac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìâ SCALING CLUSTER DOWN\")\nprint(\"=\" * 80)\n\ntry:\n    from snowflake.ml.runtime_cluster import scale_cluster\n\n    print(\"‚è≥ Reduciendo cluster a 1 contenedor...\")\n    scale_cluster(\n        expected_cluster_size=1\n    )\n    print(\"‚úÖ Cluster reducido a 1 contenedor\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Error al reducir cluster: {str(e)[:200]}\")\n    print(\"   El cluster puede seguir escalado...\")\n",
      "id": "276b5e6f-72f3-4356-8732-bd9ef5619273",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Summary\n",
      "id": "0f08a538-891c-4197-b144-c197d694d5ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üéâ MANY MODEL TRAINING (MMT) COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìä Summary:\")\nprint(f\"   ‚úÖ Models trained: {len(registered_models)}/16\")\nprint(f\"   ‚è±Ô∏è  Training time: {elapsed_minutes:.2f} minutes\")\nprint(f\"   üîß Algorithms: LGBM / XGB / SGD (per group)\")\nprint(f\"   üìà Hyperparameters: Group-specific (from hyperparameter search)\")\n\nif registered_models:\n    print(f\"\\nüèÜ Model Performance by Group:\")\n    for partition_id in sorted(registered_models.keys()):\n        model = training_run.get_model(partition_id)\n        print(\n            f\"   {partition_id}: RMSE={model.rmse:.2f}, MAE={model.mae:.2f}, Samples={model.training_samples:,}\"\n        )\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review model performance by group\")\nprint(\n    \"   2. Run 05_create_partitioned_model.py to create partitioned model (combines all 16)\"\n)\nprint(\n    \"   3. Run 06_partitioned_inference_batch.py for batch inference with automatic routing\"\n)\n\nprint(\"\\n\" + \"=\" * 80)\n\n",
      "id": "601fe49b-46f9-4e9f-86ad-192b1a03d7db",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}