{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Partitioned Model (16 models from 04)\nLoads group-specific models from Registry, creates CustomModel with partitioned API and registers it.\n",
      "id": "33132b79-91d0-484b-9df8-ec9c74e58ee2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import custom_model, task\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nsession = get_active_session()\n\n# Configuration: Database, schemas, and tables\nDATABASE = \"BD_AA_DEV\"\nSTORAGE_SCHEMA = \"SC_STORAGE_BMX_PS\"\nMODELS_SCHEMA = \"SC_MODELS_BMX\"\nTRAIN_TABLE_CLEANED = f\"{DATABASE}.{STORAGE_SCHEMA}.TRAIN_DATASET_CLEANED\"\nPARTITIONED_MODEL_NAME = \"UNI_BOX_REGRESSION_PARTITIONED\"\n\n# Column constants\nSTATS_NTILE_GROUP_COL = \"STATS_NTILE_GROUP\"\n\n# Excluded columns (metadata columns, not features) - defined once at the beginning\nEXCLUDED_COLS = [\n    \"CUSTOMER_ID\",\n    \"BRAND_PRES_RET\",\n    \"PROD_KEY\",\n    \"WEEK\",\n    \"FEATURE_TIMESTAMP\",\n    STATS_NTILE_GROUP_COL,\n]\n\nsession.sql(f\"USE DATABASE {DATABASE}\").collect()\nsession.sql(f\"USE SCHEMA {STORAGE_SCHEMA}\").collect()\nregistry = Registry(session=session, database_name=DATABASE, schema_name=MODELS_SCHEMA)\nprint(f\"‚úÖ {session.get_current_database()}.{session.get_current_schema()}\")\n",
      "id": "593ecac6-5b41-4ed2-bd1e-7f251bc095a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Load the 16 Models (PRODUCTION)\n",
      "id": "2b802609-df58-4e1c-9e24-94b144b3bfcc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\ngroups_df = session.sql(\n    f\"\"\"\n    SELECT DISTINCT {STATS_NTILE_GROUP_COL} AS GROUP_NAME\n    FROM {TRAIN_TABLE_CLEANED}\n    WHERE {STATS_NTILE_GROUP_COL} IS NOT NULL\n    ORDER BY {STATS_NTILE_GROUP_COL}\n\"\"\"\n)\ngroups_list = [row[\"GROUP_NAME\"] for row in groups_df.collect()]\nprint(f\"\\nüìä Groups: {len(groups_list)}\")\n\nloaded_models = {}\nfeature_cols = None\n\nfor group_name in groups_list:\n    model_name = f\"uni_box_regression_{group_name.lower()}\"\n    try:\n        model_ref = registry.get_model(model_name)\n        model_version = model_ref.version(\"PRODUCTION\")\n        native_model = model_version.load()\n        if feature_cols is None:\n            feature_cols = getattr(native_model, \"feature_cols\", None)\n            if not feature_cols:\n                raise ValueError(\"Model has no feature_cols; run 04 first.\")\n        loaded_models[group_name] = {\"model\": native_model, \"model_version\": model_version, \"model_name\": model_name}\n        ver_name = getattr(model_version, \"name\", str(model_version))\n        print(f\"‚úÖ {group_name} (version: {ver_name})\")\n    except Exception as e:\n        print(f\"‚ùå {group_name}: {str(e)[:100]}\")\n\nif not loaded_models:\n    raise ValueError(\"No models loaded. Run 04_many_model_training.py first.\")\nif len(loaded_models) < len(groups_list):\n    print(f\"‚ö†Ô∏è  {len(loaded_models)}/{len(groups_list)} models loaded\")\n# Collect PRODUCTION versions in a readable way (library versions may expose different attributes)\nversions_loaded = {\n    getattr(m[\"model_version\"], \"version\", getattr(m[\"model_version\"], \"version_name\", str(m[\"model_version\"])))\n    for m in loaded_models.values()\n}\nprint(f\"\\n‚úÖ {len(loaded_models)} models, {len(feature_cols)} features (alias PRODUCTION ‚Üí {versions_loaded or 'N/A'})\")\n",
      "id": "4fd0267b-166e-4a3e-8473-265360287ad2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Define Partitioned Model Class\n",
      "id": "c15d2c33-b21c-41fe-80f9-2d0443283eb8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîß DEFINING PARTITIONED MODEL CLASS\")\nprint(\"=\" * 80)\n\n\nclass PartitionedUniBoxModel(custom_model.CustomModel):\n    \"\"\"Partitioned model: routes predictions by STATS_NTILE_GROUP.\"\"\"\n\n    def __init__(self, model_context):\n        super().__init__(model_context)\n        # Use the same feature columns detected when loading base models\n        self.feature_cols = feature_cols\n\n    @custom_model.partitioned_api\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Prediction per group; routes according to STATS_NTILE_GROUP.\"\"\"\n        if len(input_df) == 0:\n            return pd.DataFrame(\n                columns=[\n                    \"CUSTOMER_ID\",\n                    \"STATS_NTILE_GROUP\",\n                    \"WEEK\",\n                    \"BRAND_PRES_RET\",\n                    \"PROD_KEY\",\n                    \"predicted_uni_box_week\",\n                ]\n            )\n\n        # Single standard: UPPERCASE (avoids case-insensitive duplicates in signature)\n        part_col = STATS_NTILE_GROUP_COL\n        if part_col not in input_df.columns:\n            raise ValueError(f\"Missing required column '{part_col}' in input_df. Columns: {list(input_df.columns)}\")\n        group_name = input_df[part_col].iloc[0]\n\n        model_key = group_name.lower()\n        try:\n            model = self.context.model_ref(model_key)\n        except Exception:\n            try:\n                model = self.context.model_ref(group_name.lower())\n            except Exception:\n                raise ValueError(f\"Model not found for group: {group_name}. Keys: {list(self.context.models.keys())}\")\n\n        # Select only the known feature columns\n        X = input_df[self.feature_cols].fillna(0).astype(np.float64, errors=\"ignore\")\n        pred_out = model.predict(X)\n\n        if isinstance(pred_out, pd.DataFrame):\n            pred_col = next((c for c in pred_out.columns if \"PREDICT\" in c.upper() or \"OUTPUT\" in c.upper()), pred_out.columns[0])\n            predictions = np.asarray(pred_out[pred_col], dtype=np.float64).ravel()\n        elif hasattr(pred_out, \"flatten\"):\n            predictions = np.asarray(pred_out).flatten()\n        else:\n            predictions = np.asarray(pred_out).ravel()\n\n        # Propagate context columns to avoid ambiguous JOINs in inference\n        required_ctx = [\"CUSTOMER_ID\", \"WEEK\", \"BRAND_PRES_RET\", \"PROD_KEY\"]\n        missing_ctx = [c for c in required_ctx if c not in input_df.columns]\n        if missing_ctx:\n            raise ValueError(f\"Missing required context columns: {missing_ctx}. Columns: {list(input_df.columns)}\")\n\n        cust = input_df[\"CUSTOMER_ID\"].values\n        week_vals = input_df[\"WEEK\"].values\n        brand_vals = input_df[\"BRAND_PRES_RET\"].values\n        prod_key_vals = input_df[\"PROD_KEY\"].values\n\n        return pd.DataFrame(\n            {\n                \"CUSTOMER_ID\": cust,\n                STATS_NTILE_GROUP_COL: group_name,\n                \"WEEK\": week_vals,\n                \"BRAND_PRES_RET\": brand_vals,\n                \"PROD_KEY\": prod_key_vals,\n                \"predicted_uni_box_week\": predictions,\n            }\n        )\n\n\nprint(\"‚úÖ PartitionedUniBoxModel class defined\")\n",
      "id": "5ff68cab-093f-41cb-a885-88dc8e682ff5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Create Model Context and Partitioned Model\n",
      "id": "5891a7c7-ab8f-4ab2-82d1-eb30214a1e88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üì¶ CREATING PARTITIONED MODEL\")\nprint(\"=\" * 80)\n\nmodels_dict = {}\nfor group_name, model_info in loaded_models.items():\n    model_key = group_name.lower()\n    models_dict[model_key] = model_info[\"model\"]\n    print(f\"   Added {group_name} ‚Üí {model_key}\")\nprint(f\"\\n‚úÖ ModelContext created with {len(models_dict)} models\")\n\nmodel_context = custom_model.ModelContext(models=models_dict)\npartitioned_model = PartitionedUniBoxModel(model_context=model_context)\nprint(\"‚úÖ Partitioned model created\")\n",
      "id": "6a4abdf5-1a29-4752-965f-f5c3fbe9efc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Sample Input (aligned with 06: includes week and brand_pres_ret as context)\n",
      "id": "2afd8ca6-ed7e-4c57-86f8-5a41d219397e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìã PREPARING SAMPLE INPUT (ALIGNED WITH 06)\")\nprint(\"=\" * 80)\n\n# Use the same exclusion list as 02/04/06 to ensure sample_input has the same signature as inference\n# EXCLUDED_COLS is already in UPPER CASE\nexcluded_cols_with_target = EXCLUDED_COLS + [\"UNI_BOX_WEEK\"]\nexcluded_upper_sample = {col for col in excluded_cols_with_target}\n\n# Get training schema to identify numeric features (same as 06 with inference)\ntraining_schema = session.sql(f\"DESCRIBE TABLE {TRAIN_TABLE_CLEANED}\").collect()\ncol_type_dict_sample = {row[\"name\"].upper(): str(row[\"type\"]).upper() for row in training_schema}\nall_cols_sample = [row[\"name\"] for row in training_schema]\n\nNUMERIC_PREFIXES_SAMPLE = (\"FLOAT\", \"NUMBER\", \"INTEGER\", \"BIGINT\", \"DOUBLE\")\nfeature_cols_sample = [\n    c for c in all_cols_sample\n    if c.upper() not in excluded_upper_sample\n    and (col_type_dict_sample.get(c.upper()) or \"\").startswith(NUMERIC_PREFIXES_SAMPLE)\n]\n\n# Verify that feature_cols_sample matches feature_cols from models (should be equal)\nif set(c.upper() for c in feature_cols_sample) != set(c.upper() for c in feature_cols):\n    print(f\"‚ö†Ô∏è  Feature mismatch: sample has {len(feature_cols_sample)}, models have {len(feature_cols)}\")\n    print(f\"   Using feature_cols from models to maintain compatibility\")\n    feature_cols_for_sample = feature_cols\nelse:\n    # Sort feature_cols_sample according to feature_cols order to maintain consistency\n    feature_cols_for_sample = [c for c in feature_cols if c.upper() in {x.upper() for x in feature_cols_sample}]\n    if len(feature_cols_for_sample) != len(feature_cols):\n        print(f\"‚ö†Ô∏è  Some model features are not in sample, using all model features\")\n        feature_cols_for_sample = feature_cols\n\ntraining_df = session.table(TRAIN_TABLE_CLEANED)\n\n# Detect actual names (case-insensitive) to avoid NULL columns due to casing (WEEK vs week)\ncust_col = next((c for c in training_df.columns if c.upper() == \"CUSTOMER_ID\"), \"CUSTOMER_ID\")\npart_col = next((c for c in training_df.columns if c.upper() == STATS_NTILE_GROUP_COL), STATS_NTILE_GROUP_COL)\nweek_col = next((c for c in training_df.columns if c.upper() == \"WEEK\"), \"WEEK\")\nbrand_col = next((c for c in training_df.columns if c.upper() == \"BRAND_PRES_RET\"), \"BRAND_PRES_RET\")\nprod_key_col = next((c for c in training_df.columns if c.upper() == \"PROD_KEY\"), \"PROD_KEY\")\n\n# Select and alias context columns with fixed names for model signature\nbase_selected = training_df.select(\n    # Single standard: UPPERCASE\n    F.col(cust_col).alias(\"CUSTOMER_ID\"),\n    F.col(part_col).alias(STATS_NTILE_GROUP_COL),\n    # Use UPPERCASE to avoid case-insensitive collisions in signature inference\n    F.col(week_col).alias(\"WEEK\"),\n    F.col(brand_col).alias(\"BRAND_PRES_RET\"),\n    F.col(prod_key_col).alias(\"PROD_KEY\"),\n    *[F.col(c) for c in feature_cols_for_sample],\n)\n\nsample_input_sp = (\n    base_selected.filter(F.col(STATS_NTILE_GROUP_COL).isin(list(loaded_models.keys())))\n    .group_by(STATS_NTILE_GROUP_COL)\n    .agg(\n        # Ensure NO-NULL in context columns to infer signature (Snowflake ML requires at least one non-null)\n        F.min(F.col(\"CUSTOMER_ID\")).alias(\"CUSTOMER_ID\"),\n        F.coalesce(F.min(F.col(\"WEEK\")), F.lit(\"000000\")).alias(\"WEEK\"),\n        F.coalesce(F.min(F.col(\"BRAND_PRES_RET\")), F.lit(\"UNKNOWN\")).alias(\"BRAND_PRES_RET\"),\n        F.coalesce(F.min(F.col(\"PROD_KEY\")), F.lit(\"UNKNOWN\")).alias(\"PROD_KEY\"),\n        *[F.min(F.col(c)).alias(c) for c in feature_cols_for_sample],\n    )\n    .select(\"CUSTOMER_ID\", STATS_NTILE_GROUP_COL, \"WEEK\", \"BRAND_PRES_RET\", \"PROD_KEY\", *feature_cols_for_sample)\n    .limit(min(16, len(loaded_models)))\n)\n# Count first to verify we have data (using Snowpark, no pandas conversion yet)\nsample_count = sample_input_sp.count()\nif sample_count == 0:\n    raise ValueError(f\"Sample input is empty. Verify that TRAIN_DATASET_CLEANED has the same {STATS_NTILE_GROUP_COL} as loaded models.\")\n\n# Only convert to pandas when necessary for model registration (small sample, minimal memory)\nsample_input = sample_input_sp.to_pandas()\nprint(f\"‚úÖ Sample input prepared: {len(sample_input)} rows (one per group)\")\nprint(f\"   Columns: CUSTOMER_ID, {STATS_NTILE_GROUP_COL}, WEEK, BRAND_PRES_RET, PROD_KEY, {len(feature_cols_for_sample)} features\")\n\n# Final safeguard in pandas (UPPERCASE only; no extra columns created)\nsample_input[\"WEEK\"] = sample_input[\"WEEK\"].fillna(\"000000\")\nsample_input[\"BRAND_PRES_RET\"] = sample_input[\"BRAND_PRES_RET\"].fillna(\"UNKNOWN\")\nsample_input[\"PROD_KEY\"] = sample_input[\"PROD_KEY\"].fillna(\"UNKNOWN\")\n\n# Force context types to string so model signature expects VARCHAR (and matches inference).\n# (In training WEEK usually comes numeric; if NUMBER is inferred here, it fails when inference receives VARCHAR.)\nsample_input[\"CUSTOMER_ID\"] = sample_input[\"CUSTOMER_ID\"].astype(str)\nsample_input[STATS_NTILE_GROUP_COL] = sample_input[STATS_NTILE_GROUP_COL].astype(str)\nsample_input[\"WEEK\"] = sample_input[\"WEEK\"].astype(str)\nsample_input[\"BRAND_PRES_RET\"] = sample_input[\"BRAND_PRES_RET\"].astype(str)\nsample_input[\"PROD_KEY\"] = sample_input[\"PROD_KEY\"].astype(str)\n\n# Quick debug (will be seen in notebook/script execution)\ntry:\n    null_week = int(sample_input[\"WEEK\"].isna().sum())\n    null_brand = int(sample_input[\"BRAND_PRES_RET\"].isna().sum())\n    null_prod_key = int(sample_input[\"PROD_KEY\"].isna().sum())\n    print(f\"üîé sample_input nulls ‚Äî week: {null_week}, brand_pres_ret: {null_brand}, prod_key: {null_prod_key}\")\nexcept Exception:\n    pass\n",
      "id": "105b0444-5e3c-4b85-8396-000564be1d51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Register Partitioned Model\n",
      "id": "d0463d5b-b9ba-4c95-97a2-071d6483544c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üìù REGISTERING PARTITIONED MODEL\")\nprint(\"=\" * 80)\n\nversion_date = datetime.now().strftime(\"%Y%m%d_%H%M\")\n\nprint(f\"\\nüìù Registering in Model Registry...\")\nprint(f\"   Name: {PARTITIONED_MODEL_NAME}\")\nprint(f\"   Version: v_{version_date}\")\n\ntry:\n    mv = registry.log_model(\n        partitioned_model,\n        model_name=PARTITIONED_MODEL_NAME,\n        version_name=f\"v_{version_date}\",\n        comment=f\"Partitioned regression model for uni_box_week - Combines {len(loaded_models)} group-specific models (LGBM/XGB/SGD)\",\n        metrics={\n            \"num_groups\": len(loaded_models),\n            \"num_features\": len(feature_cols),\n            \"model_type\": \"mixed\",\n            \"groups\": \",\".join(sorted(loaded_models.keys())),\n        },\n        sample_input_data=sample_input,\n        task=task.Task.TABULAR_REGRESSION,\n        options={\"function_type\": \"TABLE_FUNCTION\"},\n    )\n\n    print(\"\\n‚úÖ Partitioned model registered successfully!\")\n    # Move PRODUCTION alias in a \"clean\" way:\n    # 1) remove alias from previous version (if exists)\n    # 2) assign alias to new version\n    model_fqn = f\"{DATABASE}.{MODELS_SCHEMA}.{PARTITIONED_MODEL_NAME}\"\n    # NOTE: the version_name logged above is \"v_<timestamp>\" (lowercase).\n    # We use exactly that identifier to avoid ambiguity.\n    new_version_name = f\"v_{version_date}\"\n    try:\n        session.sql(f\"ALTER MODEL {model_fqn} VERSION PRODUCTION UNSET ALIAS\").collect()\n        print(\"üßπ Alias 'PRODUCTION' removed from previous version\")\n    except Exception as alias_unset_err:\n        print(f\"‚ÑπÔ∏è  Could not remove previous alias (may not exist): {str(alias_unset_err)[:120]}\")\n\n    session.sql(\n        # Alias is an IDENTIFIER; avoid single quotes to prevent parsing errors.\n        f\"ALTER MODEL {model_fqn} VERSION {new_version_name} SET ALIAS=PRODUCTION\"\n    ).collect()\n    print(\"üè∑Ô∏è  Alias 'PRODUCTION' moved to new version\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå Error registering model: {str(e)}\")\n    raise\n",
      "id": "2264226a-3310-4bb8-93d2-2379b6d58342",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Verify Registration\n",
      "id": "98c4c25a-f95b-4042-9df7-bad97a1065e0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üîç VERIFYING REGISTRATION\")\nprint(\"=\" * 80)\n\nresult = session.sql(\n    f\"\"\"\n    SHOW MODELS LIKE '{PARTITIONED_MODEL_NAME}' \n    IN SCHEMA {DATABASE}.{MODELS_SCHEMA}\n\"\"\"\n).collect()\n\nif result:\n    print(\"‚úÖ Partitioned model found in registry\")\n\n    versions = session.sql(\n        f\"\"\"\n        SHOW VERSIONS IN MODEL {DATABASE}.{MODELS_SCHEMA}.{PARTITIONED_MODEL_NAME}\n    \"\"\"\n    ).collect()\n\n    print(f\"\\nüìä Versions: {len(versions)}\")\n    for v in versions[-3:]:\n        print(f\"   - {v['name']}\")\nelse:\n    print(\"‚ùå Model not found in registry\")\n",
      "id": "868aadf4-71a6-4c06-9d6d-9ee4f1182772",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Summary\n",
      "id": "8772759c-c74d-4fde-aefe-034c1dee792b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ PARTITIONED MODEL CREATION COMPLETE!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úÖ Source models: {len(loaded_models)} group-specific models\")\nprint(f\"   ‚úÖ Partitioned model: {PARTITIONED_MODEL_NAME}\")\nprint(f\"   ‚úÖ Version: v_{version_date}\")\nprint(f\"   ‚úÖ Alias: PRODUCTION\")\nprint(f\"   ‚úÖ Features: {len(feature_cols)}\")\nprint(\n    f\"   ‚úÖ Groups: {', '.join(sorted(loaded_models.keys())[:5])}... ({len(loaded_models)} total)\"\n)\n\nprint(\"\\nüí° Next Steps:\")\nprint(\"   1. Review partitioned model registration\")\nprint(\"   2. Run 06_partitioned_inference_batch.py for batch inference\")\nprint(\n    \"   3. Use partitioned inference syntax: TABLE(model!PREDICT(...) OVER (PARTITION BY stats_ntile_group))\"\n)\n\nprint(\"\\nüéØ Key Benefits:\")\nprint(\"   - 16 models combined into one partitioned model\")\nprint(\"   - Automatic routing by stats_ntile_group\")\nprint(\"   - Consistent inference syntax\")\nprint(\"   - Each group uses its optimized hyperparameters\")\n\nprint(\"\\n\" + \"=\" * 80)\n",
      "id": "b13dddee-8730-4676-8144-79858c5fbbe5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}